{"id":2305.00379,"title":"Image Completion via Dual-path Cooperative Filtering","abstract":"Given the recent advances with image-generating algorithms, deep image\ncompletion methods have made significant progress. However, state-of-art\nmethods typically provide poor cross-scene generalization, and generated masked\nareas often contain blurry artifacts. Predictive filtering is a method for\nrestoring images, which predicts the most effective kernels based on the input\nscene. Motivated by this approach, we address image completion as a filtering\nproblem. Deep feature-level semantic filtering is introduced to fill in missing\ninformation, while preserving local structure and generating visually realistic\ncontent. In particular, a Dual-path Cooperative Filtering (DCF) model is\nproposed, where one path predicts dynamic kernels, and the other path extracts\nmulti-level features by using Fast Fourier Convolution to yield semantically\ncoherent reconstructions. Experiments on three challenging image completion\ndatasets show that our proposed DCF outperforms state-of-art methods.","authors":"Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger","published_date":"2023-04-30T03:54:53Z","link":"http:\/\/arxiv.org\/abs\/2305.00379v1","markdown":"# Image Completion via Dual-Path Cooperative Filtering\n\n###### Abstract\n\nGiven the recent advances with image-generating algorithms, deep image completion methods have made significant progress. However, state-of-art methods typically provide poor cross-scene generalization, and generated masked areas often contain blurry artifacts. Predictive filtering is a method for restoring images, which predicts the most effective kernels based on the input scene. Motivated by this approach, we address image completion as a filtering problem. Deep feature-level semantic filtering is introduced to fill in missing information, while preserving local structure and generating visually realistic content. In particular, a Dual-path Cooperative Filtering (DCF) model is proposed, where one path predicts dynamic kernels, and the other path extracts multi-level features by using Fast Fourier Convolution to yield semantically coherent reconstructions. Experiments on three challenging image completion datasets show that our proposed DCF outperforms state-of-art methods.\n\nPourya Shamsolmoali\\({}^{1}\\), Masoumeh Zareapoor\\({}^{2}\\), Eric Granger\\({}^{3}\\)\\({}^{1}\\)Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, China\n\n\\({}^{2}\\)School of Automation, Shanghai Jiao Tong University, China\n\n\\({}^{3}\\)Lab. d'imagerie, de vision et d'intelligence artificielle, Dept. of Systems Eng., ETS, Canada Image Completion, Image Inpainting, Deep Learning.\n\n## 1 Introduction\n\nThe objective of image completion (inpainting) is to recover images by reconstructing missing regions. Images with inpainted details must be visually and semantically consistent. Therefore, robust generation is required for inpainting methods. Generative adversarial networks (GANs) [2, 18] or auto-encoder networks [16, 20, 21] are generally used in current state-of-the-art models [10, 11, 19] to perform image completion. In these models, the input image is encoded into a latent space by generative network-based inpainting, which is then decoded to generate a new image. The quality of inpainting is entirely dependent on the data and training approach, since the procedure ignores priors (for example smoothness among nearby pixels or features). It should be noted that, unlike the generating task, image inpainting has its own unique challenges. First, image inpainting requires that the completed images be clean, high-quality, and natural. These constraints separate image completion from the synthesis tasks, which focuses only on naturalness. Second, missing regions may appear in different forms, and the backgrounds could be from various scenes. Given these constraints, it is important for the inpainting method to have a strong capacity to generalize across regions that are missing. Recent generative networks have made substantial progress in image completion, but they still have a long way to go before they can address the aforementioned problems.\n\nFor instance, RFRNet [7] uses feature reasoning on the auto-encoder architecture for the task of image inpainting. As shown in Fig. 1, RFRNet produces some artifacts in output images. JPGNet and MISF [5, 8] are proposed to address generative-based inpainting problems [7, 12, 15] by reducing artifacts using image-level predictive filtering. Indeed, image-level predictive filtering reconstructs pixels from neighbors, and filtering kernels are computed adaptively based on the inputs. JPGNet is therefore able to retrieve the local structure while eliminating artifacts. As seen in Fig. 1, JPGNet's artifacts are more efficiently smoother than RFRNet's. However, many details may be lost, and the actual structures are not reconstructed. LaMa [19] is a recent image inpainting approach that uses Fast Fourier Convolution (FFC) [3] inside their ResNet-based LaMa-Fourier model to address the lack of receptive field for producing repeated patterns in the missing areas. Previously, researchers struggled with global self-attention [22] and its computational complexity, and they were still unable to perform satisfactory recovery for repeated man-made structures as effectively as with LaMa. Nonetheless, as the missing regions get bigger and pass the object boundary, LaMa creates faded structures.\n\nFigure 1: Examples of an image completed with our DCF model compared to baseline methods on the Paris dataset. DCF generates high-fidelity and more realistic images.\nIn [12], authors adopts LaMa as the base network, and can captures various types of missing information by utilizing additional types of masks. They use more damaged images in the training phase to improve robustness. However, such a training strategy is unproductive. Transformer-based approaches [20, 23] recently have attracted considerable interest, despite the fact that the structures can only be estimated within a low-resolution coarse image, and good textures cannot be produced beyond this point. Recent diffusion-based inpainting models [13, 17] have extended the limitations of generative models by using image information to sample the unmasked areas or use a score-based formulation to generate unconditional inpainted images, however, these approaches are not efficient in real-world applications.\n\nTo address this problem, we introduce a new neural network architecture that is motivated by the predictive filtering on adaptability and use large receptive field for producing repeating patterns. In particular, this paper makes two key contributions. First, semantic filtering is introduced to fill the missing image regions by expanding image-level filtering into a feature-level filtering. Second, a Dual-path Cooperative Filtering (DCF) model is introduced that integrates two semantically connected networks - a kernel prediction network, and a semantic image filtering network to enhance image details.\n\nThe semantic filtering network supplies multi-level features to the kernel prediction network, while the kernel prediction network provides dynamic kernels to the semantic filtering network. In addition, for efficient reuse of high-frequency features, FFC [3] residual blocks are utilized in the semantic filtering network to better synthesize the missing regions of an image, leading to improved performance on textures and structures. By linearly integrating neighboring pixels or features, DCF is capable of reconstructing them with a smooth prior across neighbors. Therefore, DCF utilizes both semantic and pixel-level filling for accurate inpainting. Following Fig. 1, the propose model produces high-fidelity and realistic images. Furthermore, in comparison with existing methods, our technique involves a dual-path network with a dynamic convolutional operation that modifies the convolution parameters based on different inputs, allowing to have strong generalization. A comprehensive set of experiments conducted on three challenging benchmark datasets (CelebA-HQ [6], Places2 [24], and Paris StreetView [4]), shows that our proposed method yields better qualitative and quantitative results than state-of-art methods.\n\n## 2 Methodology\n\nPredictive filtering is a popular method for restoring images that is often used for image denoising tasks [14]. We define image completion as pixel-wise predictive filtering:\n\n\\[I_{c}=I_{m}\\vartriangle T, \\tag{1}\\]\n\nin which \\(I_{c}\\in\\mathbb{R}^{(H\\times W\\times 3)}\\) represents a complete image, \\(I_{m}\\in\\mathbb{R}^{(H\\times W\\times 3)}\\) denotes the input image with missing regions from the ground truth image \\(I_{gr}\\in\\mathbb{R}^{(H\\times W\\times 3)}\\). The tensor \\(T\\in\\mathbb{R}^{(H\\times W\\times N^{2})}\\) has \\(HW\\) kernels for filtering each pixel and the pixel-wise filtering operation is indicated by the operation \\({}^{\\prime}\\vartriangle^{\\prime}\\). Rather than using image-level filtering, we perform the double-path feature-level filtering, to provides more context information. Our idea is that, even if a large portion of the image is destroyed, semantic information can be maintained. To accomplish semantic filtering, we initially use an auto-encoder network in which the encoder extracts features of the damaged image \\(I_{m}\\), and the decoder maps the extracted features to the complete image \\(I_{c}\\). Therefore, the encoder can be defined by:\n\n\\[f_{L}=\\rho(I_{m})=\\rho_{L}(...\\rho_{l}(...\\rho_{2}(\\rho_{1}(I_{m})))), \\tag{2}\\]\n\nin which \\(\\rho(.)\\) denotes the encoder while \\(f_{l}\\) represents the feature taken from the deeper layers (\\(l^{th}\\)), \\(f_{l}=\\rho_{l}(f_{l-1})\\). For instance, \\(f_{l}\\) shows the last layer's result of \\(\\rho(.)\\).\n\nIn our encoder network, to create remarkable textures and semantic structures within the missing image regions, we adopt Fast Fourier Convolutional Residual Blocks (FFC-Res) [19]. The FFC-Res shown in Fig. 2 (b) has two FFC layers. The channel-wise Fast Fourier Transform (FFT) [1] is the core of the FFC layer [3] to provide a whole image-wide receptive field. As shown in Fig. 2 (c), the FFC layer divides channels into two branches: a) a local branch, which utilizes standard convolutions to capture spatial information, and b) a global branch, which employs a Spectral Transform module to analyze global structure and capture long-range context.\n\nFigure 2: Overview of the proposed architecture. (a) Our proposed DCF inpainting network with (b) FFC residual block to have a larger receptive field. (c) and (d) show the architecture of the FFC and Spectral Transform layers, respectively.\nOutputs of the local and global branches are then combined. Two Fourier Units (FU) are used by the Spectral Transform layer (Fig. 2 (d)) in order to capture both global and semi-global features. The FU on the left represents the global context. In contrast, the Local Fourier Unit on the right side of the image takes in one-fourth of the channels and focuses on the semi-global image information. In a FU, the spatial structure is generally decomposed into image frequencies using a Real FFT2D operation, a frequency domain convolution operation, and ultimately recovering the structure via an Inverse FFT2D operation. Therefore, based on the encoder the network of our decoder is defined as:\n\n\\[I_{c}=\\rho^{-1}(f_{L}), \\tag{3}\\]\n\nin which \\(\\rho^{-1}(.)\\) denotes the decoder. Then, similar to image-level filtering, we perform semantic filtering on extracted features according to:\n\n\\[\\hat{f}_{l}[r]=\\sum_{s\\in\\mathcal{N}_{\\kappa}}T_{\\kappa}^{l}[s-r]f_{l}[s], \\tag{4}\\]\n\nin which \\(r\\) and \\(s\\) denote the image pixels' coordinates, whereas the \\(\\mathcal{N}_{\\kappa}\\) consist of \\(N^{2}\\) closest pixels. \\(T_{\\kappa}^{l}\\) signifies the kernel for filtering the \\(\\kappa^{th}\\) component of \\(T_{l}\\) through its neighbors \\(\\mathcal{N}_{\\kappa}\\). To incorporate every element-wise kernel, we use the matrix \\(T_{l}\\) as \\(T_{\\kappa}^{l}\\). Following this, Eq. (2) is modified by substituting \\(f_{l}\\) with \\(\\hat{f}_{l}\\). In addition, we use a predictive network to predict the kernels' behaviour in order to facilitate their adaptation for two different scenes.\n\n\\[T_{l}=\\varphi_{l}(I_{m}), \\tag{5}\\]\n\nin which \\(\\varphi_{l}(.)\\) denotes the predictive network to generate \\(T_{l}\\). In Fig. 2(a) and Table 2, we illustrate our image completion network which consist of \\(\\rho(.),\\rho^{-1},\\) and \\(\\varphi_{l}(.)\\). The proposed network is trained using the \\(L_{1}\\) loss, perceptual loss, adversarial loss, and style loss, similar to predictive filtering.\n\n## 3 Experiments\n\nIn this section, the performance of our DCF model is compared to state-of-the-art methods for image completion task. Experiments are carried out on three datasets, CelebA-HQ [6], Places2 [24], and Paris StreetView [4] at \\(256\\times 256\\) resolution images. With all datasets, we use the standard training and testing splits. In both training and testing we use the diverse irregular mask (20%-40% of images occupied by holes) given by PConv [9] and regular center mask datasets. The code is provided at _DCF_.\n\n**Performance Measures:** The structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and Frechet inception distance (FID) are used as the evaluation metrics.\n\n### Implementation Details\n\nOur proposed model's framework is shown in Table 2.\n\n**Loss functions.** We follow [15] and train the networks using four loss functions, including \\(L_{1}\\) loss (\\(\\ell_{1}\\)), adversarial loss (\\(\\ell_{A}\\)), style loss (\\(\\ell_{S}\\)), and perceptual loss (\\(\\ell_{P}\\)), to obtain images with excellent fidelity in terms of quality as well as semantic levels. Therefore, we can write the reconstruction loss (\\(\\ell_{R}\\)) as:\n\n\\[\\ell_{R}=\\lambda_{1}\\ell_{1}+\\lambda_{a}\\ell_{A}+\\lambda_{p}\\ell_{P}+\\lambda_ {s}\\ell_{S}. \\tag{6}\\]\n\n\\begin{table}\n\\begin{tabular}{l|c|c||c|c} \\hline \\multicolumn{4}{c||}{Feature extracting network} & \\multicolumn{2}{c}{Predicting network} \\\\ \\hline Layer & In. & Out\/size & In. & Out\/size \\\\ \\hline \\hline conv(7,3,64) & \\(I_{m}\\) & \\(f_{1}\\) \/ 256 & \\(I_{m}\\) & \\(e_{1}\\) \/ 256 \\\\ conv(4,64,128) & \\(f_{1}\\) & \\(f_{2}\\) \/ 128 & \\(e_{1}\\) & \\(e_{2}\\) \/ 128 \\\\ pooling & \\(f_{2}\\) & \\(f_{2}\\) \/ 64 & \\(e_{2}\\) & \\(e_{2}\\) \/ 64 \\\\ conv(4,128,256) & \\(f_{2}\\) & \\(f_{3}\\) \/ 64 & \\([f_{2}^{\\prime},e_{2}^{\\prime}]\\) & \\(e_{3}\\) \/ 64 \\\\ \\(f_{3}\\) \\(\\\nin which \\(\\lambda_{1}=1\\), \\(\\lambda_{a}=\\lambda_{p}=0.1\\), and \\(\\lambda_{s}=250\\). More details on the loss functions can be found in [15].\n\n**Training setting.** We use Adam as the optimizer with the learning rate of \\(1e-4\\) and the standard values for its hyperparameters. The network is trained for 500k iterations and the batch size is 8. The experiments are conducted on the same machine with two RTX-3090 GPUs.\n\n### Comparisons to the Baselines\n\n**Qualitative Results.** The proposed DCF model is compared to relevant baselines such as RFRNet [7], JPGNet [5], and LaMa [19]. Fig. 3 and Fig. 4 show the results for the Places2 and CelebA-HQ datasets respectively. In comparison to JPGNet, our model preserves substantially better recurrent textures, as shown in Fig. 3. Since JPGNet lacks attention-related modules, high-frequency features cannot be successfully utilized due to the limited receptive field. Using FFC modules, our model expanded the receptive field and successfully project source textures on newly generated structures. Furthermore, our model generates superior object boundary and structural data compared to LaMa. Large missing regions over larger pixel ranges limit LaMa from hallucinating adequate structural information. However, ours uses the advantages of the coarse-to-fine generator to generate a more precise object with better boundary. Fig. 4 shows more qualitative evidence. While testing on facial images, RFRNet and LaMa produce faded forehead hairs and these models are not robust enough. The results of our model, nevertheless, have more realistic textures and plausible structures, such as forehead form and fine-grained hair.\n\n**Quantitative Results.** On three datasets, we compare our proposed model with other inpainting models. The results shown in Table 2 lead to the following conclusions: 1) Compared to other approaches, our method outperforms them in terms of PSNR, SSIM, and FID scores for the most of datasets and mask types. Specifically, we achieve 9% higher PNSR on the Places2 dataset's irregular masks than RFRNet. It indicates that our model has advantages over existing methods. 2) We observe similar results while analyzing the FID. On the CelebA-HQ dataset, our method achieves 2.5% relative lower FID than LaMa under the center mask. This result indicates our method's remarkable success in perceptual restoration. 3) The consistent advantages over several datasets and mask types illustrate that our model is highly generalizable.\n\n## 4 Conclusion\n\nDual-path cooperative filtering (DCF) was proposed in this paper for high-fidelity image inpainting. For predictive filtering at the image and deep feature levels, a predictive network is proposed. In particular, image-level filtering is used for details recovery, whereas deep feature-level filtering is used for semantic information completion. Moreover, in the image-level filtering the FFC residual blocks is adopted to recover semantic information and resulting in high-fidelity outputs. The experimental results demonstrate our model outperforms the state-of-art inpainting approaches.\n\n#### Acknowledgments\n\nThis research was supported in part by NSFC China. The corresponding author is Masoumeh Zareapoor.\n\n\\begin{table}\n\\begin{tabular}{l|l|c c|c c|c c} \\hline \\hline \\multirow{3}{*}{} & \\multirow{3}{*}{Method} & \\multicolumn{3}{c|}{CelebA-HQ} & \\multicolumn{3}{c|}{Places2} & \\multicolumn{3}{c}{Paris StreetView} \\\\ \\cline{3-8}  & & Irregular & Center & Irregular & Center & Irregular & Center \\\\ \\hline \\multirow{8}{*}{PSNR\\(\\uparrow\\)} & RFRNet [7] & 26.63 & 21.32 & 22.58 & 18.27 & 23.81 & 19.26 \\\\  & JPGNet [5] & 25.54 & 22.71 & 23.93 & 19.22 & 24.79 & 20.63 \\\\  & TFill [23] & 26.84 & 23.65 & 24.32 & 20.49 & 25.46 & 21.85 \\\\  & LaMa [19] & 27.31 & 24.18 & **25.27** & 21.67 & 25.84 & 22.59 \\\\  & GLaMa [12] & 28.17 & 25.13 & 25.08 & 21.83 & 26.23 & 22.87 \\\\  & DCF (ours) & **28.34** & **25.62** & 25.19 & **22.30** & **26.57** & **23.41** \\\\ \\hline \\multirow{8}{*}{SSIM\\(\\uparrow\\)} & RFRNet [7] & 0.934 & 0.912 & 0.819 & 0.801 & 0.862 & 0.849 \\\\  & JPGNet [5] & 0.927 & 0.904 & 0.825 & 0.812 & 0.873 & 0.857 \\\\  & TFill [23] & 0.933 & 0.907 & 0.826 & 0.814 & 0.870 & 0.857 \\\\  & LaMa [19] & 0.939 & 0.911 & 0.829 & 0.816 & 0.871 & 0.856 \\\\  & GLaMa [12] & 0.941 & 0.925 & **0.833** & 0.817 & 0.872 & 0.858 \\\\  & DCF (ours) & **0.943** & **0.928** & 0.832 & **0.819** & **0.876** & **0.861** \\\\ \\hline \\multirow{8}{*}{FID\\(\\downarrow\\)} & RFRNet [7] & 17.07 & 17.83 & 15.56 & 16.47 & 40.23 & 41.08 \\\\  & JPGNet [5] & 13.92 & 15.71 & 15.14 & 16.23 & 37.61 & 39.24 \\\\  & TFill [23] & 13.18 & 13.87 & 15.48 & 16.24 & 33.29 & 34.41 \\\\  & LaMa [19] & 11.28 & 12.95 & 14.73 & 15.46 & 32.30 & 33.26 \\\\  & GLaMa [12] & 11.21 & 12.91 & 14.70 & 15.35 & 32.12 & 33.07 \\\\ \\cline{2-8}  & DCF w.o. Sem-Fil & 14.34 & 15.24 & 17.56 & 18.11 & 42.57 & 44.38 \\\\  & DCF w.o. FFC & 13.52 & 14.26 & 15.83 & 16.98 & 40.54 & 41.62 \\\\  & DCF (ours) & **11.13** & **12.63** & **14.52** & **15.09** & **31.96** & **32.85** \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 2: Ablation study and quantitative comparison of our proposed and state-of-art methods on center and free form masked images from the CelebA-HQ, Places2, and Paris StreetView datasets.","pdf_link":"http:\/\/arxiv.org\/pdf\/2305.00379v1.pdf"}
{"id":2307.16362,"title":"High Sensitivity Beamformed Observations of the Crab Pulsar's Radio\n  Emission","abstract":"We analyzed four epochs of beamformed EVN data of the Crab Pulsar at 1658.49\nMHz. With the high sensitivity resulting from resolving out the Crab Nebula, we\nare able to detect even the faint high-frequency components in the folded\nprofile. We also detect a total of 65951 giant pulses, which we use to\ninvestigate the rates, fluence, phase, and arrival time distributions. We find\nthat for the main pulse component, our giant pulses represent about 80% of the\ntotal flux. This suggests we have a nearly complete giant pulse energy\ndistribution, although it is not obvious how the observed distribution could be\nextended to cover the remaining 20% of the flux without invoking large numbers\nof faint bursts for every rotation. Looking at the difference in arrival time\nbetween subsequent bursts in single rotations, we confirm that the likelihood\nof finding giant pulses close to each other is increased beyond that expected\nfor randomly occurring bursts - some giant pulses consist of causally related\nmicrobursts, with typical separations of $\\sim\\!30{\\rm\\;\\mu s}$ - but also find\nevidence that at separations $\\gtrsim\\!100{\\rm\\;\\mu s}$ the likelihood of\nfinding another giant pulse is suppressed. In addition, our high sensitivity\nenabled us to detect weak echo features in the brightest pulses (at\n$\\sim\\!0.4\\%$ of the peak giant pulse flux), which are delayed by up to\n$\\sim\\!300{\\rm\\;\\mu s}$.","authors":"Rebecca Lin, Marten H. van Kerkwijk","published_date":"2023-07-31T01:36:55Z","link":"http:\/\/arxiv.org\/abs\/2307.16362v2","markdown":"# High Sensitivity Beamformed Observations of the Crab Pulsar's Radio Emission\n\n###### Abstract\n\nWe analyzed four epochs of beamformed EVN data of the Crab Pulsar at \\(1658.49\\rm\\,MHz\\). With the high sensitivity resulting from resolving out the Crab Nebula, we are able to detect even the faint high-frequency components in the folded profile. We also detect a total of \\(65951\\) giant pulses, which we use to investigate the rates, fluence, phase, and arrival time distributions. We find that for the main pulse component, our giant pulses represent about 80% of the total flux. This suggests we have a nearly complete giant pulse energy distribution, although it is not obvious how the observed distribution could be extended to cover the remaining 20% of the flux without invoking large numbers of faint bursts for every rotation. Looking at the difference in arrival time between subsequent bursts in single rotations, we confirm that the likelihood of finding giant pulses close to each other is increased beyond that expected for randomly occurring bursts - some giant pulses consist of causally related microbursts, with typical separations of \\(\\sim 30\\rm\\ \\mu s\\) - but also find evidence that at separations \\(\\gtrsim\\!100\\rm\\ \\mu s\\) the likelihood of finding another giant pulse is suppressed. In addition, our high sensitivity enabled us to detect weak echo features in the brightest pulses (at \\(\\sim\\!0.4\\%\\) of the peak giant pulse flux), which are delayed by up to \\(\\sim\\!300\\rm\\ \\mu s\\).\n\nPulsars (1306) -- Radio bursts (1339) -- Very long baseline interferometry (1769) 0000-0002-4818-2886]Rebecca Lin\n\n0000-0002-4882-0886]Marten H. van Kerkwijk\n\n0000-0002-4882-0886]D.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A. Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.A Leiden-Wagner, A. Leiden-Wagner, A.\nInvestigation of the emission from the Crab Pulsar is complicated by propagation effects along the line of sight, especially at lower frequencies, \\(\\lesssim 2\\ \\mathrm{GHz}\\). While dispersion can be removed using coherent de-dispersion (either during recording, or afterwards with baseband data), scattering effects are difficult to remove. This includes echoes due to propagation in the Crab Nebula itself, which sometimes are bright and obvious (Backer et al., 2000; Lyne et al., 2001), but can also be quite faint (Driessen et al., 2019), making it difficult to disentangle them from microbursts without having a good pulse sample to look for repeating structure.\n\nAnother complication in studying the emission of the Crab Pulsar is the radio-bright nebula in which the pulsar resides. This contributes noise and hence many previous studies relied on long integrations to observe both the weaker pulse components and echoes in the average profile. But the contribution to the noise can be reduced by resolving the nebula, using large dishes or arrays, such as the VLA, Arecibo, and Westerbork (Moffett & Hankins, 1996; Cordes et al., 2004; Karuppusamy et al., 2010; Lewandowska et al., 2022).\n\nIn this paper, we use the European VLBI Network (EVN) to resolve out the Crab Nebula and obtain high sensitivity data. In Section 2, we describe our observations and data reduction, and in Section 3, we present the resulting pulse profiles and the components that are detectable at our high sensitivity. We turn to an analysis of GPs in Section 4, investigating their rates, fluence, phase, and arrival time distributions, as well as weak echoes seen in the brightest GPs. We summarize our findings in Section 5.\n\n## 2 Observations and Data Reduction\n\nWe analyze observations of the Crab Pulsar taken by the EVN, projects EK036 A-D, at four epochs between 2015 Oct and 2017 May (see Table 1). Throughout these observations, calibrator sources were also observed resulting in breaks in our data. While many dishes participated in these observations, for our analysis we only use telescope data that had relatively clean signals across the frequency range of \\(1594.49-1722.49\\ \\mathrm{MHz}\\) in both circular polarizations. At each single dish, real-sampled data were recorded in either 2 bit MARK 5B or VDIF format1, covering the frequency range in either eight contiguous \\(16\\ \\mathrm{MHz}\\) wide bands or four contiguous \\(32\\ \\mathrm{MHz}\\) wide bands.\n\nFootnote 1: For specifications of MARK5B and VDIF, see [https:\/\/www.haystack.mit.edu\/haystack-memo-series\/mark-5-memos\/](https:\/\/www.haystack.mit.edu\/haystack-memo-series\/mark-5-memos\/) and [https:\/\/vlbi.org\/wp-content\/uploads\/2019\/03\/VDIF_specification_Release_1.1.1.pdf](https:\/\/vlbi.org\/wp-content\/uploads\/2019\/03\/VDIF_specification_Release_1.1.1.pdf), respectively.\n\nFor these datasets, single dish data were processed and then combined coherently to form a tied-array beam as described in Lin et al. (2023). The resulting RFI-removed, normalized, de-dispersed (using dispersion measures (DMs) listed in Table 1), parallactic angle corrected, and phased baseband data were squared to form intensity data. As in Lin et al. (2023), we estimate the system equivalent flux density (SEFD) for the phased EVN array as \\((S_{\\text{CN}}+\\langle S_{\\text{tel}}\\rangle)\/N_{\\text{tel}}\\approx 140-160\\ \\mathrm{ Jy}\\), where \\(S_{\\text{CN}}\\approx 833\\ \\mathrm{Jy}\\) is the SEFD of the Crab Nebula at our observing frequency (Bietenholz et al., 1997), \\(\\langle S_{\\text{tel}}\\rangle\\simeq 300\\ \\mathrm{Jy}\\) is the average nominal SEFD of the telescopes2 and \\(N_{\\text{tel}}=7\\ \\mathrm{or}\\ 8\\) is the number of telescopes used. By combining the single dishes into a synthesized beam, we resolve out the radio-bright Crab Nebula and increase our sensitivity, thus allowing us to investigate the weaker radio emission of the Crab Pulsar.\n\nFootnote 2: [http:\/\/old.evlbi.org\/cgi-bin\/EVNcalc](http:\/\/old.evlbi.org\/cgi-bin\/EVNcalc).\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c c c} \\hline \\hline Observation & & \\(t_{\\text{sep}}\\)a  & & & DMc  & & & Giant Pulsesd  & & \\\\  & Date & (h) & Telescopes usedb  & & & & Giant Pulsesd  & \\\\  & Date & (h) & Telescopes usedb  & & & & & & \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: Observation and Giant Pulse Log.\n## 3 Pulse Profiles\n\nFor each of the phased EVN datasets, we create folded pulse profiles using polyco files generated with tempo2(Hobbs and Edwards, 2012) from the monthly Jodrell Bank Crab Pulsar ephemerides3(Lyne et al., 1993) and DM from Table 1. We averaged over all frequencies and used \\(512\\) phase bins, rotating in phase such that the MP is at phase \\(0\\). We show the resulting profiles in Figure 1, with each profile scaled to its maximum to ease comparison. With our high sensitivity, we can see all five pulse components expected from the multifrequency overview of Hankins et al. (2015), corresponding to the LFC, MP, IP, HFC1 and HFC2 (with the latter two detected at \\(\\sim\\!1.66\\ \\mathrm{GHz}\\) for the first time).\n\nFootnote 3: [http:\/\/www.jb.man.ac.uk\/~pulsar\/crab.html](http:\/\/www.jb.man.ac.uk\/~pulsar\/crab.html).\n\nWe fit the pulse components in the EKO36 datasets with five Gaussians to look for possible changes, both between our epochs and relative to the compilation from Hankins et al. (2015). Our fitted parameters are presented in Table 2, together with the values inferred from Hankins et al. (2015). One sees that the results for our four observations are all consistent. At \\(1.4\\ \\mathrm{GHz}\\), Lyne et al. (2013) found that the separations between the MP and IP and between the MP and LFC increase at a rate of \\(0\\fdg 5\\pm 0\\fdg 2\\) per century and \\(11\\arcdeg\\pm 2\\arcdeg\\) per century, respectively. Using these rates, we expect pulse phase changes for the IP and LFC of \\(\\sim\\!0\\fdg 008\\) and \\(\\sim\\!0\\fdg 17\\), respectively, which are not detectable within our uncertainties.\n\nComparing with Hankins et al. (2015), we find good agreement in pulse phase for all components (though now we do need to take into account the drift in pulse phase). We noticed, however, that while the widths of our LFC, HFC1 and HFC2 are consistent with those given by Hankins et al. (2015), the widths of the MP and IP seem smaller, even if they are still within the nominal, rather large uncertainties of Hankins et al. (2015). Looking in more detail at their Figure 3 with measurements, one sees considerable scatter for the MP and IP, even though those strong, narrow peaks should be the easiest to measure. This might suggest that some profiles were slightly smeared (e.g., because the data were not dedispersed to exactly the right DM, which is known to vary for the Crab Pulsar, or because of changes in scattering timescale at lower frequencies, see McKee et al., 2018). For a comparison with recent data, we estimated widths from the \\(2-4\\) and \\(4-6\\ \\mathrm{GHz}\\) pulse profiles in Figure 1 of Lewandowska et al. (2022), which were taken using the VLA in D configuration to resolve out the Crab Nebula and thus have high signal-to-noise ratio; we find these are all consistent with ours.\n\nFigure 1: Folded pulse profile of the Crab Pulsar at \\(1658.49\\ \\mathrm{MHz}\\) from EK036 observations in \\(512\\) phase bins centered on the MP. At this frequency, 5 components: LFC, MP, IP, HFC1 and HFC2 are visible. In the left panel, the profiles are normalized to their peak MP component. As the HFC1 and HFC2 components (indicated by arrows) are very faint, we show the grey region of the left panel zoomed in by a factor of \\(15\\) in the right panel, with vertical lines marking the peak of these components.\nAt lower frequencies, the pulse profiles often show echo features (e.g., Driessen et al., 2019). At our frequencies, those are expected to be too weak at delays where they might be seen in the folded pulse profile, and indeed we see none. However, at frequencies like ours, echoes can still be seen in individual pulses. For instance, at \\(1.4\\;\\mathrm{GHz}\\), Crossley et al. (2004) saw that individual bright pulses all had an echo delayed at \\(\\sim\\!50\\;\\mathrm{\\mu s}\\) (which had no counterpart at \\(4.9\\;\\mathrm{GHz}\\)). From aligning GPs before stacking them in our datasets, Lin et al. (2023) also saw hints of echo features within \\(\\sim\\!25\\;\\mathrm{\\mu s}\\) of the peaks of GPs in EK036 B and D. In Section 4.6, we confirm echoes in our data using a more careful analysis, finding that for EK036 D faint echoes are visible out to to \\(\\sim\\!300\\;\\mathrm{\\mu s}\\).\n\n## 4 Giant Pulses\n\n### Search\n\nIn Lin et al. (2023), we searched for GPs by flagging peaks above \\(8\\sigma\\) in a \\(16\\;\\mathrm{\\mu s}\\) wide running average of the intensity time stream. While we reliably found GPs, the long time window meant we could not distinguish between bursts arriving in quick succession within that time window. Hence, the previous technique was unsuitable for one of our goals, of measuring arrival time differences between bursts, including between the microbursts that GPs sometimes are composed of. Below, we describe a revised technique, which allows us to more reliably identify multiple bursts (see Figure 2). Unsurprisingly, with our new technique we detected more multiple bursts than we had previously, as can be seen by comparing numbers listed in Section 6.3 of Lin et al. 2023) with those in Table 3.\n\nFor every pulsar period in the EK036 dataset, we take \\(2.0\\;\\mathrm{ms}\\) snippets of baseband data centered at the MP and\n\n\\begin{table}\n\\begin{tabular}{l l l l l} \\hline \\hline \\multicolumn{1}{c}{\n\\begin{tabular}{c} Pulse \\\\ Comp. \\\\ \\end{tabular} } & Obs.\/ & Amplitude & Pulse Phase & FWHM \\\\  & Ref. & (\\%) & (deg.) & (deg.) \\\\ \\hline LFC\\(\\dots\\) & A & 3.6(3) & \\(-38.0(3)\\) & 7.5(6) \\\\  & B & 3.35(17) & \\(-37.67(19)\\) & 7.7(4) \\\\  & C & 3.7(2) & \\(-37.2(3)\\) & 7.7(6) \\\\  & D & 3.9(2) & \\(-37.8(2)\\) & 8.1(5) \\\\  & H15 & \\(\\dots\\) & \\(-35.78(14)\\) & 7.2(12) \\\\ MP \\(\\dots\\) & A & & & 2.786(11) \\\\  & B & & & 2.708(7) \\\\  & C & & & 2.756(11) \\\\  & D & & & 2.836(9) \\\\  & H15 & & & 3.9(11) \\\\ IP\\(\\dots\\) & A & 15.2(4) & 145.38(4) & 3.48(10) \\\\  & B & 15.2(2) & 145.28(3) & 3.59(7) \\\\  & C & 15.3(4) & 145.25(4) & 3.46(10) \\\\  & D & 14.4(3) & 145.28(4) & 3.59(8) \\\\  & H15 & \\(\\dots\\) & 145.25(4) & 5.4(11) \\\\ HFC1\\(\\dots\\) & A & 0.58(13) & 203(3) & 28(7) \\\\  & B & 0.88(9) & 198.4(13) & 25(3) \\\\  & C & 0.68(12) & 194(3) & 34(7) \\\\  & D & 0.94(11) & 196.2(15) & 36(5) \\\\  & H15 & \\(\\dots\\) & 198.2(8) & 25(5) \\\\ HFC2\\(\\dots\\) & A & 1.5(2) & 259.7(8) & 11.8(19) \\\\  & B & 1.19(14) & 259.2(7) & 11.7(16) \\\\  & C & 1.23(19) & 257.7(9) & 12(2) \\\\  & D & 1.51(15) & 259.8(7) & 14.8(16) \\\\  & H15 & \\(\\dots\\) & 259.1(4) & 11.6(12) \\\\ \\hline \\end{tabular} Note. \u2013Amplitudes and phases are relative to the MP. H15 refers to Hankins et al. (2015), and corresponding values are from evaluating the fits presented in his Tables 2 and 3 at our central observing frequency of \\(1658.49\\;\\mathrm{MHz}\\). The phases for the LFC and IP have been extrapolated to MJD 57607 (midway between EK036 A and D) using \\(d\\phi\/dt\\) values from Lyne et al. (2013). Numbers in parentheses are \\(1\\sigma\\) uncertainties in the last digit.\n\n\\end{table}\nTable 2: Properties of the Pulse Profile Components.\n\nFigure 2: Sample MP pulse rotations with GPs as detected by our algorithm (see Section 4.1 for details), shown at a time resolution of \\(1.25\\;\\mathrm{\\mu s}\\). _Top_: Single pulse with scattering tail. _Middle_: Two pulses, each with their own scattering tail. _Bottom_: A profile showing the difficulties inherent in classifying pulses: our algorithm found three pulses, but if another algorithm were to classify this as two or four pulses, that would also seem reasonable.\nIP component phase windows (roughly \\(2\\) times the size of the pulse component determined from the folded pulse profile) and create pulse intensity stacks for each component4. We average these stack across the eight frequency bands and bin over 10 time samples, or \\(0.625~{}\\mu\\)s, a value chosen to be large enough for a reliable GP detection yet well less than the scattering timescale of \\(\\sim\\)\\(5~{}\\mu\\)s during these observations (Lin et al., 2023). To detect GPs, we first subtract the off-pulse region (determined from the \\(0.5~{}\\mathrm{ms}\\) region on either side of each pulse stack), then filter with a uniform filter of size \\(5\\) (\\(3.125~{}\\mu\\)s), and finally record all samples above a detection threshold of \\(5\\sigma\\).\n\nFootnote 4: We only search for GPs inside these windows since Lin et al. (2023) found none outside for the same dataset.\n\nTo turn these sets of above-the-noise locations into detections of individual GPs, we use the following three-step process5. First, we connect detections within \\(8\\) samples (\\(5~{}\\mu\\)s, i.e., of order the scattering time), since those are likely related. Second, we remove detections spanning \\(4\\) samples (\\(2.5~{}\\mu\\)s) or less, since these are likely spurious. Third, we increase the width of a detection by \\(4\\) samples (\\(2.5~{}\\mu\\)s) on either side, mostly to ensure that if we integrate over the mask, we will capture most of the flux independent of pulse strength. With this procedure, the minimum final pulse width is \\(8.125~{}\\mu\\)s, slightly larger than the scattering timescale, and we confidently detect pulses above a threshold of \\(\\sim\\)\\(0.15~{}\\mathrm{kJy}~{}\\mu\\)s. The brightest GP we detect has a fluence of \\(\\sim 560~{}\\mathrm{kJy}~{}\\mu\\)s. With our relatively high initial detection threshold, we do not find any GPs outside our pulse windows, suggesting that we have no false detections in our sample. Nevertheless, as can be seen from the overall pulse statistics in Table 1, we find many GPs, about \\(2-3\\) per second or about one for every dozen pulsar rotations.\n\nFootnote 5: Using the binary_closing, binary_opening and binary_dilation functions, respectively, from scipy\u2019s multidimensional image processing functions (Virtanen et al., 2020).\n\nIn some pulse rotations, we detect more than one distinct GP, where \"distinct\" means that the pulse is separated by at least \\(5~{}\\mu\\)s (roughly the scattering timescale) from another pulse at our detection threshold. Here, we note that whether or not a GP is detected as single or multiple depends on the detection threshold: a GP classified as a single one at our threshold might be classified as separated at a higher threshold if it has two bright peaks with some flux in between (e.g., because the scattering tail of the first peak overlaps with the start of the next one, or a weaker burst fills in the space in between). This dependence on detection threshold may explain why Bhat et al. (2008) found no pulses wider than \\(10~{}\\mu\\)s, as they took a high detection cutoff, of \\(3~{}\\mathrm{kJy}~{}\\mu\\)s. This kind of arbitrariness seems unavoidable given the variety in pulse shapes that we see; it often is a rather subjective decision on what to take as a single bursts. To give a sense, we show in Figure 2 an example of a pulse rotation with a single burst as well as two examples of rotations with multiple bursts. In Section 4.5, we estimate the fraction of multiple bursts that is causally related from the statistics of pulse separations.\n\n### Rates\n\nWith the high sensitivity of the phased EVN array, we detected a total of \\(65951\\) GPs over \\(7.32~{}\\mathrm{hr}\\), implying an average detection rate of \\(2.5~{}\\mathrm{s}^{-1}\\). From Table 1, one sees that the rates are not the same for each epoch. Comparable detection rates are seen for both MP and IP GPs in EK036 A and C, but those are about a factor \\(2\\) smaller than the rates for EK036 B and D (which are comparable to each other).\n\nSimilar changes in detection rate were found for bright pulses by Lundgren et al. (1995) at \\(800~{}\\mathrm{MHz}\\), Bera & Chengalur (2019) at \\(1330~{}\\mathrm{GHz}\\) and by Kazantsev et al. (2019) at \\(111~{}\\mathrm{MHz}\\). Lundgren et al. (1995) suggests that almost\n\nFigure 3: GP pulse detection rates in each EK036 observation. Times when the telescope was not observing the Crab Pulsar are shaded grey. The MP (blue) and IP (orange) detection rates appear to scale together and are relatively constant across each observation.\ncertainly, these are due to changes in the scattering screen, which are known to cause changes in the scattering time on similar timescales and are expected to cause changes in magnification as well. To verify that there are no variations at shorter timescales, we calculated rates at roughly \\(5\\,\\mathrm{min}\\) intervals. As can be seen in Figure 3, we find that in a given epoch, the rates are indeed steady.\n\n### Fluences\n\nThe fluence distribution of the Crab Pulsar's GPs is typically described by power-law approximations to the reverse cumulative distribution,\n\n\\[N_{\\mathrm{GP}}(E>E_{0})=CE_{0}^{\\alpha}, \\tag{1}\\]\n\nwhere \\(\\alpha\\) is the power-law index, \\(C\\) a proportionality constant, and \\(E_{0}\\) the GP fluence such that \\(N_{\\mathrm{GP}}(E>E_{0})\\) is the occurrence rate of GPs above \\(E_{0}\\). For our data, one sees in Figure 4, that for all observations the distributions indeed appear power-law like at high fluence, with \\(\\alpha\\approx-2.0\\) and \\(-1.6\\) for MP and IP, respectively. These values are roughly consistent with values found at similar frequencies: e.g., Popov & Stappers (2007) find \\(-1.7\\) to \\(-3.2\\) for MP GPs and \\(-1.6\\) for IP GPs at \\(1197\\,\\mathrm{MHz}\\), and Majid et al. (2011) finds \\(\\alpha=-1.9\\) for the combined MP and IP distribution at \\(1664\\,\\mathrm{MHz}\\).\n\nHowever, as noted by Hankins et al. (2015) already, the power-law indices show large scatter and should be taken as roughly indicative only, showing, e.g., that at higher frequencies, very bright pulses are relatively rare. Indeed, in our data, like in more sensitive previous studies (e.g., Lundgren et al., 1995; Popov & Stappers, 2007; Bhat et al., 2008; Karuppusamy et al., 2010), the fluence distribution clearly flattens at lower fluences. At the very low end, this is because our detection method misses more pulses, but the changes above \\(\\sim 0.2\\,\\mathrm{kJy}\\,\\mathrm{\\mu s}\\) are real. This turnover may at least partially explain why a variety of power-law indices was found previously, as the measured index will depend on what part of the fluence distribution is fit (which will depend also on the magnification by scattering), as well as why for very high fluences, well away from the turn-over, the power-law index seems fairly stable (Bera & Chengalur, 2019).\n\nComparing the distributions for the different epochs, one sees that they are very similar except for a shift left or right in the figure. This confirms that the differences in rates seen between the epochs are due differences in magnification due to scintillation (and not due to the Crab Pulsar varying the rate at which pulses are emitted, which would, to first order, shift the distributions up and down).\n\nAs the fluence distributions looked roughly parabolic in log-log space, we also show cumulative log-normal distributions in Figure 4, of the form,\n\n\\[N_{\\mathrm{GP}}(E>E_{0})=\\frac{A}{2}\\left[\\mathrm{erfc}\\left(\\frac{\\ln E_{0}- \\mu}{\\sigma\\sqrt{2}}\\right)\\right], \\tag{2}\\]\n\nwhere \\(A\\) is a scale factor, \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of \\(\\ln E_{0}\\), and \\(\\mathrm{erfc}\\) is the complementary error function. One sees that these describe the observed cumulative distributions quite well.\n\nFigure 4: Reverse cumulative GP fluence distribution showing the occurrence rates of GPs. For comparison, power-law distributions (solid black lines) and log-normal distributions (dashed black line) are shown, with indices \\(\\alpha\\) and widths \\(\\sigma\\) as listed in the legend.\nIf the intrinsic distributions were log-normal, it would imply that especially for the MP, most of the flux is already captured and that the total rate of GPs is not much larger than our detection rate. For the log-normal distribution shown in Figure 4, for the MP, \\(A=2.7\\ \\mathrm{s}^{-1}\\) and the mean GP fluence is \\(\\langle E\\rangle=\\exp(\\mu+\\frac{1}{2}\\sigma^{2})=1.2\\ \\mathrm{kJy\\,\\mu s}\\) and only 1.5% of the total flux is below \\(0.15\\ \\mathrm{kJy\\,\\mu s}\\), while for the IP, \\(A=1.6\\ \\mathrm{s}^{-1}\\) and \\(\\langle E\\rangle=0.24\\ \\mathrm{kJy\\,\\mu s}\\), and 13% of the flux is below.\n\nWe can verify whether our MP GPs account for most of the flux by calculating pulse profiles with and without removing pulse rotations where GPs are detected. As can be seen in Figure 5, significant flux remains in both MP and IP. For the MP, even though the remaining signal is brighter in epochs B and D, the fraction is lower: about 18% in B and D, in comparison with 23% in A and C. This again can be understood if the larger detection rate is due to an overall magnification: a larger fraction of the pulses - and hence of the total flux - is detected.\n\nOur result is similar (but more constraining) than that of Majid et al. (2011), who showed that at least \\(54\\%\\) of overall pulsed energy flux for the Crab Pulsar is emitted in the form of GPs. But it is in contrast for what is seen by Abbate et al. (2020) for PSR J1823\\(-\\)3021A, where the detected GPs make up only a small fraction of the integrated pulse emission (\\(4\\%\\) and \\(2\\%\\) for their C1 and C2 components, respectively), and by Geyer et al. (2021) for PSR J0540\\(-\\)6919, where the detected GPs only make up \\(7\\%\\) of the total flux. This might indicate a difference in the emission process. As these authors noted, however, a larger population of undetected GPs may still be hidden below their detection threshold.\n\nFor our observations, for both MP and IP, the residual flux is much larger than expected based on the log-normal distribution, thus indicating that the true fluence distribution has more pulses at low fluence (many more for the IP); if additional pulses were emitted also in rotations that we do not detect them, their typical fluence would be the residual flux integrated over one cycle, which is \\(\\sim 25\\ \\mathrm{Jy\\,\\mu s}\\) for MP and a little less for IP. This is well below our detection limit, so consistent in that sense, but from the distributions shown in Figure 4, one would expect a much smaller rate than once per pulse period at \\(25\\ \\mathrm{Jy\\,\\mu s}\\). This might suggest that there are even more but typically fainter bursts (note that it cannot be fainter bursts accompanying the GPs we already detect, since we excluded the full rotations in calculating the resid\n\nFigure 5: Mean and median MP and IP pulse profiles obtained using all pulse rotations (in blue and orange, respectively) and using only those in which no GPs were detected (green and red, respectively) in \\(6.25\\ \\mathrm{\\mu s}\\) bins. Note that because the noise in an individual profile is not normally distributed, but rather follows a \\(\\chi_{k}^{2}\\) distribution, the median is slightly below zero in the off-pulse region, by \\((1-2\/3k)^{3}-1\\simeq-6\/9k\\simeq-0.0002\\) of the SEFD of \\(\\sim\\!150\\ \\mathrm{Jy}\\) (Section 2), or \\(\\sim\\!-0.03\\ \\mathrm{Jy}\\) given \\(k=3200\\) degrees of freedom (complex dedispersed timestream squared, averaged over 2 polarizations, 8 bands, and 100 time bins).\nual emission), or that there is some steady underlying emission. It would be worthwhile to test this with more sensitive future observations.\n\n### Pulse Phases\n\nDefining the time of arrival of a GP as the time when an increase in flux is first detected, the longitude windows where MP and IP GPs occur have total widths of \\(\\sim 680\\)\\(\\mu\\)s and \\(860\\)\\(\\mu\\)s (or \\(\\sim\\!7\\fdg 3\\) and \\(\\sim\\!9\\fdg 2\\)), respectively (averaged over the four epoch). As can be seen in Figure 6, the majority of GPs occur within much narrower windows: the root-mean-square deviations around the mean arrival phases are \\(\\sim\\!100\\)\\(\\mu\\)s and \\(\\sim\\!130\\)\\(\\mu\\)s (or \\(\\sim\\!1\\fdg 1\\) and \\(\\sim\\!1\\fdg 4\\)), respectively. The number distribution is roughly Gaussian, with a slightly negative skewness (i.e., a longer tail toward earlier phases and thus with a mode towards later phases). This was also observed by Majid et al. (2011) at a similar frequency of \\(1664\\)\\(\\mathrm{MHz}\\). In EKO36 D, a few MP pulses are detected beyond the range found in the other epochs. As we will discuss in Section 4.6, these \"outlier\" detections are due to echoes (hence, they are are omitted in our determinations of widths above).\n\nIn Figure 6, we also show the flux distributions as a function of pulse phase, including the median flux of the GPs detected in any given phase bin. One sees no obvious variation, i.e., no hint of, e.g., brighter pulses having an intrinsically narrower phase distribution. This suggests that only the probability of seeing a pulse depends on pulse phase. In our earlier work on these data, where we studied how the pulse spectra and their correlations are affected by scattering (Lin et al., 2023), we concluded that we resolved the regions from which the nanoshots that comprise individual GPs are emitted, and that this is most easily understood if the emitting plasma is ejected highly relativistically, with \\(\\gamma\\simeq 10^{4}\\) (as was already suggested by Bij et al., 2021). If so, the emission would be beamed to angles much smaller than the width of the phase windows, and the range of phases over which we observe GPs would reflect the range of angles over which plasma is ejected.\n\n### Arrival Times\n\nSeveral studies (e.g., Karuppusamy et al., 2010; Majid et al., 2011) have found that GPs in different rotations are not correlated, and that there is no correlation between MP and IP GPs, but that instead the distribution of the time delays between successive GPs follows an exponential distribution, as expected for a Poissonian process. Within a given cycle, though, multiple correlated microbursts can occur (Sallmen et al., 1999; Hankins and Eilek, 2007).\n\nWith our high sensitivity, we can investigate this in more detail. In Table 3 we show the number of rotations in which we detect multiple MP or IP bursts (i.e., double, triple etc.), as well as the number expected (listed only where larger than 0) for the case where all events are independent,\n\n\\[N_{n}=p_{n}N_{r}=\\begin{pmatrix}N_{\\mathrm{p}}\\\\ n\\end{pmatrix}\\left(\\frac{1}{N_{r}}\\right)^{n}\\left(1-\\frac{1}{N_{r}}\\right)^{ N_{\\mathrm{p}}-n}N_{r}, \\tag{3}\\]\n\nwhere \\(p_{n}\\) is the probability of a given rotation to have \\(n\\) bursts (assuming a binomial distribution), \\(N_{r}\\) is the total number of rotations observed, and \\(N_{\\mathrm{p}}\\) is the total number of bursts found (and where for numerical values we inserted numbers from Table 1: \\(N_{\\mathrm{p}}=N_{\\mathrm{MP}}\\) or \\(N_{\\mathrm{IP}}\\) and \\(N_{r}=t_{\\mathrm{exp}}\/P_{\\mathrm{Crab}}\\), where \\(P_{\\mathrm{Crab}}=33.7\\)\\(\\mathrm{ms}\\) is the rotation period of the pulsar). One sees that we detect significantly more multiples than expected by chance6, i.e., some of the detected pulses are composed of multiple, causally related microbursts.\n\nFootnote 6: In Lin et al. (2023), we wrongly concluded the multiples were consistent with arising by chance. Sadly, we used incorrect estimates of \\(N_{n}\\).\n\nIn principle, one could estimate the number of independent bursts, \\(N_{\\mathrm{p}}^{\\mathrm{ind}}\\), in each epoch by subtracting from \\(N_{\\mathrm{p}}\\) the excess pulses from Table 3, but this would not be quite correct since the excess would be relative to estimates made using the total number of observed pulses \\(N_{\\mathrm{p}}\\), not the (lower) number of independent pulses \\(N_{\\mathrm{p}}^{\\mathrm{ind}}\\). One could iterate, but an easier, unbiased estimate of \\(N_{\\mathrm{p}}^{\\mathrm{ind}}\\) can be made using the observed fraction of rotations in which we do not see any bursts, which should equal \\(N_{0}\/N_{r}=p_{0}=\\left(1-1\/N_{r}\\right)^{N_{\\mathrm{p}}^{\\mathrm{ind}}}\\). Solving for \\(N_{\\mathrm{p}}^{\\mathrm{ind}}\\), we find that \\(N_{\\mathrm{p}}^{\\mathrm{ind}}=fN_{\\mathrm{p}}\\) with fractions \\(f\\) that are consistent between all epochs, at \\(91.8\\pm 0.2\\) and \\(95.2\\pm 0.5\\)% for MP and IP, respectively. Hence, about 8 and 5% of the detected MP and IP pulses, respectively, are extra components. Or, as fractions of independent MP and IP pulses, \\((6,1,0.12)\\) and \\((4,0.3,0.0)\\%\\), respectively, are causally related double, triple, or quadruple microbursts.\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c} \\hline \\hline Observation & \\multicolumn{3}{c}{MP} & \\multicolumn{3}{c}{\\(\\dots\\)} & IP & \\multicolumn{3}{c}{\\(\\dots\\)} \\\\ Code & 2 & 3 & 4 & 5 & 6 & 2 & 3 & 4 \\\\ \\hline \\hline EK036 A & 1820(599) & 200(12) & 24 & 0 & 0 & 144(17) & 4 & 2 \\\\ EK036 B & 1431(611) & 170(18) & 22 & 3 & 1 & 237(43) & 16 & 2 \\\\ EK036 C & 611(213) & 67 (4) & 6 & 0 & 0 & 54( 7) & 4 & 0 \\\\ EK036 D & 934(395) & 117(10) & 23 & 6 & 1 & 116(19) & 9 & 0 \\\\ \\hline \\end{tabular} Note. \u2013 Numbers in parentheses are those expected if bursts occur randomly; for that case, one does not expect to find any rotations with 4 or more MP bursts or 3 or more IP bursts. Note that our GP detection method does not differentiate between microbursts and echoes, which becomes important for a few very bright pulses in EKO36 D, for which echoes were present. In addition, we are not able to distinguish microbursts that occur very close together in time. The number of detections differ from Lin et al. (2023) as a different, more robust, search algorithm is implemented here (see Section 4.1).\n\n\\end{table}\nTable 3: Number of Rotations with Multiple Bursts.\nTo investigate the distributions further, we show histograms of the time delay between pulses in Figure 7. Overdrawn are expectations for randomly arriving, independent pulses. We constructed these by bootstrapping, where we repeatedly reassign new random pulse cycles to our observed sets of pulses, and then recalculate the time delay distributions. Note that in our bootstraps, we do not randomize pulse phase, so that the observed phase distribution is correctly reflected in the time delays. One sees that as a function of pulse cycle (right column panels for MP and IP GPs in Fig. 7), the time delay distributions are not well defined.\n\nFigure 6: MP GP and IP GP fluence and count distributions as a function of pulse phase for each EK036 observation. We used pulse phase bins of \\(0.1\\%\\) and fluence bins of \\(0.1\\ \\mathrm{dex}\\). The light purple line in the fluence panels show the median for bins with more than \\(2\\) detected pulses.\nure 7), the observed histograms follow the expected exponential distribution (although the observed counts are slightly lower than the expected ones because not all pulses are independent, as is implicitly assumed in the bootstraps).\n\nFor the time delays between pulses that occur in the same cycle (left column panels for MP and IP GPs in Figure 7), the observed distributions are very different from those expected for randomly occurring bursts. One sees a large peak at short delays, representing the excess microbursts from Table 3, following a roughly exponential distribution with a mean time between bursts of \\(\\sim 30\\;\\mu\\)s or so. Intriguingly, at somewhat larger time difference, there seem to be fewer bursts than expected for independent events. This suggests that while a given detection has an enhanced probability of being in a group of causally related microbursts, the occurrence of a burst also suppresses the likelihood of another, independent, burst being produced in the same rotation. Thus, our results confirm that GPs are often composed of multiple microbursts, and they indicate that another, independent GP is less likely to occur right after.\n\n### Scattering Features\n\nIn Figure 6, one sees that in EK036 D, several MP GPs were detected at pulse phases quite far from the median phase. To investigate this, we looked at the arrival times of all GPs detected in EK036 D (see left panel of Figure 8). We found that the outliers occurred in two pulse rotations, which turned out to contain the brightest GPs in EK036 D. Looking at the pulse profiles of these brightest GPs, one sees that they are very similar (see right panels of Figure 8). In fact, closer\n\nFigure 7: Time delays between successive GPs for the MP (in blue) and IP (in orange) components for each EK036 observation. On the left MP and IP columns, time delays within a pulse rotation are shown with bins of \\(10\\;\\mu\\)s and \\(20\\;\\mu\\)s for the MP and IP respectively; the low counts in the first bin reflect the minimum separation of \\(8.75\\;\\mu\\)s between detected pulses. On the right MP and IP columns, time delays in pulse rotations are shown with bins of \\(1\\) rotation and \\(4\\) rotations for the MP and IP respectively. The red lines show the average time delay histograms for \\(1000\\) bootstrap iterations, in which we randomized the rotation in which a pulse was seen (but not the phase, to keep the observed phase distribution).\nexamination reveals that all of the brightest GPs detected in EK036 D show similar pulse profiles. This implies that the pulses far from the median pulse phase arrive late because they are actually weak echoes of the main burst, with amplitudes down to \\(\\sim 0.4\\%\\) of the peak flux and delays up to \\(\\sim 300~{}\\mu\\)s.\n\nIn Figure 9, we show singular value decomposition (SVD) approximations of the average MP GP profile for each epoch (for the IP, too few bright pulses were available). This was created from MP GP rotations with peak intensities greater than \\(200~{}\\mathrm{Jy}\\) and seemingly single peaks, aligned using time offsets found by correlation with a reference pulse. To avoid giving too much weight to the brightest pulses, and thus risking that remaining substructure enters the average profile, we normalized each rotation by the intensity at the correlation maximum before doing the SVD. One sees that all profiles are fairly sharply peaked, but sit on top of a base, which has the expected asymmetric part extending to later time due to scattering, as well as a more symmetric component, likely resulting from the collective effect of faint microbursts. Comparing the epochs, one sees that for EK036 A-C, the profile dropoff is relatively smooth and becomes undetectable after \\(\\sim\\!200~{}\\mu\\)s, while in EK036 D, the tail is much longer, extending to \\(\\sim\\!400~{}\\mu\\)s, and is much more bumpy.\n\nAlmost certainly, all bumps are echoes, including those at shorter delay in EK036 B (more clearly seen in the linear-scale plots in Lin et al.2023), Indeed, looking carefully at the stack of profiles in Figure 9, one sees that the echoes in EK036 D drift in time, moving slightly further away from the MP during the observation, with perhaps even a hint that echoes further away from the main bursts drift faster than those closer in. (Note that this stack is not completely linear in time, although given that the GP detection rate is roughly constant throughout, it is not far off.) This change in time is expected for echoes off a structure with changing distance from the line of sight, and indeed has been seen for a very prominent echo by Backer et al. (2000); Lyne et al. (2001). Overall, our observations suggests echoes are common, as also concluded from daily monitoring at \\(600~{}\\mathrm{MHz}\\) by Serafin-Nadeau et al. (2023, in prep.).\n\nFigure 8: _Left_: MP GPs and IP GPs detected in the EK036 D data. The gray shaded regions indicate when the telescope was not observing the Crab Pulsar and the black vertical lines mark our MP GP and IP GP windows. In the inset, we show two pulse rotations containing the brightest GPs \u201cA\u201d and \u201cB\u201d, in red and orange respectively. _Right, Top_: Waterfalls of the two brightest pulses in EK036 D with \\(1~{}\\mu\\)s time resolution and \\(1~{}\\mathrm{MHz}\\) frequency resolution. _Right, Bottom_: Pulse profile of the two brightest pulses in EK036 D with \\(1~{}\\mu\\)s time resolution scaled to the peak of each pulse. Pulses \u201cA\u201d and \u201cB\u201d show similar features and we conclude that during the EK036 D observations, weak echoes were present at large delays.\n## 5 Summary of Conclusions\n\nThe fine time resolution and high sensitivity in our beam-formed EVN data allowed us to confidently detect \\(65951\\) GPs with fluences above \\(\\sim 150\\ \\mathrm{Jy\\ \\mu s}\\) over a short period of \\(7.32\\mathrm{hr}\\). Within each of our four observations, we found that the GP detection rates are fairly constant, but that between epochs they differ by a factor of \\(\\sim\\!2\\). Similar changes were seen previously, and were suggested by Lundgren et al. (1995) to reflect changes in overall magnification of the scattering screens along the line of sight.\n\nThe changes in magnification are consistent with the pulse fluence distributions, which are power-law like at high fluence, but with a flattening at lower fluences; the distributions from the different epochs can be shifted to each other with a change in fluence scale. We noted that the fluence distributions are similar to what is expected for log-normal distributions, but found that the residual signals seen in the GP phase windows after removing the GPs we detected were larger than expected if the log-normal distribution continued also below our detection limit. Nevertheless, it suggests that with only somewhat more sensitive observations, it should be possible to get a fairly complete sampling of all GPs that contribute to the average flux, at least for the MP component.\n\nAnalyzing the pulse phase distributions, we confirm previous observations showing that the majority of GPs occur within very narrow phase windows. Furthermore, we observe no significant variations in the median flux distributions as a function of pulse phase. This suggests that it is the probability of observing a pulse that depends on pulse phase, not its energy, implying that the angle within which a pulse is emitted is much narrower than the rotational phase window, as expected if the plasma causing them is travelling highly relativistically (Bij et al., 2021; Lin et al., 2023).\n\nWith our high detection rates, we were able to investigate the distribution of time delays between successive bursts within the same pulse rotation. We detect a larger number than expected if all bursts were due to a Poissonian process, and infer that \\(\\sim\\!5\\%\\) of bursts come in groups of 2 or 3 causally related microbursts, with a typical separation in time of \\(\\sim\\!30\\ \\mu\\)s.\n\nAdditionally, our high sensitivity revealed weak echo features for individual bright pulses, which drift slightly but sig\n\nFigure 9: _Line plots_: SVD approximation of the MP pulse profile for all observations. In EK036 B, echoes are seen close to the profile\u2019s peak (see Lin et al., 2023 for more details). The profile for EK036 D shows multiple weak echoes up to \\(\\sim\\!300\\ \\mu\\)s. _Image_: The MP pulse stack for EK036 D, using a logarithmic colour scale to bring out faint features. Each pulse is aligned by correlating with the rotation with the brightest pulse in EK036 D (which is appears to be a simple single microburst) and then normalized by the intensity at time \\(0\\) (the black dashed line). The echoes appear to move out over time, as one can see by comparing the location of the most prominent faint echo with the dashed white vertical line near it (time is increasing both upwards and to the right in this image).\nnificantly even over our timescales of just a few hours. We infer that echo events are not rare.\n\nGiven our findings, we believe even more sensitive follow-up studies of the Crab Pulsar would be very useful. This would be possible using more small dishes (spaced sufficiently far apart that the Crab Nebula is well-resolved) and by recording a larger bandwidth.\n\n## Acknowledgements\n\nWe thank the anonymous referee for their comments, which improved the clarity of this manuscript. We thank the Toronto Scintillometry group, and in particular Nikhil Mahajan, for useful discussion on GP statistics. Computations were performed on the Niagara supercomputer at the SciNet HPC Consortium (Loken et al., 2010; Ponce et al., 2019). SciNet is funded by: the Canada Foundation for Innovation; the Government of Ontario; Ontario Research Fund - Research Excellence; and the University of Toronto. M.Hv.K. is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) via discovery and accelerator grants, and by a Killam Fellowship.\n\nThe European VLBI Network (EVN) is a joint facility of independent European, African, Asian, and North American radio astronomy institutes. Scientific results from data presented in this publication are derived from the following EVN project codes: EK036 A-D.\n\nastropy (Astropy Collaboration et al., 2013, 2018, 2022), Baseband (Van Kerkwijk et al., 2020), CALC10 (Ryan & Vandenberg, 1980), numpy (Harris et al., 2020), matplotlib (Hunter, 2007), pulsarbat (Mahajan & Lin, 2023), scipy (Virtanen et al., 2020), tempo2 (Hobbs & Edwards, 2012).\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2307.16362v2.pdf"}
{"id":2301.07687,"title":"Maybe, Maybe Not: A Survey on Uncertainty in Visualization","abstract":"Understanding and evaluating uncertainty play a key role in decision-making.\nWhen a viewer studies a visualization that demands inference, it is necessary\nthat uncertainty is portrayed in it. This paper showcases the importance of\nrepresenting uncertainty in visualizations. It provides an overview of\nuncertainty visualization and the challenges authors and viewers face when\nworking with such charts. I divide the visualization pipeline into four parts,\nnamely data collection, preprocessing, visualization, and inference, to\nevaluate how uncertainty impacts them. Next, I investigate the authors'\nmethodologies to process and design uncertainty. Finally, I contribute by\nexploring future paths for uncertainty visualization.","authors":"Krisha Mehta","published_date":"2022-12-14T00:07:06Z","link":"http:\/\/arxiv.org\/abs\/2301.07687v1","markdown":"# Maybe, Maybe Not: A Survey on Uncertainty in Visualization\n\n###### Abstract\n\nUnderstanding and evaluating uncertainty play a key role in decision-making. When a viewer studies a visualization that demands inference, it is necessary that uncertainty is portrayed in it. This paper showcases the importance of representing uncertainty in visualizations. It provides an overview of uncertainty visualization and the challenges authors and viewers face when working with such charts. I divide the visualization pipeline into four parts, namely data collection, preprocessing, visualization, and inference, to evaluate how uncertainty impacts them. Next, I investigate the authors' methodologies to process and design uncertainty. Finally, I contribute by exploring future paths for uncertainty visualization.\n\n## 1 Introduction\n\nWith a rise in the complexity and dimensionality of data, analyzing and modeling data becomes more challenging. When most of our decisions are data-driven, it becomes imperative that we know the nature of the data and the patterns it contains. As a result, analyzing the inherent uncertainty in the data is gaining more significance. In various fields, uncertainty can signify different things. For instance, data bias, random or systematic error, and statistical variance are all factors that contribute to data uncertainty. Without understanding the underlying uncertainty in our data, we cannot make accurate predictions. Similarly, to observe the true structure of our data and as well as identify patterns in it, we need to visualize it. Today, we can no longer undermine the significance of uncertainty nor ignore the importance of visualizations for data analysis.\n\nAs mentioned before, uncertainty is bound to exist whenever there is data. Therefore representation of uncertainty in data visualizations is crucial. Consider the example of hurricane path maps, as shown in Figure 1. The increase in the width of the predicted path with time is not due to an increase in the size of the hurricane. Instead, it is representing the inherent uncertainty in the data. In other words, the visualization indicates that compared to Friday, Sunday's hurricane path is more difficult to predict with any degree of accuracy.\n\nInformation tends to be withheld from the viewer when one does not portray uncertainty in the visualization. Therefore the viewer might occasionally be ignorant of this exclusion. This breach of trust can have significant consequences for both the author and the viewer. Given this significance, it is reasonable to assume that visualizations frequently include uncertainty. But how often do we encounter charts that represent uncertainty? How frequently do we check for bias in graphs that represent public surveys? As it turns out, not frequently.\n\nIn a recent study [9], 121 journalism articles, social science surveys, and economic estimates were examined. Out of 449 visualizations created for inference, the study demonstrates that only 14 accurately depict uncertainty. \"What's Going on in This Graph?\" is a New York Times (NYT) initiative to increase graphical literacy, especially among students. Different categories of charts, such as maps, parts-to-whole, and associations, are published for students to explore and analyze. When I looked into the distribution of these charts, I found that only 6 out of the 136 charts show uncertainty.\n\nThe question I ask is, do we actually examine uncertainty representations when we come across them in order to make decisions, or do we simply ignore them? Does uncertainty offer value or just clutter these visualizations? I try to investigate these questions in this paper. Visualizations are an integral part of newspapers, government bills, and business earnings reports to name a few. The public uses them to gain insights, spot trends, and make decisions.\n\nHence, when we visualize data, it becomes critical to support those visualizations with information about uncertainty. People frequently use visualizations to examine data and make observations. A lack of uncertainty representation could result in incorrect and erroneous interpretations. However, it can be challenging to visualize uncertainty. There are limited standard guidelines or protocols that authors can follow when they create such charts. Given these drawbacks, uncertainty visualization is considered one of the top research problems in data visualization [13]. With the help of a few uncertainty visualization examples, this survey studies how uncertainty contributes to every phase in visualization. Most research in this area focuses on creating charts with uncertainty and how viewers may perceive them. However, uncertainty is also influential in the other parts of the data visualization process, such as during data collection and preprocessing.\n\n**The objectives of this paper are as follows:**\n\n* Provide an entry point for anyone who wants to learn about uncertainty visualization\n* Delineate the significance of uncertainty visualizations\n* Explore how uncertainty influences every phase of the data visualization process\n\nFigure 1: An example chart for Matthew showing its five-day forecast track [5]\n* Understand the challenges authors and viewers face when interacting with it\n* Discuss the open problems and future research directions in the field\n\nThis work is divided into the following sections. Section 2 defines uncertainty and describes the relationship between uncertainty and visualization. In Section 3, I classify the data visualization pipeline into four phases, analyzing the involvement of uncertainty in each phase. The classification helps look at each phase individually, focusing on the challenges and bottlenecks authors and viewers face when working with uncertainty visualization. Finally, I study some state-of-the-art methods to visualize uncertainty and discuss future directions for research. I conclude the paper in Section 4.\n\n## 2 Uncertainty and Visualization\n\nVisualizations are incredibly important for examining, analyzing, and interpreting data in the era of big data. Visualizations are evidence that a picture really does say a thousand words. They aid viewers in seeing trends, background noise, and outliers. Asking the correct questions can be quite challenging when there is an abundance of data. Through visualizations, viewers can determine what questions the data can help answer. With improvements in hardware, software, and graphics theory, data visualizations are adopted more frequently and widely [26]. Viewers use visualizations to make decisions. However, making decisions and drawing observations by looking at visualizations can be complex due to the statistical variance and uncertainty present in these visualizations.\n\nAs mentioned previously, uncertainty can have different definitions based on different scenarios [3]. Broadly speaking, uncertainty is classified into two types, aleatory and epistemic. Aleatory uncertainty rises from random fluctuation and unknown outcomes when an experiment is run multiple times in a consistent environment. For example, in a drug trial, a participant's blood pressure can vary due to stress and anxiety. There might also be measurement errors in the sphygmomanometer. Aleatory uncertainty can be minimized by controlling individual factors and increasing the number of readings. Epistemic uncertainty, on the other hand, rises from a lack of knowledge, like predicting the outcome of the same experiment in a completely different, unknown environment. For example, predicting the effect of a drug on a new disease. Uncertainty can be measured, like risks but can also be unquantified, like bias. While aleatory uncertainty is more widely represented in the visualizations [25], both types can be represented with distribution graphs.\n\nUncertainty and visualizations are interweaved, and working with one often requires working with the other. In 1644, Michael Florent van Langren was one of the first researchers to use visualization for statistical analysis [25]. He used a 1D line graph to present the 12 known estimated longitudinal distances between Toledo and Rome, as shown in Figure 2. Instead of using a table to show this data, Langren used this graph to showcase the wide range of variation. Even though all the distances were over-estimated (actual distance, in longitude, is shown using the arrow), the graph remains classic in demonstrating the power of visualization.\n\nThe popular Anscombe's quartet [1] is a perfect example of how data with similar statistics might have a very different distribution which is observed when visualized. The quartet consists of four datasets with 11 points having nearly the same mean, sample variance, correlation, linear regression, and coefficient of determination. The four datasets may appear very similar to viewers looking at the data and the descriptive statistics. However, when one visualizes them, the difference in their distribution is very evident, as shown in Figure 3. Looking at data in tabular form may hide insightful observations and can lead to erroneous conclusions. Today, researchers across all domains use extensive libraries such as [12, 19, 22, 4, 11] to analyze data uncertainty.\n\nUsing visualizations to represent and study uncertainty in data is widely adopted. However, uncertainty in visualizations is often not communicated [9]. One of the earliest instances of uncertainty being presented can be traced back to the 18th century. Joseph Priestley, a British scientist, created \"A Chart of Biography\" to present the lifespans of famous people as shown in Figure 4. He used horizontal lines to portray the lifetime of about 2000 people and used dots before or after the lines to communicate uncertainty.\n\nVisualizations of uncertainty, however, are not common. Numerous factors influence why authors decide against visualizing uncertainty. Since they do not know all the information about the dataset, viewers may draw inaccurate conclusions in the absence of uncertainty representation. Nevertheless, introducing more uncertainty could also make the audience feel too overwhelmed to pay attention to it. The study of why visualizing uncertainty is rare is\n\nFigure 4: Priestley\u2019s Chart of Biography [21]\n\nFigure 3: Anscombe\u2019s quartet represents four datasets with similar statistics but very different distributions.\n\nFigure 2: Langren\u2019s line graph is one of the first visualizations to present uncertainty\nstill in its early stages. In the section that follows, I go through each of these issues in more detail and look at how uncertainty affects every stage of data visualization.\n\n## 3 Uncertainty in Visualization\n\nPrevious works in the field have attempted to classify the data visualization process differently. [14] considers sampling, modeling, visualization, and decision-making as the primary sources of uncertainty. This paper follows a similar classification. I divide the visualization pipeline into **data collection, preprocessing, visualization and inference** as shown in Figure 5. Pang et al. [18] classify the process into data collection, derivation, and visualization and discuss how uncertainty is introduced in each stage.\n\nUnder the data collection phase, the paper mainly discusses the uncertainty added due to measurement errors. However, there are other sources, such as bias and sampling error, that the paper fails to describe. I investigate these uncertainties in Section 3.3.1. The authors then discuss the change data undergoes when it is preprocessed. These changes include converting one unit to another, rescaling, and resampling. However, they do not mention other vital issues such as missing data, approximation, and interpolation that I examine in Section 3.3.2. Next, the authors highlight how uncertainty also influences the data visualization stage itself. They mainly focus on radiosity and volume rendering, while this paper delves more into 2D visualizations. Finally, I explore how viewers infer these visualizations and the challenges they face while making a decision from these charts.\n\nUncertainty is presented at every phase of this classification. However, understanding and evaluating uncertainty in each of these phases is unique. Therefore, authors are required to approach these uncertainties based on their type and complexity, understand their abstraction, and then present them in visualizations in a way that is easy to grasp.\n\nGiven the interdisciplinary nature of visualizations, the format, quantity, and type of data used to create them vary immensely. Different data implies different data collection processes and uncertainties. Uncertainty is intertwined with data acquisition and can arise from random variables and modeling errors [14]. Pang et al. [18] explain how almost all acquired data has statistical variation. Collected data can have errors, bias, and variance. [23] study how bias can be introduced during the process of collecting data. Datasets are prone to various biases that include but are not limited to selection bias, volunteer bias, admission bias, survivor bias, and misclassification bias.\n\nIt is imperative that datasets resemble the true population as closely as possible. Data can also contain different types of errors, such as coverage error, sampling error, nonresponse error, and measurement error [7]. Missing data points is another common challenge researchers face during data collection.\n\nCorrecting these errors is not always possible, but they can be mentioned in the visualization to inform the viewer. However, uncertainty is often ignored when authors create visualizations. Other times this uncertainty in data is not communicated to them [9]. For example, when I analyze a piece called \"Free Speech\" (as shown in Figure 6) published in the What's Going On in This Graph section of the NYT. [16], we can see how information about uncertainty from the data source is not mentioned directly in the graph. The bars of the graph do not sum to 100 percent since they are missing the no-response segment. The article mentions that the margin of error for the sample is +\/- 3.1%, but the graph makes no mention of it.\n\nEfforts are being made by researchers to improve the way uncertainty in the data collection phase is captured, processed, and communicated. Athawale et al. [2] propose using statistical summary maps to represent uncertainty in scalar field data caused by data acquisition.\n\n### _Data Preprocessing_\n\nRaw data is imperfect and can consist of noise and error. Once data is collected, it undergoes processing for accuracy and standardization. However, this phase adds uncertainty to the data that may not be immediately evident. For example, fundamental transformations like rounding off values, converting data from one unit to another, rescaling, resampling, and quantizing can add uncertainty [1]. Even though this might seem minor, the impact can be significant. For example, based on whether we take the value of pi as 22\/7(3.14285) or 3.14159, the area of the Sun can vary by a difference of 239x106 sq. miles.\n\nA significant setback that most datasets suffer from is missing data. Data can have missing values for many reasons, such as instrument malfunction, incomplete observations, and lost data. Missing values leave a gap in the dataset, which makes room for uncertainty. Working with such uncertainty requires the authors to take extra measures during preprocessing. Authors attempt to find close estimates of the missing values to provide the viewers with a complete picture. One way to tackle this problem is by deleting the complete entry that has the missing value. This leads to a loss of data and insights. Another option is to make an educated guess about the missing value. However, this is highly unreliable and often not recommended. Using interpolation, imputation, or other techniques can induce errors [3].\n\nSometimes, authors choose to encode these estimated values differently in their designs to inform the viewer about the gap in the dataset. However, how authors choose to visualize this encoding becomes very influential in how viewers perceive these graphs. Whether authors highlight, downplay, annotate or remove the missing values determines how much confidence and credibility the\n\nFigure 5: The data visualization process divided into four stages to show how uncertainty affects each stage\n\nFigure 6: Free Speech, a graph by the New York Times based on a national poll including 1,507 U.S residents [16]\nviewer shows in the visualization [24].\n\n### Visualization Creation\n\nSince uncertainty isgrained in different parts of the data collection process, it is not easy to identify and control it. However, once the data is cleaned and processed, the authors face a new problem. Creating visualizations requires authors to make various decisions on behalf of the viewer. Authors are expected to choose the type of visualization based on data type, which may lead them to choose the scaling, sorting, ordering, and aesthetics [27]. Compelling visualizations are accurate and suggest an understanding and interpretation of data. Hence, it is the author's responsibility to analyze data correctly before creating any visualizations. Midway [15] describes ten design principles authors can follow to create charts. However, none of those principles discuss how uncertainty can be presented. Creating effective visualizations is hard. However, when we add uncertainty representation, the task becomes much more complex [17]. The data visualization community of researchers, designers, journalists, etc., has been reluctant to add uncertainty to their charts. Authors are aware of how significant uncertainty visualization is. Yet, they choose to exclude uncertainty when they design their charts for various reasons discussed below.\n\n#### 3.2.1 Uncertainty is hard to represent\n\nThough data is replete with uncertainty, the difficulty lies in determining if it should be represented and how. If the uncertainty has no direct relationship to the goal of the visualization, then it may not be included in the visualization. But this is not a conclusion that authors can quickly draw. The rise in techniques of visualizing uncertainty can make it harder for authors to decide which one to choose from. One of the biggest challenges in visualizing uncertainty is discovering and communicating the relationship and impact that the uncertainty has on the data. Data visualization is often a preferred choice for analysis due to its ability to present high-dimensional data. However, uncertainty also has dimensions, generally classified into scalar, vector, and tensor [20]. While scalar and vector fields of uncertainty are depicted in charts, tensor fields are often avoided. Mapping these dimensions of uncertainty along with the dimensions of data is challenging and often overlooked when creating charts. Instead, authors tend to simplify uncertainty to align with the dimensionality of the data.\n\n#### 3.2.2 Uncertainty is hard to calculate and verify\n\nAnother reason why authors choose to exclude uncertainty from their charts is that calculating uncertainty is complex [9]. It is well known that even mathematicians and statisticians sometimes find it challenging to calculate the error or variance in a dataset. Verifying if the presented uncertainty is correct is challenging. Moreover, if the authors make an error while designing their charts, they end up providing wrong information to the viewers and losing their trust.\n\n#### 3.2.3 Viewers may be overwhelmed\n\n[9] explains why the inclusion of uncertainty in graphs is not widely adopted. Authors believe that uncertainty can be challenging for the viewers to perceive and understand. As a result, viewers may choose to either look at an alternative graph that does not contain any uncertainty representation or overlook the uncertainty in their graph altogether.\n\n#### 3.2.4 Uncertainty can add clutter to the visualization\n\nAuthors can be unsure of how effective communicating uncertainty is. They also worry about adding more information to an already visually complex visualization. For many authors, the goal of a chart is to express a signal [9] that can be useful to their viewers. This signal tends to present a single point or a single source of truth. Uncertainty tends to challenge that notion by obfuscating the signal. Additionally, expressing the intricacy of uncertainty through a visual abstraction is challenging. The dimensionality of the data also plays a vital role in deciding whether uncertainty should be represented or not. An increase in the dimensionality of data makes it harder for the human visual system to perceive it effectively. Sometimes even two-dimensional charts can be overwhelming for the viewer. In such a case, representing uncertainty adds visual overload [20].\n\n### Visualization Inference\n\nUncertainty is hard to understand and analyze. When faced with perceiving an uncertain visualization, viewers can get confused or derive inaccurate information from it. One easy method viewers tend to use is to ignore the uncertainty in the graph altogether. Another way is to substitute tricky calculations with easy ones or use heuristics to make decisions. However, this may not always give a correct observation. The most common approach to show uncertainty is by using box plots and error bars. Though widely used, viewers may find them challenging to analyze [6]. Sometimes visualizing uncertainty as frequency instead of distribution provide a better understanding.\n\nCurrently, research is being done to create visualizations that help understand uncertainty more intuitively. For example, hypothetical outcome plots (HOPs) represent uncertainty by animating a finite set of individual draws [10]. This approach expects no prior knowledge of the domain from the viewer. However, using HOPs in physical media might be challenging. Bubble treemaps [8] are another approach for visualizing uncertainty. These circular treemaps encode additional information about uncertainty by allocating additional space for visuals.\n\nWhile uncertainty is still underrepresented in visualizations, more researchers are slowly adding it to their designs. One of the significant setbacks in uncertainty visualizations for authors is calculating uncertainty, while for viewers, it is graphical literacy. Efforts can be taken to increase this literacy through different programs gradually. Furthermore, work should be done to understand what visualization type best suits a given uncertainty type. This relationship can also depend on the type of data being represented and the target audience viewing the graph. For example, it is necessary for graphs published in newspapers and reports to be easily understandable by the public. Hence, studies focusing on visualizing uncertainty with no prior knowledge or information can be very insightful.\n\n## 4 Conclusion\n\nUncertainty visualization is one of the most complex research areas in data visualization today. This work provided an overview of uncertainty visualization and the relationship between uncertainty and visualization. I divided the visualization pipeline into four phases and surveyed papers to study how uncertainty interacts with each phase of the process. The work also investigated why the representation of uncertainty is not widely practiced by the data visualization community and the challenges viewers face when inferring from such a graph. Lastly, I discussed a few state-of-the-art methods to design uncertainty visualization and offered a glance into the interesting future research this field has to offer.\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2301.07687v1.pdf"}
{"id":2309.09088,"title":"Enhancing GAN-Based Vocoders with Contrastive Learning Under\n  Data-limited Condition","abstract":"Vocoder models have recently achieved substantial progress in generating\nauthentic audio comparable to human quality while significantly reducing memory\nrequirement and inference time. However, these data-hungry generative models\nrequire large-scale audio data for learning good representations. In this\npaper, we apply contrastive learning methods in training the vocoder to improve\nthe perceptual quality of the vocoder without modifying its architecture or\nadding more data. We design an auxiliary task with mel-spectrogram contrastive\nlearning to enhance the utterance-level quality of the vocoder model under\ndata-limited conditions. We also extend the task to include waveforms to\nimprove the multi-modality comprehension of the model and address the\ndiscriminator overfitting problem. We optimize the additional task\nsimultaneously with GAN training objectives. Our results show that the tasks\nimprove model performance substantially in data-limited settings.","authors":"Haoming Guo, Seth Z. Zhao, Jiachen Lian, Gopala Anumanchipalli, Gerald Friedland","published_date":"2023-09-16T20:04:16Z","link":"http:\/\/arxiv.org\/abs\/2309.09088v2","markdown":"# Enhancing Gan-Based Vocoders with Contrastive Learning Under Data-Limited Condition\n\n###### Abstract\n\nVocoder models have recently achieved substantial progress in generating authentic audio comparable to human quality while significantly reducing memory requirement and inference time. However, these data-hungry generative models require large-scale audio data for learning good representations. In this paper, we apply contrastive learning methods in training the vocoder to improve the perceptual quality of the vocoder without modifying its architecture or adding more data. We design an auxiliary task with mel-spectrogram contrastive learning to enhance the utterance-level quality of the vocoder model under data-limited conditions. We also extend the task to include waveforms to improve the multi-modality comprehension of the model and address the discriminator overfitting problem. We optimize the additional task simultaneously with GAN training objectives. Our result shows that the tasks improve model performance substantially in data-limited settings. Our analysis based on the result indicates that the proposed design successfully alleviates discriminator overfitting and produces audio of higher fidelity.\n\nHaoming Guo, Seth Z. Zhao, Jiachen Lian, Gopala Anumanchipalli, Gerald Friedland University of California, Berkeley\n\n+\nFootnote \u2020: This paper is based on Haoming\u2019s thesis [1] at University of California, Berkeley.\n\n**Index Terms**: GAN, self-supervised learning, vocoder\n\n## 1 Introduction\n\nGenerative Adversarial Networks (GANs) [2] have been widely used in vocoders and have achieved the state-of-the-art in the domain [3, 4, 5]. However, training GAN vocoders still meets two challenges, data insufficiency and discriminator overfitting.\n\nIn the realm of single-speaker speech synthesis, the limited size of available datasets poses a significant challenge. To enhance the performance of vocoders operating under such constraints, we propose the use of unsupervised learning techniques to extract additional self-supervised signals for training. Self-supervised learning (SSL) methods have demonstrated efficacy in a diverse array of speech domains, including representation learning [6, 7, 8, 9, 10], synthesis [11, 12, 13, 14], and multi-modality [15, 16]. Drawing on the exceptional transfer learning capabilities of SSL, we seek to harness this power in the realm of Vocoder modeling, focusing specifically on the application of contrastive learning. Although contrastive learning has been explored in the context of speech recognition [6], we are unaware of any previous efforts to apply this approach to Vocoder modeling. In this work, our aim is to leverage contrastive learning as an auxiliary task to enhance the vocoding performance of GAN generators under data-limited conditions.\n\nThe second challenge, discriminator overfitting, is also shown to be crucial, especially on small dataset [17, 18, 19], and the convergence of GAN also critically depends on the quality of discriminators [20]. Contrastive learning on the discriminator has been proved to alleviate this problem in image generation [21], and the method, in general, is also shown to increase model's performance and robustness on vision and language tasks [22, 23, 24, 25]. However, in speech synthesis, a naive approach of mel-spectrogram contrastive learning will only involve the generator, which encodes mel-spectrograms, but not the discriminator, which encodes the waveform. Therefore, we propose to extend the training to the discriminator by using a multi-modal contrastive task between mel-spectrograms and waveforms.\n\nOur contributions can be summarized as the following.\n\n1. We propose a contrastive learning task with masked mel-spectrograms to improve the performance on limited data.\n2. We design a novel contrastive learning task of matching mel-spectrogram to waveforms to regularize the discriminator and improve the perceptual quality of the generator.\n3. We implement a framework for integrating contrastive learning into the GAN training pipeline.\n4. We provide experimental results and in-depth analysis of the methods' effectiveness compared to the baseline.\n\n## 2 Methods\n\nIn this section, we first introduce the auxiliary contrastive task that we have designed for the GAN vocoder model. Subsequently, we explicate the details of how we modified the task to train both the generator and the discriminator of the\nvocoder model. Finally, we illustrate our proposed training framework, which synergizes the contrastive task with GAN objectives. It is worth noting that we have utilized the same model architecture as HiFi-GAN [4]. However, it is pertinent to mention that our method can be applied to other GAN frameworks for vocoders as well.\n\n### Mel-spectrogram Contrastive Learning\n\nIn our GAN model, the generator takes a mel-spectrogram as input and outputs a raw waveform through a stack of convolutional layers. We use a learnable feed-forward layer to project the features of the convolutional layers onto a latent space \\(R^{D}\\), where elements of similar semantics are close to each other through contrastive learning. For each anchor in a batch of \\(N\\) samples, we apply masking on randomly selected intervals in time and frequency to create a positive sample, while all other \\((N-1)\\) input samples and \\((N-1)\\) masked samples are used as negative samples. Together, the method results in \\(1\\) positive pair and \\(2(N-1)\\) negative pairs in the batch. We then adapt the InfoNCE loss [26] used in CLIP [27] for our loss function as follows:\n\n\\[\\mathcal{L}_{cl}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\log\\frac{\\text{exp}(\\tau \\mathbf{v}_{i}\\cdot\\mathbf{v}_{k})}{\\sum_{j=1;i\\neq j}^{2N}\\text{exp}(\\tau \\mathbf{v}_{i}\\cdot\\mathbf{v}_{j}))}\\right) \\tag{1}\\]\n\nwhere \\(\\mathbf{v}_{k}\\in R^{D}\\) is the masked sample from \\(\\mathbf{v}_{i}\\in R^{D}\\) and \\(\\tau\\) is a temperature parameter. This method is shown in Fig. 1.\n\n### Mel-spectrogram Waveform Contrastive Learning\n\nIn addition to training solely the generator, we propose a novel task that involves contrastive spectrogram-waveform matching. This task serves to train both the generator and the discriminators, promoting rich semantic representation and preventing overfitting of the discriminators to the real or fake classification. The method is illustrated in Fig. 2. For a batch of pairs of mel-spectrograms and waveforms, we assign the labels of the true pairs to be positive and those of the other pairs to be negative, resulting in \\(N\\) positive pairs and \\(N(N-1)\\) negative pairs in a batch of \\(N\\) samples. We use the backbone of the generator to encode the mel-spectrogram and the backbone of the discriminator to encode the waveform. Similar to the method in section 2.1, we use two separate feed-forward layers to project each encoded feature to the same latent dimension \\(R^{D}\\). Then, we perform the modified loss function\n\n\\[\\mathcal{L}_{cl}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\log\\frac{\\text{exp}(\\tau \\mathbf{v}_{i}\\cdot\\mathbf{w}_{i})}{\\sum_{j=1;i\\neq j}^{N}\\text{exp}(\\tau \\mathbf{v}_{i}\\cdot\\mathbf{w}_{j}))}\\right) \\tag{2}\\]\n\nwhere \\(\\mathbf{w}_{i}\\in R^{D}\\) is the latent embedding of the waveform corresponding to the \\(i\\)th mel-spectrogram, \\(\\mathbf{v}_{i}\\in R^{D}\\) is the latent embedding of the \\(i\\)th mel-spectrogram, and \\(\\tau\\) is a temperature parameter. HiFi-GAN contains multiple discriminators, so we calculate a contrastive loss between the mel-spectrogram embedding and each of the waveform embeddings and sum them up. For simplicity, we refer them as one discriminator in this paper unless otherwise mentioned.\n\n### Multi-tasking Framework\n\nTo integrate contrastive learning with GAN tasks, we adopt a multi-tasking framework that makes auxiliary tasks a joint optimization objective with original learning goals [28]. As illustrated in Fig. 3, we create additional heads for the training\n\nFigure 1: **Illustration of Mel-spectrogram Contrastive Learning.** The Mel Encoder is the backbone of the generator. This method only trains the generator in a GAN framework.\n\nFigure 2: **Illustration of Mel-spectrogram & Waveform Contrastive Learning.** The Mel Encoder is the backbone of the generator, and the Wave Encoder is the backbone of the discriminator. Therefore, this method trains both the generator and discriminator.\ngenerator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:\n\n\\[\\mathcal{L}_{G}=\\mathcal{L}_{adv}+\\lambda_{fm}\\mathcal{L}_{fm}+\\lambda_{mel} \\mathcal{L}_{mel}+\\lambda_{el}\\mathcal{L}_{cl} \\tag{3}\\]\n\n\\[\\mathcal{L}_{D}=\\mathcal{L}_{adv}+\\mathcal{I}_{disc}\\lambda_{cl}\\mathcal{L}_{cl} \\tag{4}\\]\n\nwhere \\(\\mathcal{L}_{G}\\) is the total loss for the generator and \\(\\mathcal{L}_{D}\\) is the total loss for the discriminator. \\(\\mathcal{L}_{adv}\\) is the adversarial loss, \\(\\mathcal{L}_{fm}\\) is the feature matching loss, and \\(\\mathcal{L}_{mel}\\) is the mel-spectrogram reconstruction loss in the original HiFi-GAN training pipeline. \\(\\mathcal{L}_{mel}\\) can be either of the contrastive loss described in section 2.1 or 2.2, and \\(\\mathcal{I}_{disc}\\) is an indicator of whether the latter is used. Each loss is weighted with a \\(\\lambda\\) coefficient which can be set as hyperparameters. We use a \\(\\lambda_{fm}\\) of 2, \\(\\lambda_{mel}\\) of 45 from the HiFi-GAN setting [4] and a \\(\\lambda_{cl}\\) of 1.\n\n## 3 Experiments\n\n### Experimental Setting\n\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\n\n#### 3.1.1 Dataset\n\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also used in other vocoder works like HiFi-GAN [4]. LJSpeech is a public single-speaker dataset with 13100 short English audio clips whose durations span from 1 second to 10 seconds. We use the default data split with 12950 training samples and 150 validation samples. We use the same preprocessing configurations with HiFi-GAN, including 80 bands of mel-spectrograms as input and FFT size of 1024, window size of 1024, and hop size of 256 for conversion from waveform to mel-spectrograms.[4]\n\n#### 3.1.2 Implementation details\n\nFor experimental comparison on audio quality, we choose the most powerful HiFi-GAN V1 and the most lightweight HiFi-GAN V3 as the baseline methods, and we use the same model architecture as the backbone to apply the contrastive tasks described in section 2.1 and 2.2. Under the multi-tasking framework, we train HiFi-GAN along with the contrastive learning methods with a batch size of 16, an AdamW optimizer, and a learning rate of 0.0002. For the following experiments on the full dataset, all models are trained for 400k steps (about 96 hours) on one Nvidia TITAN RTX GPU. The experiments on 20% of the dataset train for 300k steps (about 72 hours) on the same device, and those on 4% of the dataset train for 200k steps. The model inference time on GPU is about 70ms for V1 models and 32ms for V3 models.\n\n#### 3.1.3 Evaluation metrics\n\nTo objectively evaluate our models compared to the baseline, we measure the mean average error (MAE) and mel-cepstral distortion (MCD) [30] on mel-spectrograms. On both metrics, lower scores indicate closer alignment with the ground truth. We also include a 5-scale mean opinion score (MOS) on audio quality as a subjective evaluation performed on 50 samples excluded from the training set.\n\n\\begin{table}\n\\begin{tabular}{l|c c|c} \\hline \\hline Model & MAE & MCD & MOS (CI) \\\\ \\hline Ground Truth & - & - & 4.32 (\\(\\pm 0.05\\)) \\\\ \\hline HiFi-GAN V1 & **0.111** & **4.203** & **4.21** (\\(\\pm 0.05\\)) \\\\ + Mel CL & 0.114 & 4.289 & 4.18 (\\(\\pm 0.06\\)) \\\\ + Mel-Wave CL & 0.113 & 4.228 & 4.20 (\\(\\pm 0.05\\)) \\\\ \\hline HiFi-GAN V3 & **0.203** & 7.786 & 4.10 (\\(\\pm 0.05\\)) \\\\ + Mel CL & 0.204 & 7.766 & **4.13** (\\(\\pm 0.07\\)) \\\\ + Mel-Wave CL & **0.203** & **7.723** & 4.09 (\\(\\pm 0.06\\)) \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: Objective and subjective evaluation results for models with mel-spectrogram contrastive loss (Mel CL) and mel-spectrogram contrastive loss (Mel-Wave CL). Models are trained on the full training set. CI is 95% confidence interval of the MOS score.\n\nFigure 3: **Illustration of our multi-tasking frameworks.** GAN-based Vocoder models [3, 4] follow an adversarial network (**top**) consisting of a generator that generates raw waveforms from mel-spectrograms and a discriminator that aims to distinguish real from generated waveform samples. To incorporate the auxiliary contrastive learning task, we propose a multi-tasking (**bottom**) framework, which we set the contrastive task as additional learning objectives along with the original GAN optimization objectives. This framework applies to both contrastive learning methods described in section 2.1 and 2.2.\n### Results\n\nWe present the results of models trained on full data with the multi-tasking framework in Table 1. Below, we refer Mel CL as the mel-spectrogram contrastive learning in section 2.1, and Mel-Wave CL as the mel-spectrogram waveform contrastive learning in section 2.2. For V1 models, the baseline performs slightly better than the proposed methods by margins of 0.02 on MAE, 0.025 on MCD, and 0.01 on MOS. For V3 models, on the objective tests, we observe that the model trained with mel-spectrogram contrastive loss has comparable performance with the baseline, while the one trained with mel-spectrogram waveform contrastive loss achieves the highest scores on both metrics. The results show that our proposed methods have at least comparable performance to the baseline HiFi-GAN when training on the full dataset. On the subjective tests, the V3 model with Mel CL achieves the highest MOS score, 0.03 above the V3 baseline. The model with Mel-Wave CL has a similar MOS score with the baseline on the full dataset. Overall, when trained on the full dataset, the proposed methods have limited gains on top of the baseline.\n\nTo investigate how each model performs under data limitation, we train the three models on 20% of the dataset and evaluate them with the same validation set. We present the results in Table 2. With less data, the baseline HiFi-GAN V3 suffers a significant performance degradation across all metrics, including 0.371 on MCD and 0.22 on MOS. Meanwhile, the V3 model trained with Mel CL experiences an increase of 0.194 on MCD and a drop of 0.18 on MOS. The V3 model trained with Mel-Wave CL has an increase of 0.251 on MCD and a drop of only 0.05 on MOS. It suggests Mel-Wave CL is most resistant to data insufficiency. The two proposed methods have comparable scores on the objective evaluation, but the model with Mel-Wave CL obtains a significantly higher score on the subjective test, 0.16 higher than the V3 baseline. The findings align with our hypothesized alleviation of discriminator overfitting by Mel-Wave CL, which is a more severe problem on the small training dataset. Both of the proposed methods perform substantially better than the baseline by 0.07 and 0.16 respectively.\n\nA similar trend exists in the HiFi-GAN V1 experiments, where Mel-Wave CL achieves the best scores and the least performance drop on all metrics. One slightly surprising finding is that the larger model V1 often experiences a smaller performance drop compared to the smaller model V3 when trained on 20% data. Typically, a larger model is expected to be more prone to overfitting when trained on less data, which should lead to a larger performance drop. In this specific case, however, HiFi-GAN V1 has a larger generator but the same discriminator as HiFi-GAN V3 [4], which is our suspected reason for the finding. Overall, the results show the benefits of additional supervision signals from contrastive learning in data-limited situations and the superior performance of Mel-Wave CL on a small dataset.\n\n## 4 Conclusion\n\nThis paper describes our proposed contrastive learning framework to improve GAN vocoders. Our results show the legacy of using contrastive learning as an auxiliary task that facilitates vocoder training without adding more data or modifying model architecture. We demonstrate that the proposed framework is significant especially when training on limited data by extracting additional supervision signals and reducing discriminator overfitting.\n\nFor future work, we plan to repeat the experiments on different model architectures and datasets to test our method's generalizability. In particular, we want to test its extension to multi-speaker datasets, another domain where data insufficiency is critical. We will also explore other metrics to evaluate the discriminator overfitting problem more holistically.\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2309.09088v2.pdf"}
{"id":2307.16404,"title":"Nonvolatile Magneto-Thermal Switching in MgB2","abstract":"Ongoing research explores thermal switching materials to control heat flow.\nSpecifically, there has been interest in magneto-thermal switching (MTS)\nmaterials based on superconductors, which only exhibited switching behavior\nwhen a magnetic field was applied. However, a recent report highlighted\nnonvolatile MTS in commercial Sn-Pb solders, attributed to magnetic flux\ntrapping. In this study, we focused on flux trapping in a type-II\nsuperconductor MgB2. Magnetization and thermal conductivity measurements under\nmagnetic fields were conducted on polycrystalline MgB2. We confirmed that\nmagnetic flux was indeed trapped in MgB2 even after demagnetization.\nAdditionally, we observed nonvolatile MTS in MgB2 as well as Sn-Pb solders.\nThese results suggest that the nonvolatile MTS may be a widespread\ncharacteristic of superconducting materials with flux trapping.","authors":"Hiroto Arima, Yoshikazu Mizuguchi","published_date":"2023-07-31T04:59:19Z","link":"http:\/\/arxiv.org\/abs\/2307.16404v1","markdown":"# Nonvolatile Magneto-Thermal Switching in MgB\\({}_{2}\\)\n\n###### Abstract\n\nOngoing research explores thermal switching materials to control heat flow. Specifically, there has been interest in magneto-thermal switching (MTS) materials based on superconductors, which only exhibited switching behavior when a magnetic field was applied. However, a recent report highlighted nonvolatile MTS in commercial Sn-Pb solders, attributed to magnetic flux trapping. In this study, we focused on flux trapping in a type-II superconductor MgB\\({}_{2}\\). Magnetization and thermal conductivity measurements under magnetic fields were conducted on polycrystalline MgB\\({}_{2}\\). We confirmed that magnetic flux was indeed trapped in MgB\\({}_{2}\\) even after demagnetization. Additionally, we observed nonvolatile MTS in MgB\\({}_{2}\\) as well as Sn-Pb solders. These results suggest that the nonvolatile MTS may be a widespread characteristic of superconducting materials with flux trapping.\nThe recent advancements in electronic device technology have spurred research into thermal switching materials, which enable control of heat flow through external parameters[1; 2]. Recent progress has been made in the development of thermal switching materials, where the control of thermal conductivity (\\(\\kappa\\)) is achieved through the application of electric[3] and magnetic fields[4; 5]. Among these materials, superconductors have received particular attention in magneto-thermal switching (MTS) research [6; 7]. Here, we introduce an index to assess the effectiveness of MTS known as the MTS ratio (MTSR). The MTSR is calculated as the ratio of the change in \\(\\kappa\\) between the presence and absence of a magnetic field. The MTSR is expressed as [\\(\\kappa(H)\\) - \\(\\kappa(0\\) Oe)] \/ \\(\\kappa(0\\) Oe). It is widely recognized that, in the normal state, heat is carried by charge carriers, whereas in the superconducting state, heat transport by Cooper pairs is negligible. Consequently, the phase transition from the superconducting state to the normal state results in an increase in \\(\\kappa\\). Recent studies reported MTSR of 650 % for Nb[6] and over 1000 % for high purity 5N-Pb[7]. However, previously reported MTS using superconductors had a limitation, \\(\\kappa(H)\\) returned to its initial value \\(\\kappa(0\\) Oe) when the magnetic field was reduced to zero, indicating that MTS was effective only in the presence of a magnetic field. In the most recent discovery reported in arXiv: 2307.05957 (preprint)[8], a nonvolatile MTS, which retains the altered \\(\\kappa(H)\\) even when the magnetic field is completely removed, has been identified. Surprisingly, this nonvolatile MTS material was discovered in commercially available Sn-Pb solders. The nonvolatile MTSR is defined as [\\(\\kappa\\) (0 Oe, demagnetized) - \\(\\kappa(0\\) Oe, initial)]\/\\(\\kappa\\) (0 Oe, initial), and it has been determined that the nonvolatile MTSR of flux-core-free Sn45-Pb55 solder was 150 %. The origin of nonvolatile MTS in Sn-Pb solders is attributed to the presence of magnetic flux trapped in the solder even after the applied magnetic field is removed, resulting in a partial loss of superconducting bulkiness at \\(H=0\\) Oe. While magnetic flux trapping in Sn-Pb solders is relatively rare due to both Sn and Pb being type-I superconductors, the magnetic flux trap after demagnetization is commonly observed in type-II superconductor samples.\n\nIn this study, our primary focus is on exploring the occurrence of nonvolatile MTS in type-II superconductors, with particular emphasis on MgB\\({}_{2}\\), which has been studied for its flux trapping properties[9; 10]. MgB\\({}_{2}\\) was discovered in 2001 and stands out among intermetallic superconductors for having the highest superconducting transition temperature \\(T_{\\rm SC}\\sim 39\\) K under ambient pressure [11]. This compound exhibits a unique characteristic as a multi-gap superconductor, with multiple conduction bands and independent superconducting gaps present on the Fermi surface[12; 13]. Shortly after its discovery, it was observed that grain boundaries in MgB\\({}_{2}\\) could\nserve as effective pinning centers, contributing to high critical current density (\\(J_{\\rm c}\\)) in superconducting materials[14; 15; 16; 17]. Consequently, extensive research has been conducted to investigate the relationship between magnetic flux trapping at grain boundaries and \\(J_{\\rm c}\\).\n\nUntil now, the association between magnetic flux trapping and nonvolatile MTS has solely been reported in Sn-Pb solders. To gain a deeper understanding of this phenomenon, it is essential to explore other materials. MgB\\({}_{2}\\) presents an appealing platform for investigating nonvolatile MTS due to the existing body of research on flux trapping effects at grain boundaries[9]. While previous studies have conducted thermal conductivity measurements under magnetic field on MgB\\({}_{2}\\)[18; 19], there has been no specific focus on nonvolatile MTS. In this study, magnetization measurements and thermal conductivity measurements under magnetic fields were conducted for commercial MgB\\({}_{2}\\). Notably, nonvolatile MTS was also observed in MgB\\({}_{2}\\).\n\nPolycrystalline MgB\\({}_{2}\\) used in this experiment was a commercially available powder sample (99%, KOJUNDO). Before the measurements, the powder sample underwent a high-pressure sintering process. In this experiment, high-pressure sintering was performed at relatively low temperatures to suppress grain growth. The specific conditions for this high-pressure sintering entailed a pressure of 3 GPa and a temperature of 400 \\({}^{\\circ}\\)C, sustained around 30 minutes. The crystal structure was examined through powder X-ray diffraction employing the Cu-K\\(\\alpha\\) radiation using the \\(\\theta\\)-2\\(\\theta\\) method (Miniflex-600 RIGAKU). The Rietveld refinement of the XRD data was performed using the RIETAN-FP package[20]. The scanning electron microscope (SEM, TM3030, Hitachi High-Tech) was used for microstructure observation. The thermal conductivity was measured using a Physical Property Measurement System (PPMS, Quantum Design) equipped with a thermal transport option (TTO). The measurement employed a four-probe steady-state method, incorporating a heater, two thermometers, and a base-temperature terminal. For the thermal conductivity measurements of MgB\\({}_{2}\\), a cylindrical sample with a diameter of 4.61 mm and a height of 4.10 mm was employed. The magnetization measurements were carried out using a superconducting quantum interference device (SQUID) magnetometry technique, employing the Magnetic Property Measurement System (MPMS3, Quantum Design) in a VSM (vibrating sample magnetometry) mode. In this experiment, thermal conductivity measurements were conducted on a high-pressure sintered MgB\\({}_{2}\\) sample within a week. Subsequently, the sample was crushed, and further analyses including XRD and magnetization measurements, and SEM imaging were performed. All the experiments were carried out using the same batch of sample.\n\nFigure 1 illustrates the XRD patterns obtained from the high-pressure sintered MgB\\({}_{2}\\) sample.\nIn the high-pressure sintered sample, the presence of MgB\\({}_{4}\\) and MgO were detected as an impurity, alongside the main MgB\\({}_{2}\\) peaks. The reliability factor, denoted as \\(R_{\\rm wp}\\), was determined to be \\(R_{\\rm wp}=3.7\\) %, and the goodness-of-fit indicator, represented by \\(S\\), was calculated as \\(S=1.8\\). The results of Rietveld refinement indicated that the sample composition consisted of approximately 90 % MgB\\({}_{2}\\), 5 % MgB\\({}_{4}\\), and 5% MgO. The as-purchased MgB\\({}_{2}\\) powder contained a similar amount of MgB\\({}_{4}\\) and MgO. The discrepancy with the nominal purity of 99% MgB\\({}_{2}\\) is likely a result of certain compounds not being accounted for in the chemical analysis. Furthermore, the XRD profile exhibited broadening, implying lattice strain induced by the high-pressure sintering process.\n\nFigure 2 shows the SEM image of the high-pressure sintered MgB\\({}_{2}\\). Numerous granular grains were observed in the structure of the high-pressure sintered MgB\\({}_{2}\\), with the majority of the grain sizes measuring less than approximately 5 \\(\\mu\\)m.\n\nFigure 3 (a) illustrates the temperature dependence of the magnetization \\(4\\pi M\\) measured at 10 Oe under both zero-field-cooling (ZFC) and field-cooling (FC) conditions. The magnetization measurement under ZFC demonstrates a large shielding signal below \\(T_{\\rm SC}\\sim 39\\) K. The difference between ZFC and FC measurements is a characteristic behavior commonly observed in type-II superconductors. The temperature dependence of \\(4\\pi M\\) exhibited broadening, which has also been reported in previous studies on high-pressure sintered MgB\\({}_{2}\\)[17]. The exact cause of this broadening is not yet clear, but the inhomogeneity of the crystals likely plays a role, as suggested by the broad profile observed in the XRD measurement. Figure 3 (b) depicts the temperature dependence of \\(4\\pi M\\) measured at 10 Oe after FC at three different fields : 1000 Oe, 10000 Oe, and 70000 Oe. In all cases, \\(4\\pi M\\) exhibited ferromagnetic-like behavior below \\(T_{\\rm SC}\\), similar to the findings of previously reported hydrogen-rich superconductors[21] and Sn-Pb solders[8], implying the presence of trapped magnetic flux at grain boundaries of MgB\\({}_{2}\\). The value of magnetization at 1.8 K increased as the field increased from 1000 Oe to 10000 Oe, but it did not change further with the application of a higher magnetic field. This suggests that the amount of trapped magnetic flux increases with the applied magnetic field, but there is a threshold where the trapped magnetic flux saturates. To further discuss, we show the \\(4\\pi M\\)-\\(H\\) curves at 2.5 K and 4.0 K in Figs. 3(c) and 3(e), respectively. These curves display the distinct shape commonly observed in type-II superconductors, which signifies the presence of flux trapping in the material. As depicted in Figures 3(d) and 3(f), the inner magnetic flux density (\\(B\\)) given by \\(B=H+4\\pi M\\) near 0 Oe is displayed at 2.5 K and 4.0 K. The results at 2.5 K and 4.0 K showed similarities: immediately after the zero-field-cooling, the initial magnetic flux density of MgB\\({}_{2}\\) was \\(B=0\\). However, upon applying a magnetic field to\nMgB\\({}_{2}\\), \\(B\\) did not return to its initial value when the applied field reached \\(H\\) = 0, due to the magnet flux trapping. The magnetic flux density trapped at \\(H\\) = 0 Oe was 500 G for both temperatures.\n\nFigure 4 (a) depicts the temperature dependence of \\(\\kappa\\) in both a zero magnetic field and a magnetic field of 10000 Oe. In the absence of a magnetic field, \\(\\kappa\\) decreased as the temperature decreased. The observed variation in the slope of \\(\\kappa\\) at approximately 10 K was consistent with previous measurements on polycrystalline MgB\\({}_{2}\\)[22]. Furthermore, \\(\\kappa\\) at 50 K in this experiment was approximately 3.5 W\/Km, which aligns with the order of magnitude reported in previous studies, where values ranged from 5 W\/Km[23] to 9 W\/Km[22]. It is noted that thermal conductivity is a sensitive indicator of grain boundaries, and therefore, the discrepancy with previous studies is attributed to the sample dependence. When a magnetic field of 10000 Oe was applied, a similar trend in \\(\\kappa\\) was observed, but the decrease in \\(\\kappa\\) was suppressed. This can be attributed to the suppression of the superconducting state in MgB\\({}_{2}\\) under the magnetic field. Figures 4(b) and 4(c) illustrate the magnetic field dependence of \\(\\kappa\\) at 2.5 K and 4 K, respectively. When the MgB\\({}_{2}\\) was zero-field-cooled to 2.5 K, the initial \\(\\kappa\\) in the absence of magnetic field was 6.9 mW\/Km. When a magnetic field was applied, \\(\\kappa\\) increased and reached a value of 14.0 mW\/Km at 10000 Oe. As the magnetic field gradually decreased from 10000 Oe, \\(\\kappa\\) showed a decrease. However, the value at 0 Oe deviated from the initial value, indicating nonvolatile MTS. Upon further reduction of the magnetic field, a minimum value of \\(\\kappa\\) was observed, followed by an increase in \\(\\kappa\\). Similar trends were observed when the magnetic field was increased from -10000 Oe. As mentioned earlier, the presence of approximately 500 G of trapped magnetic flux in MgB\\({}_{2}\\) after demagnetization partially suppresses the superconducting state and prevented \\(\\kappa\\) from returning to its initial value. The nonvolatile MTSR observed in MgB\\({}_{2}\\) at 2.5 K in this experiment was 18 %, which is smaller than to that of flux-core-free Sn45-Pb55 solder[8]. Furthermore, nonvolatile MTS was also observed at 4.0 K, although the nonvolatile MTSR decreased to that at 2.5 K, reaching 15 %.\n\nThe primary discovery of this study is the confirmation of nonvolatile MTS occurring in the magnetic flux trapped at the grain boundaries of the type-II superconductor MgB\\({}_{2}\\). This finding diverges from prior research, which predominantly focused on composites such as Sn-Pb solders. Notably, the phenomenon of flux trapping at grain boundaries has been observed not only in MgB\\({}_{2}\\) but also in other type-II superconductors, including cuprate superconductors and iron-based superconductors [24]. This suggests that the trapping of flux at grain boundaries is a widespread occurrence in various types of type-II superconducting materials. In this study, the maximum value of the nonvolatile MTSR achieved for MgB\\({}_{2}\\) remained relatively small at 18 % at 2.5 K. To\nfurther enhance the nonvolatile MTSR, potential methods include controlling the grain boundary size to increase the trapped magnetic flux and regulating the thermal conductivity in the normal conducting region. However, further systematic investigations are required in this regard. Recent advancements in machine learning have contributed to the elucidation of heat conduction mechanisms in grain boundaries and nanopolycrystals [25]. Given that nonvolatile MTS is a relatively new phenomenon, it is crucial to not only investigate the thermal conductivity under magnetic field in various materials but also consider theoretical approaches that utilize machine learning to gain a deeper understanding of nonvolatile MTS.\n\nThe motivation for this study was derived from the discovery of nonvolatile MTS induced by magnetic flux trapping in Sn-Pb solders. Drawing inspiration from this phenomenon, our research focused on investigating the magnetic field dependence of thermal conductivity in type-II superconductor MgB\\({}_{2}\\), a material renowned for its ability to trap magnetic flux at grain boundaries. Through our experiments, we successfully observed nonvolatile MTS in MgB\\({}_{2}\\) and identified magnetic flux trapping as the underlying mechanism. Moving forward, it is imperative to extend this research to encompass other type-II superconductors with effective pinning centers. Such endeavors will contribute to a deeper understanding of nonvolatile MTS at a fundamental level and facilitate improvements in both the nonvolatile MTSR and the operational temperature range, thereby paving the way for potential engineering applications.\n\n## Acknowledgment\n\nWe thank O. Miura and K. Uchida for supports in experiments and fruitful discussion on the results. This work was partly supported by JST-ERATO (JPMJER2201), TMU Research Project for Emergent Future Society, and Tokyo Government-Advanced Research (H31-1).\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2307.16404v1.pdf"}
{"id":2307.1641,"title":"HiREN: Towards Higher Supervision Quality for Better Scene Text Image\n  Super-Resolution","abstract":"Scene text image super-resolution (STISR) is an important pre-processing\ntechnique for text recognition from low-resolution scene images. Nowadays,\nvarious methods have been proposed to extract text-specific information from\nhigh-resolution (HR) images to supervise STISR model training. However, due to\nuncontrollable factors (e.g. shooting equipment, focus, and environment) in\nmanually photographing HR images, the quality of HR images cannot be\nguaranteed, which unavoidably impacts STISR performance. Observing the quality\nissue of HR images, in this paper we propose a novel idea to boost STISR by\nfirst enhancing the quality of HR images and then using the enhanced HR images\nas supervision to do STISR. Concretely, we develop a new STISR framework,\ncalled High-Resolution ENhancement (HiREN) that consists of two branches and a\nquality estimation module. The first branch is developed to recover the\nlow-resolution (LR) images, and the other is an HR quality enhancement branch\naiming at generating high-quality (HQ) text images based on the HR images to\nprovide more accurate supervision to the LR images. As the degradation from HQ\nto HR may be diverse, and there is no pixel-level supervision for HQ image\ngeneration, we design a kernel-guided enhancement network to handle various\ndegradation, and exploit the feedback from a recognizer and text-level\nannotations as weak supervision signal to train the HR enhancement branch.\nThen, a quality estimation module is employed to evaluate the qualities of HQ\nimages, which are used to suppress the erroneous supervision information by\nweighting the loss of each image. Extensive experiments on TextZoom show that\nHiREN can work well with most existing STISR methods and significantly boost\ntheir performances.","authors":"Minyi Zhao, Yi Xu, Bingjia Li, Jie Wang, Jihong Guan, Shuigeng Zhou","published_date":"2023-07-31T05:32:57Z","link":"http:\/\/arxiv.org\/abs\/2307.16410v1","markdown":"# HiREN: Towards Higher Supervision Quality for Better Scene Text Image Super-Resolution\n\n###### Abstract\n\nScene text image super-resolution (STISR) is an important pre-processing technique for text recognition from low-resolution scene images. Nowadays, various methods have been proposed to extract text-specific information from high-resolution (HR) images to supervise STISR model training. However, due to uncontrollable factors (_e.g._ shooting equipment, focus, and environment) in manually photographing HR images, the quality of HR images cannot be guaranteed, which unavoidably impacts STISR performance. Observing the quality issue of HR images, in this paper we propose a novel idea to boost STISR by first enhancing the quality of HR images and then using the enhanced HR images as supervision to do STISR. Concretely, we develop a new STISR framework, called High-Resolution ENhancement (HiREN) that consists of two branches and a quality estimation module. The first branch is developed to recover the low-resolution (LR) images, and the other is an _HR quality enhancement_ branch aiming at generating high-quality (HQ) text images based on the HR images to provide more accurate supervision to the LR images. As the degradation from HQ to HR may be diverse, and there is no pixel-level supervision for HQ image generation, we design a kernel-guided enhancement network to handle various degradation, and exploit the feedback from a recognizer and text-level annotations as weak supervision signal to train the HR enhancement branch. Then, a _quality estimation module_ is employed to evaluate the qualities of HQ images, which are used to suppress the erroneous supervision information by weighting the loss of each image. Extensive experiments on Text\/Zoom show that HiREN can work well with most existing STISR methods and significantly boost their performances.\n\n Scene text image super-resolution, scene text recognition, super-resolution, resolution enhancement\n\n## I Introduction\n\nScene text recognition [1, 2] (STR), which aims at recognizing texts from scene images has wide applications in scene text based image understanding (_e.g._ auto-driving [3], TextVQA [4], Doc-VQA [5], and ViteVQA [6]). Despite the fact that STR has made great progress with the rapid blossom of deep learning in recent years, performance of text recognition from low-resolution (LR) text images is still unsatisfactory [7]. Therefore, scene text image super-resolution (STISR) [8, 9, 7] is gaining popularity as a pre-processing technique to recover the missing details in LR images for boosting text recognition performance as well as the visual quality of the scene texts.\n\nAs shown in Fig. 1(a), recent STISR works usually try to directly capture pixel-level (via \\(L1\\) or \\(L2\\) loss) or text-specific information from high-resolution (HR) text images to supervise the training of STISR models. For instance, Gradient profile loss [7] calculates the gradient fields of HR images as ground truth for sharpening the boundaries of the super-resolution (SR) images. PCAN [10] is proposed to learn sequence-dependent features and high-frequency information of the HR images to better reconstruct SR text images. STT [8] exploits character-level attention maps from HR images to assist the recovery. [11] and TG [9] extract stroke-level information from HR images through specific networks to provide more fine-grained supervision information. [12, 13, 14] additionally introduce external modules to extract various text-specific clues to facilitate the recovery and use the supervision from HR images to finetune their modules.\n\nAlthough various techniques that extract information from the HR images have been proposed to improve the recognition accuracy, they all assume that the HR images are completely trustworthy, which is actually not true, due to the uncontrollable factors (e.g. shooting equipment, focus, and environment) in manually photographing the HR images. As shown in Fig. 1(c), the HR images may suffer from blurring (the 1st and 2nd cases) and low-contrast (the 3rd case), which unavoidably impacts the performance of STISR. In the worst case, these quality issues may cause the failure of recognition on HR images and lead to wrong supervision information. What is worse, the HR quality problem in real world is absolutely not negligible, as the recognition accuracy on HR images can be as low as 72.4% (see Tab. II).\n\nConsidering the fact that improving the photographing of LR\/HR images and eliminating environmental impacts are extremely expensive (if not impossible) in the wild, and applying huge models for extracting more accurate information is also time-consuming and costly, in this paper we propose a novel solution to advance STISR by first enhancing the quality of HR images and then using the enhanced HR images as supervision to perform STISR. To this end, we develop a new, general and easy-to-use STISR framework called **H**igh-**R**esolution **EN**chancement (HiREN) to improve STISR by providing more accurate supervision. In particular, as shown in Fig. 1(b), besides the typical LR recovery branch, HiREN additionally introduces an HR enhancement branch that aims at improving the quality of HR images and a quality estimation (QE) module to conduct a quality-aware supervision. Here, the\nresulting high-quality (HQ) images, instead of the HR images as in existing works, are used to supervise the LR recovery branch. Note that the degradation from HQ to HR is unknown, and there is no explicit supervision for HR enhancement, existing STISR approaches are not able to solve the task of HR enhancement. To tackle these problems, on the one hand, we introduce a degradation kernel predictor to generate the degradation kernel and then use this kernel as a clue to enhance various degraded HR images. On the other hand, we exploit the feedback of a scene text recognizer and text-level annotations as weak supervision signal to train the HR enhancement branch. What is more, to suppress the erroneous supervision information, a quality estimation (QE) module is proposed to evaluate the quality of the HQ images through the normalized Levenshtein similarity [15] of the recognized text and the ground truth, and then use this quality estimation to weight the loss of each HQ image.\n\nSuch design above offers our method four-fold advantages:\n\n* _General_. Our framework can work with most existing STISR approaches in a plug-and-play manner.\n* _Easy-to-use_. After training the HR enhancement branch, our method can be plugged online to the training of existing techniques easily.\n* _Efficient_. HiREN does not introduce additional cost during inference. What is more, HiREN can also be deployed offline by caching all the enhanced HR images. This offline deployment does not introduce any additional training cost.\n* _High-performance_. Our method can significantly boost the performances of existing methods.\n\nContributions of this paper are summarized as follows:\n\n* We propose a novel approach for STISR. To the best of our knowledge, this is the first work to consider and exploit the quality of HR images in STISR. That is, different from existing approaches that extract various text-specific information, Our work pioneers the exploration of the quality issue of HR images.\n* We develop a general, efficient and easy-to-use **H**igh-**R**esolution **EN**hancement (HiREN) framework to boost STISR by improving the supervision information from the HR images.\n* We conduct extensive experiments on TextZoom, which show that HiREN is compatible with most existing STISR methods and can significantly lift their performances.\n\nThe rest of this paper is organized as follows: Section II surveys related works and highlights the differences between our method and the existing ones; Section III presents our method in detail; Section IV introduce the experimental results of our method and performance comparisons with existing methods; Section V further discusses the quality issues of HR images, error cases and limitations of the proposed method; Section VI concludes the paper while pinpointing some issues of future study.\n\n## II Related Work\n\nIn this section, we briefly review the super-resolution techniques and some typical scene text recognizers. According to whether exploiting text-specific information from HR images, recent STISR methods can be roughly divided into two groups: generic super-resolution approaches and scene text image super-resolution approaches.\n\n### _Generic Image Super-Resolution_\n\nGeneric image super-resolution methods [16, 17, 18, 19] usually recover LR images through pixel information\n\nFig. 1: Overview of existing STISR approaches and our method, and examples illustrating the quality problem of HR images. (a) The framework of existing STISR methods; (b) The HiREN framework; (c) Some examples of low-quality HR images and their enhanced results (HQ) by our method, as well as the recognized results. For each case, the 1st row shows HR and HQ images, the 2nd row presents the normalized HR and HQ images to highlight their visual differences, and the 3rd row gives the recognized characters: red indicates incorrectly recognized, and black means correctly recognized.\nfrom HR images captured by pixel loss functions. In particular, SRCNN [20] is a three-layer convolutional neural network. [21] and SRResNet [22] adopt generative adversarial networks to generate distinguishable images. [23] employs convolutional layers, transposed convolution and sub-pixel convolution layers to extract and upscale features. RCAN [24] and SAN [25] introduce attention mechanisms to boost the recovery. Nowadays, transformer-structured approaches [26, 27, 28] are proposed to further advance the task of generic image super-resolution. Nevertheless, these approaches ignore text-specific properties of the scene text images, which leads to low recognition performance when applied to STISR.\n\n### _Scene Text Image Super-Resolution_\n\nRecent approaches focus on extracting various text-specific information from the HR images, which is then utilized to supervise model training. Specifically, [29, 30] calculate text-specific losses to boost performance. [31] proposes a multi-task framework that jointly optimizes recognition and super-resolution branches. [7] introduces TSRN and gradient profile loss to capture sequential information of text images and gradient fields of HR images for sharpening the texts. PCAN [10] is proposed to learn sequence-dependent and high-frequency information of the reconstruction. STT [8] makes use of character-level information from HR images extracted by a pre-trained transformer recognizer to conduct a text-focused super-resolution. [32] proposes a content perceptual loss to extract multi-scale text recognition features to conduct a content aware supervision. TPGSR [12], TATT [13], and C3-STISR [14] extract text-specific clues to guide the super-resolution. In particular, TPGSR is the first method that additionally introduces a scene text recognizer to provide text priors. Then, the extracted priors are fed into the super-resolution to iteratively benefit the super-resolution. TATT [13] introduces a transformer-based module, which leverages global attention mechanism, to exert the semantic guidance of text prior to the text reconstruction process. C3-STISR [14] is proposed to learn triple clues, including recognition clue from a STR, linguistical clue from a language model, and a visual clue from a skeleton painter to rich the representation of the text-specific clue. TG [9] and [11] exploit stroke-level information from HR images via stroke-focused module and skeleton loss for more fine-grained super-resolution. Compared with generic image super-resolution approaches, these methods greatly advance the recognition accuracy through various text-specific information extraction techniques. Nevertheless, they all assume that HR images are completely trustable, which is actually not true in practice. As a result, their extracted supervision information may be erroneous, which impacts the STISR performance. Since HiREN applies these methods to implement the LR recovery branch, to elaborate the differences among various super-resolution techniques in this paper, we give a summary of these methods in Tab. I on three major aspects: how their super-resolution blocks and loss functions are designed, and whether they use iterative super-resolution technique to boost the performance.\n\n### _Scene Text Recognition_\n\nScene text recognition (STR) [33, 1, 2, 34, 35] has made great progress in recent years. Specifically, CRNN [36] takes CNN and RNN as the encoder and employs a CTC-based [37] decoder to maximize the probabilities of paths that can reach the ground truth. ASTER [38] introduces a spatial transformer network (STN) [39] to rectify irregular text images. MORAN [40] proposes a multi-object rectification network. [41, 42, 43] propose novel attention mechanisms. AutoSTR [44] searches backbone via neural architecture search (NAS) [45]. More recently, semantic-aware [46, 43], transformer-based [47], linguistics-aware [48, 49], and efficient [50, 51] approaches are proposed to further boost the performance. Although these methods are able to handle irregular, occluded, and incomplete text images, they still have difficulty in recognizing low-resolution images. For example, as can be seen in Sec. IV-C, CRNN, MORAN, and ASTER only achieve the recognition accuracy of 27.3%, 41.1% and 47.2% respectively when directly using LR images as input. What is more, finetuning these recognizers is insufficient to accurately recognize texts from LR images, as reported in [7]. Therefore, a pre-processor is required for recovering the details of low-resolution images.\n\n### _Difference between Our Method and Existing STISR Works_\n\nThe motivation of HiREN is totally different from that of existing STISR approaches. As described above, existing methods focus on extracting text-specific information from HR images to supervise STISR. On the contrary, HiREN first lifts the quality of HR images, then uses the enhanced images to supervise STISR. This allows HiREN to work with most existing STISR approaches and boost their recognition performances in a general, economic and easy-to-use way.\n\n## III Method\n\nHere, we first give an overview of our framework HiREN, then briefly introduce the LR recovery branch. Subsequently, we present the HR enhancement branch and the quality estimation module in detail, followed by the usage of HiREN.\n\n### _Overview_\n\nGiven a low-resolution (LR) image \\(I_{LR}\\in\\mathbb{R}^{C\\times N}\\). Here, \\(C\\) is the number of channels of the image, \\(N=H\\times W\\) is the collapsed spatial dimension, \\(H\\) and \\(W\\) are the height and width of image \\(I_{LR}\\). Our aim is to produce a super-resolution (SR)\n\n\\begin{table}\n\\begin{tabular}{c|c c c} \\hline Method & Super-resolution block & Loss function \\(\\mathcal{L}_{LR}\\) & Iterative \\\\ \\hline SRCNN [20] & SRCNN [20] & MSE & \\(\\times\\) \\\\ SRResNet [22] & SRResNet [22] & MSE & \\(\\times\\) \\\\ TSRN [7] & SSB [7] & Gradient profile loss [7] & \\(\\times\\) \\\\ PCAN [10] & PCA [10] & Edge guidance loss [10] & \\(\\times\\) \\\\ STT [8] & TBSRN [8] & Text-focused loss [8] & \\(\\times\\) \\\\ TPGSR [12] & SRN [7] & Gradient profile loss [7] & \\(\\checkmark\\) \\\\ TG [9] & SSB [7] & Stroke-focused loss [9] & \\(\\times\\) \\\\ \\hline \\end{tabular}\n\\end{table} TABLE I: Differences between typical STISR methods from three aspects: super-resolution block, loss function, and whether this method is iterative or not.\nimage \\(I_{SR}\\in\\mathbb{R}^{C\\times(4\\times N)}\\) with the magnification factor of \\(\\times 2\\). Fig. 2 shows the architecture of our framework HiREN, which is composed of two major branches: the _LR recovery branch_\\(f_{LR}\\) that takes \\(I_{LR}\\) as input to generate a super-resolution image \\(I_{SR}=f_{LR}(I_{LR})\\) and a corresponding loss \\(\\mathcal{L}_{o}\\), and the _HR enhancement branch_\\(f_{HR}\\) that takes \\(I_{HR}\\) as input to generate a high-quality (HQ) image \\(I_{HQ}=f_{HR}(I_{HR})\\) where \\(I_{HQ}\\in\\mathbb{R}^{C\\times(4\\times N)}\\), and a _quality estimation module_\\(f_{QE}\\) that takes \\(I_{HQ}\\) and \\(\\mathcal{L}_{o}\\) as input to compute a quality-aware loss \\(\\mathcal{L}_{LR}\\) to supervie the LR branch:\n\n\\[\\mathcal{L}_{LR}=f_{QE}(I_{HQ},\\mathcal{L}_{o}). \\tag{1}\\]\n\nDuring inference, \\(f_{HR}\\) and \\(f_{QE}\\) are removed. Thus, HiREN does not introduce extra inference cost.\n\n### _LR Recovery Branch_\n\nIn HiREN, the LR recovery branch can be one of the existing STISR approaches. As shown in Fig. 2, these methods usually work in the following way: 1) Start with a spatial transformer network (STN) [39] since in the TextZoom dataset [7] the HR-LR pairs are manually cropped and matched by humans, which may incur several pixel-level offsets. 2) Several super-resolution blocks are used to learn sequence-dependent information of text images. 3) A pixel shuffle module is employed to reshape the super-resolved image. 4) Various loss functions are served as \\(\\mathcal{L}_{o}\\) to extract text-specific information from ground truth (\\(I_{HR}\\) in existing works, \\(I_{HQ}\\) in HiREN) to provide the supervision. To elaborate the differences among various LR branches tested in this paper, we give a summary of these methods in Tab. I.\n\nAs the motivation of HiREN is totally different from that of the existing methods, our method can work with most of them and significantly improve their performances.\n\n### _HR Enhancement Branch_\n\n#### Iii-C1 Overall introduction.\n\nThe enhancement of HR images is a challenging task, where the challenges lie in two aspects that will be detailed in the sequel. Formally, the HR image \\(I_{HR}\\) and the corresponding HQ image \\(I_{HQ}\\) we are pursuing are connected by a degradation model as follows:\n\n\\[I_{HR}=k\\otimes I_{HQ}+n, \\tag{2}\\]\n\nwhere \\(\\otimes\\) denotes the convolution operation, \\(k\\) is the degradation kernel, and \\(n\\) is the additive noise that follows Gaussian distribution in real world applications [52, 53]. Different from the degradation from \\(I_{HR}\\) to \\(I_{LR}\\) where the kernel is determined by lens zooming, unfortunately, the degradation \\(k\\) of \\(I_{HQ}\\) is unknown. As shown in Fig. 1(c), such degradation can be but not limited to blurring (the 1st and 2nd cases) and low-contrast (the 3rd case). What is more, we also lack pixel-level supervision information of \\(I_{HQ}\\). These two challenges make existing STISR methods unable to enhance \\(I_{HR}\\). To cope with the first challenge, here we adopt blind image deblurring techniques [54, 55, 53, 52] to boost the recovery of \\(I_{HR}\\). Specifically, as shown in Fig. 2, our HR enhancement branch consists of two components: a _kernel predictor_\\(P\\) and a _kernel-guided enhancement network_\\(f_{ke}\\). The kernel predictor aims at estimating the degradation kernel \\(k\\) (_i.e.,_\\(k=P(I_{HR})\\) where \\(k\\in\\mathbb{R}^{d}\\), and \\(d\\) is the size of the kernel), while the kernel-guided enhancement network takes the predicted kernel and \\(I_{HR}\\) as input to conduct a kernel-guided enhancement: \\(I_{HQ}=f_{ke}(I_{HR},k)\\). The predicted kernel is utilized as a clue to strengthen the model's ability to handle various degradation and boost the recovery of HR images. As for the second challenge, we introduce a pre-trained scene text recognizer \\(R\\) to provide the supervision for generating more recognizable HQ images. And after training the HR enhancement branch \\(f_{HR}\\), HiREN uses the trained \\(f_{HR}\\) to generate HQ images, which are exploited for training the LR recovery branch.\n\n#### Iii-C2 The kernel predictor.\n\nAs shown in Fig. 3, to generate a prediction of the degradation kernel, we first utilize convolution layers to obtain a spatial estimation of the kernel. Then, we employ global average pooling [56] to output the global prediction by evaluating the spatial mean value. Thus, we can\n\nFig. 2: The framework of HiREN. Red lines are valid only during training.\nget the prediction of the kernel of size \\(\\mathbb{R}^{d}\\), in a simple yet effective way.\n\n#### Iii-C3 The kernel-guided enhancement network.\n\nAs shown in Fig. 3, our kernel-guided enhancement network is designed in the following way: 1) Start with an input convolution to change the channel number from \\(C\\) to \\(C^{\\prime}\\). 2) Repeat \\(N\\) modified SRB blocks [7]. Each block consists of two convolution layers and one Bi-directional GRU [57] (BGRU) to handle sequential text images. At this step, we first stretch the predicted kernel \\(k\\) to pixel shape, then concatenate the pixel kernel with the feature map extracted by convolution layers at channel dimension. 3) An output convolution is applied to getting the final enhanced HQ image \\(I_{HQ}\\).\n\n#### Iii-C4 Loss functions.\n\nHere, we design the loss functions of the HR enhancement branch \\(f_{HR}\\). As shown in Fig. 2, there are two loss functions in \\(f_{HR}\\). The first one is the recognition loss \\(\\mathcal{L}_{rec}\\) that is used to make the enhanced image \\(I_{HQ}\\) to be more easily recognized than \\(I_{HR}\\). It is provided by a pre-trained recognizer \\(R\\) and the text-level annotation of \\(I_{HR}\\). Suppose the encoded text-level annotation is \\(p_{GT}\\in\\mathbb{R}^{L\\times|\\mathcal{A}|}\\), where \\(L\\) is the max prediction length of recognizer \\(R\\), and \\(|\\mathcal{A}|\\) denotes the length of the alphabet \\(\\mathcal{A}\\). Then, the recognition loss can be evaluated by\n\n\\[\\mathcal{L}_{rec}=-\\sum_{j=0}^{L}p_{GT}^{j}log(R(I_{HQ})^{j}), \\tag{3}\\]\n\nwhich is the cross entropy of \\(p_{GT}\\) and \\(R(I_{HQ})\\). Beside the recognition loss, it is essential to keep the style of the enhanced images, which has also been pointed out in a recent work [8]. Though HR images are not trustworthy, pixel information from HR images can help the model to enhance the input images, rather than totally regenerate them, which is a much more challenging and uncontrollable task. In HiREN, we use mean-squared-error (MSE) to compute pixel loss to keep the style unchanged. Formally, we have\n\n\\[\\mathcal{L}_{sty}=||I_{HQ},I_{HR}||_{2}. \\tag{4}\\]\n\nWith the recognition loss Eq. (3) and the style loss Eq. (4), the whole loss function of the HR enhancement branch can be written as follows:\n\n\\[\\mathcal{L}_{HR}=\\alpha\\mathcal{L}_{rec}+\\mathcal{L}_{sty}, \\tag{5}\\]\n\nwhere \\(\\alpha\\) is a hyper-parameter to trade-off the two losses.\n\n### _Quality Estimation Module_\n\nThough we can improve the quality of supervision information with the help of the HR enhancement branch, we cannot guarantee the correctness of the supervision information. Therefore, to suppress wrong supervision information, we design a quality estimation module \\(f_{QE}\\) to evaluate the qualities of HQ images and weight the losses of HQ images according to their qualities.\n\nLet the original loss of the LR branch be \\(\\mathcal{L}_{o}\\in\\mathbb{R}^{B}\\), where \\(B\\) denotes the batch size. We adopt the Levenshtein similarity [15] between the \\(i\\)-th HQ image's recognition result \\(pred_{i}\\) of a recognizer \\(R\\) and the corresponding ground truth \\(gt_{i}\\) to measure its quality, and then utilize the quality values of all HQ images to compute the final loss:\n\n\\[\\mathcal{L}_{LR}=\\mathcal{L}_{o}[NS(pred_{1},gt_{1}),...,NS(pred_{B},gt_{B})] ^{\\top}\/B, \\tag{6}\\]\n\nwhere \\(NS(\\cdot,\\cdot)\\) denotes the Levenshtein similarity, which has the following two advantages: 1) its value falls between 0 and 1; 2) it has a smooth response, thus can gracefully capture character-level errors [58]. These advantages make it suitable to weight the losses of HQ images.\n\n### _The Usage of HiREN_\n\nIn this section, we introduce the usage of HiREN. As mentioned above, there are two ways to deploy it. One way is called \"online\", which can be easily implemented by plugged the HR enhancement branch to the training procedure of the LR recovery branch. The online installation algorithm of HiREN is given in Alg. 1. As shown in Alg. 1, the first thing we should do is to develop the HR enhancement branch (_i.e.,_ L4\\(\\sim\\)L10). Specifically, given a STISR dataset \\(\\mathcal{D}\\), we\n\nFig. 3: The structure of the HR enhancement branch, which consists of two components: (a) the kernel predictor \\(P\\), and (b) the kernel-guided enhancement network \\(f_{ke}\\).\nfirst sample HR images and their corresponding text-level annotations from \\(\\mathcal{D}\\) (L5), then generate the enhanced images \\(I_{HQ}\\) (L6). Finally, recognition loss and style loss described in Sec. III-C4 are computed to optimize the loss \\(f_{HR}\\). After that, we plug the developed HR enhancement branch to the training procedure of the LR recover branch (L11\\(\\sim\\)L16). In particular, after sampling LR and HR images from the dataset \\(\\mathcal{D}\\) (L12), we use the HR enhancement branch to generate the HQ image \\(I_{HQ}\\) (L13). Finally, the HQ image, rather than the HR image used in typical works, and the SR image are utilized to compute the text-specific loss \\(\\mathcal{L}_{l}\\) to supervise the LR recovery branch (L11\\(\\sim\\)L12).\n\nThe other way is called \"offline\", which can be implemented by caching all the enhanced HQ images. As can be checked in Alg. 2, after developing the HR enhancement branch \\(f_{HR}\\), we sample all the LR-HR image pairs in the old dataset \\(\\mathcal{D}\\). Then, the corresponding HQ images are generated and then add to the new dataset \\(\\mathcal{\\tilde{D}}\\) (L6). During training the LR recovery branch, what we need to do is to sample LR-HQ image pairs to compute the loss \\(L_{o}\\) for the optimization of the model. Such an installation does not introduce any additional training cost to the LR recovery branch. It is worth mentioning that the HR enhancement branch is removed during inference. That is, HiREN does not introduce any additional inference cost.\n\n```\n1:Input: Training dataset \\(\\mathcal{D}\\) and the developed HR enhancement branch \\(f_{HR}\\)\n2:Initialize \\(f_{LR}\\)\n3:\\(\\mathcal{\\hat{D}}=\\emptyset\\)\n4:for\\(I_{LR},I_{HR}\\sim\\mathcal{D}\\)do\n5:\\(I_{HQ}=f_{HR}(I_{HR})\\)\n6: Add \\((I_{HQ},I_{LR})\\) to \\(\\mathcal{\\hat{D}}\\)\n7:while\\(f_{LR}\\) is not converged do\n8:\\(I_{HQ},I_{LR}\\sim\\mathcal{\\hat{D}}\\)\n9:\\(I_{SR}=f_{LR}(I_{LR})\\)\n10: Compute \\(\\mathcal{L}_{o}\\) according to \\(I_{SR}\\) and \\(I_{HQ}\\)\n11: Optimize \\(f_{LR}\\) with respect to \\(\\mathcal{L}_{o}\\)\n12:return\\(f_{LR}\\)\n```\n\n**Algorithm 2** The offline usage of HiREN.\n\n## IV Performance Evaluation\n\nIn this section, we first introduce the dataset and metrics used in the experiments and the implementation details. Then, we evaluate HiREN and compare it with several state-of-the-art techniques to show its effectiveness and superiority. Finally, we conduct extensive ablation studies to validate the design of our method.\n\n### _Dataset and Metrics_\n\nTwo groups of datasets are evaluated in this paper: low-resolution scene text dataset TextZoom and regular scene text recognition datasets.\n\n#### Iv-A1 Low-resolution scene text dataset\n\nThe **TextZoom**[7] dataset consists of 21,740 LR-HR text image pairs collected by lens zooming of the camera in real-world scenarios. The training set has 17,367 pairs, while the test set is divided into three settings based on the camera focal length: easy (1,619 samples), medium (1,411 samples), and hard (1,343 samples).\n\n#### Iv-A2 Regular STR datasets\n\nThese datasets are used to check the generalization power of our model trained on TextZoom when being adapted to other datasets. In particular, three regular STR datasets are evaluated in our paper to further check the advantage of HiREN: IC15-352 [8], SVT [59], and SVTP [60]. In what follows, we give brief introductions on these datasets.\n\nThe **IC15-352** dataset is first divided in [8]. This dataset consists of 352 low-resolution images collected from the IC15 [61] dataset.\n\nStreet View Text (**SVT**) [59] is collected from the Google Street View. The test set contains 647 images. Many images in SVT are severely suffered from noise, blur, and low-resolution.\n\nSVT-Perspective (**SVTP**) [60] is proposed for evaluating the performance of reading perspective texts. Images in SVTP are picked from the side-view images in Google Street View. Many of them are heavily distorted by the non-frontal view angle. This dataset contains 639 images for evaluation.\n\nThe major metric used in this paper is word-level recognition accuracy that evaluates the recognition performance of STISR methods. Following the settings of previous works [9], we remove punctuation and convert uppercase letters to low-crease letters for calculating recognition accuracy. Besides, _Floating-point **O**perations **P**er **S**econd_ (FLOPS) is used to evaluate the computational cost of various methods. Following [9, 32], we only report _Peak Signal-to-Noise Ratio_ (PSNR) and _Structure Similarity Index Measure_ (SSIM) [62] as the auxiliary metrics to evaluate the fidelity performance because of the quality issue of the HR images.\n\n### _Implementation Details_\n\nAll experiments are conducted on 2 NVIDIA Tesla V100 GPUs with 32GB memory. The PyTorch version is 1.8. The HR enhancement branch is trained using Adam [63] optimizer with a learning rate of 0.0001. The batch size \\(B\\) is set to 48. The LR recovery branch is trained with the same optimizer and batch size but a higher learning rate of 0.001, which is suggested in [12]. The recognizer \\(R\\) used in our method is proposed in [8]. The hyper-parameters in HiREN are set as follows: \\(\\alpha\\) is set to 0.1, which is determined through grid search. The number of SRB blocks is set to 5 (_i.e.,_\\(N=5\\)) and \\(C^{\\prime}\\) is set to 32, which is the same as in [7]. The size of kernel \\(k\\) is set to 32 (_i.e.,_\\(d=32\\)), which is similar to that suggested in [52]. Our training and evaluation are based on the following protocol: save the averagely best model during training with CRNN as the recognizer, and use this model to evaluate the other recognizers (MORAN, ASTER) and the three settings (easy, medium, hard).\n\n### _Performance Improvement on SOTA Approaches_\n\n#### Iv-C1 Recognition performance improvement\n\nHere, we evaluate our method on **TextZoom**. Since HiREN is a framework that can work with most existing methods, we plug HiREN to the training of several typical super-resolution methods to\ncheck the universality and effectiveness of HiREN, including one generic method SRCNN [20], two recent proposed STISR methods TSRN [7], TG [9], and one iterative-based and clue-guided STISR method TPGSR [12]. To show that HiREN can support various recognizers, we follow previous works [12, 8, 9] and evaluate the recognition accuracy on three recognizers: CRNN [36], MORAN [40] and ASTER [38]. We re-implement these methods to unify hardware, software, and evaluation protocols for fair comparison. Generally, our results are higher than those in the original papers. For example, with CRNN the averaged accuracy of TG is boosted from 48.9% to 49.6%. All the results are presented in Tab. II.\n\nWe first check the universality of HiREN. As can be seen in Tab. II, HiREN significantly boosts the recognition performance in almost all the cases, except for one case on TPGSR, which means that HiREN can work well with various existing techniques. As for the performance improvement of HiREN, taking a non-iterative method for example. The state-of-the-art TG [9] achieves 49.6%, 57.6% and 61.2% averaged accuracy respectively with the three recognizers (see the 9th row). After equipping our method HiREN, the accuracy is lifted to 51.1%, 58.6% and 61.7% (increasing by 1.5%, 1.0%, and 0.5%) respectively (see the 10th row). This demonstrates the effectiveness of our method. Results on more datasets and recognizers are given in the supplementary materials to demonstrate its universality.\n\nIt is worth mentioning that our HR enhancement branch can also be applied to weakly supervising the enhancement of LR and HR images to lift their recognition accuracies, as shown in the 3rd and 5th rows of Tab. II. This further supports the universality of our technique. Results above show the promising application potential of our method -- not only work with STISR methods, but also pioneer weakly supervised enhancement of LR and HR text images.\n\nFurthermore, to better demonstrate the universality of HiREN, we conduct more experiments on more STR datasets and recently proposed STR datasets. We first evaluate our method on three STR datasets, including IC15-352, SVT, and SVTP. We use the STISR models (TSRN, TG, TPGSR, and our techniques performed on them) developed on the TextZoom dataset to evaluate these datasets. The experimental results on IC15-352, SVT, and SVTP are given in Tab. III. As shown in Tab. III, HiREN also works well on them and achieve lifted performance in almost all the cases. In particular, the performance of TPGSR on three datasets are lifted from 66.2%, 77.4%, 62.8% to 66.8%, 78.7%, and 63.6%, respectively, which demonstrates the advantage of HiREN.\n\nApart from that, we also give the experimental results on more recently proposed recognizers including SEED [46] and ABINet [48]. The experimental results are given in Tab. IV. As can be checked in Tab. IV, these recent recognizers still find difficulty in recognizing low-resolution text images. For example, SEED and ABINet can only correctly read 45.8% and 61.0% of LR images, which are inferior to performance of reading HR images (_i.e._, 84.8% and 89.8%). Our method HiREN can also achieve boosted performance on these recognizers in almost all the cases.\n\n#### Iv-B2 Fidelity improvement\n\nWe also report the results of fidelity improvement (PSNR and SSIM) on major existing methods in Tab. V. Notice that these fidelity metrics have the following limitations. On the one hand, PSNR and SSIM globally measure the similarity between SR image and the ground truth image, including both characters and background. With the goal of lifting the recognition ability and readability of the scene text images, STISR should put more emphasis on recovering characters rather than the background [9, 32]. On the other hand, as pointed out by our paper, HR images are suffering various quality issues. Ergo, it is inappropriate to measure the pixel similarity between erroneous HR images\n\n\\begin{table}\n\\begin{tabular}{c||c c c c|c c c c|c c c} \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{CRNN [36]} & \\multicolumn{3}{c}{MORAN [40]} & \\multicolumn{3}{c}{ASTER [38]} \\\\ \\cline{2-13}  & Easy & Medium & Hard & Average & Easy & Medium & Hard & Average & Easy & Medium & Hard & Average \\\\ \\hline \\hline LR & 37.5\\% & 21.4\\% & 21.1\\% & 27.3\\% & 56.2\\% & 35.9\\% & 28.2\\% & 41.1\\% & 64.0\\% & 42.0\\% & 31.7\\% & 47.2\\% \\\\ +HiREN & 37.7\\% & **27.9\\%** & **23.5\\%** & **30.2\\%** & **57.9\\%** & **38.2\\%** & **28.7\\%** & **42.6\\%** & **66.4\\%** & **43.4\\%** & **32.3\\%** & **48.5\\%** \\\\ \\hline HR & 76.4\\% & 75.1\\% & 64.6\\% & 72.4\\% & **89.0\\%** & 83.1\\% & 71.1\\% & 81.6\\% & 93.4\\% & 87.0\\% & 75.7\\% & 85.9\\% \\\\ +HiREN & **77.5\\%** & **75.4\\%** & **65.0\\%** & **72.9\\%** & 88.8\\% & **83.7\\%** & **71.9\\%** & **82.0\\%** & **93.5\\%** & **87.5\\%** & **76.2\\%** & **86.3\\%** \\\\ \\hline \\hline SRCNN & 39.8\\% & 23.4\\% & 21.7\\% & 29.0\\% & 57.7\\% & 36.1\\% & 28.5\\% & 41.8\\% & 65.5\\% & 41.9\\% & 31.7\\% & 47.5\\% \\\\ +HiREN & 41.6\\% & **24.0\\%** & **23.7\\%** & **30.4\\%** & **61.1\\%** & **38.6\\%** & **29.3\\%** & **44.0\\%** & **67.5\\%** & **44.7\\%** & **32.8\\%** & **49.5\\%** \\\\ \\hline TSRN & 52.8\\% & 39.8\\% & 31.6\\% & 42.1\\% & 64.5\\% & 49.3\\% & 36.7\\% & 51.1\\% & 69.7\\% & 54.8\\% & 41.3\\% & 56.2\\% \\\\ +HiREN & **56.5\\%** & **44.1\\%** & **32.2\\%** & **45.0\\%** & **68.5\\%** & **52.5\\%** & **38.6\\%** & **54.2\\%** & **73.5\\%** & **56.3\\%** & **39.2\\%** & **57.4\\%** \\\\ \\hline TG & 60.5\\% & 49.0\\% & 37.1\\% & 49.6\\% & 72.0\\% & 57.6\\% & 40.0\\% & 57.6\\% & 76.0\\% & 61.4\\% & 42.9\\% & 61.2\\% \\\\ +HiREN & **62.4\\%** & **51.2\\%** & **37.5\\%** & **51.1\\%** & **73.4\\%** & **58.4\\%** & **41.0\\%** & **58.6\\%** & **77.5\\%** & **61.5\\%** & **43.0\\%** & 61.7\\% \\\\ \\hline TPGSR & 63.1\\% & 52.0\\% & 38.6\\% & 51.8\\% & **74.9\\%** & 60.5\\% & 44.1\\% & **60.5\\%** & **78.9\\%** & 62.7\\% & 44.5\\% & 62.8\\% \\\\ +HiREN & **63.5\\%** & **52.7\\%** & **38.8\\%** & **52.4\\%** & 74.7\\% & **60.9\\%** & **44.1\\%** & **60.5\\%** & 78.3\\% & **63.5\\%** & **45.6\\%** & **63.5\\%** \\\\ \\hline \\end{tabular}\n\\end{table} TABLE II: Performance (recognition accuracy) improvement on TextZoom.\n\n\\begin{table}\n\\begin{tabular}{c|c c} \\hline Method & SEED [46] & ABINet [48] \\\\ \\hline LR & 45.8\\% & 61.0\\% \\\\ HR & 84.8\\% & 89.8\\% \\\\ \\hline TSRN & 56.3\\% & **64.0\\%** \\\\ +HiREN & **56.5\\%** & 63.8\\% \\\\ \\hline TG & 60.7\\% & **66.0\\%** \\\\ +HiREN & **60.9\\%** & 65.9\\% \\\\ \\hline TPGSR & 61.7\\% & 67.5\\% \\\\ +HiREN & **62.2\\%** & **68.1\\%** \\\\ \\hline \\end{tabular}\n\\end{table} TABLE IV: Performance of recent recognizers on TextZoom.\n\n\\begin{table}\n\\begin{tabular}{c||c c c} \\hline Method & IC15-352 & SVT \\\\ \\hline LR & 49.4\\% & 74.8\\% & 60.8\\% \\\\ \\hline TSRN & 48.9\\% & 72.6\\% & **61.4\\%** \\\\ +HiREN & **52.3\\%** & **74.8\\%** & 60.3\\% \\\\ \\hline TG & 59.1\\% & 74.2\\% & 60.2\\% \\\\ +HiREN & **61.7\\%** & **76.5\\%** & **68.5\\%** \\\\ \\hline TPGSR & 66.2\\% & 77.4\\% & 62.8\\% \\\\ +HiREN & **66.8\\%** & **78.7\\%** & **63.6\\%** \\\\ \\hline \\end{tabular}\n\\end{table} TABLE III: Performance comparison on three STR datasets with CRNN as recognizer.\nwhose pixels are not trustworthy. Therefore, we only present PSNR and SSIM as auxiliary metrics to roughly draw some conclusions.\n\nNotice that existing methods utilize SR-HR image pairs to calculate PSNR and SSIM. However, as mentioned above, the HR images are suffering from quality issues. Hence, we additionally provide the fidelity results of calculating PSNR and SSIM between SR and HQ images. The experimental results are given in Tab. V. As can be seen in Tab. V, 1) A higher PSNR does not means a higher recognition accuracy. For example, the PSNR of TG in SR-HR is inferior to that of TSRN (_i.e.,_ 21.47 v.s. 21.84) but TG performs better on recognition accuracy (_i.e.,_ 49.6% v.s. 42.1%). The reason lies in that TG is a stroke-focused technique, focusing on recovering fine-grained stroke details rather than the whole image quality including background that is minor to recognition. This is consistent with the results in [9]. 2) Comparing with the original models, after applying HiREN, the SR-HQ fidelity performance of the new models are boosted in almost all cases. 3) HiREN gets a low performance on the PSNR and SSIM of SR-HR images but obtains an improved recognition performance, which supports the quality issue of HR images.\n\n#### Iv-B3 Visualization\n\nHere, we visualize several examples in Fig. 4 to better demonstrate the performance of our technique. We can see that HiREN can help the existing methods to recover the blurry pixels better (see the 2nd \\(\\sim\\) 6th cases). In particular, a better \"ee\" in the 2nd and 3rd cases,'m' in the 4th case, 'f' in the 5th case, and 'e' in the 6th case are obtained by our technique. Besides, in some extremely tough cases where even with the HR images the recognition is hard, HiREN can still achieve better recovery (see the 7th case). These results show the power of HiREN.\n\n#### Iv-B4 Training and inference cost\n\nWe have discussed the high performance of our technique above. In this section, we provide the results of training and inference costs to show the efficiency of HiREN. Specifically, We take TG and TPGSR\n\n\\begin{table}\n\\begin{tabular}{c||c|c|c|c} \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Metrics} \\\\ \\cline{2-5}  & \\multicolumn{2}{c|}{SR-HR} & \\multicolumn{2}{c|}{SR-HQ} & \\multicolumn{2}{c}{Avg} \\\\ \\cline{2-5}  & PSNR & SSIM(\\(\\times 10^{-2}\\)) & PSNR & SSIM(\\(\\times 10^{-2}\\)) & Acc \\\\ \\hline \\hline LR & 20.35 & 69.61 & 20.73 & 68.76 & 27.3\\% \\\\ \\hline TSRN & 21.84 & 76.34 & 21.08 & 74.76 & 42.1\\% \\\\ \\hline \\(\\star\\)HiREN & **22.01** & **76.60** & **21.46** & **76.23** & **45.0\\%** \\\\ \\hline TG & **21.47** & **73.57** & **20.89** & 72.59 & 49.6\\% \\\\ \\(\\star\\)HiREN & 21.12 & 73.43 & 20.84 & **73.78** & **51.1\\%** \\\\ \\hline TPGSR & **22.05** & **76.71** & 21.05 & **76.77** & 51.8\\% \\\\ \\(\\star\\)HiREN & 21.69 & 75.97 & **21.15** & 76.44 & **52.4\\%** \\\\ \\hline \\end{tabular}\n\\end{table} TABLE V: Fidelity and recognition results on major existing methods. The results are obtained by averaging three settings (easy, medium and hard).\n\nFig. 4: Examples of generated images. Here, GT indicates ground truth. We use CRNN as the recognizer. Red\/black characters indicate incorrectly\/correctly recognized.\n\n\\begin{table}\n\\begin{tabular}{c|c c} \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Metrics} \\\\ \\cline{2-3}  & Training cost & Inference cost \\\\ \\hline TG & 19.60 & 0.91 \\\\ +HiREN(Online) & 20.59 & 0.91 \\\\ +HiREN(Offline) & 19.60 & 0.91 \\\\ \\hline TPGSR & 7.20 & 7.20 \\\\ +HiREN(Online) & 8.19 & 7.20 \\\\ +HiREN(Offline) & 7.20 & 7.20 \\\\ \\hline \\end{tabular}\n\\end{table} TABLE VI: The training and inference costs of our method. The cost is measured by the FLOPs(G).\nas baselines and add HiREN to them and count their FLOPS during training and inference. The experimental results are presented in Tab. VI. In terms of training cost, we can see that the offline deployment of HiREN does not incur any additional cost. As for online version, we can see that the additional computational cost caused by HiREN is negligible (_e.g,_ from 19.60G to 20.59G, only 0.99G). What is more, neither of the two variants introduce any additional inference cost. In conclusion, the offline deployment not only saves training and inference cost, but also significantly boosts the performance. These results validate the efficiency of our method.\n\n### _Ablation Study_\n\nWe conduct extensive ablation studies to validate the design of our method. Since our method is designed to enhance HR images during training, the metric used in this section is the recognition accuracy measured by the average accuracy of CRNN on training set, denoted as \\(Acc_{train}\\).\n\n#### Iv-D1 Design of the HR enhancement branch\n\nHere, we check the design of the HR enhancement branch. As mentioned above, two techniques are developed to promote the enhancement of HR images: kernel-guided enhancement network \\(f_{ke}\\) and the loss \\(\\mathcal{L}_{HR}\\). We conduct experiments to check their effects. The experimental results are presented in Tab. VII. Visualization of the effect of the HR enhancement branch is given in the supplementary materials.\n\n_The effect of the HR enhancement branch._ Comparing the results in the 1st and 7th rows of Tab. VII, we can see that the HR enhancement branch lifts the accuracy from 66.9% to 74.1%, which proves the effect of the branch as a whole.\n\n_The effect of kernel-guided enhancement network._ To check the power of the kernel-guided enhancement network, we design a variant that removes the kernel predictor. Comparing the results of the 2nd and 7th rows in Tab. VII, we can see that the variant without the kernel predictor is inferior to that with the kernel predictor (72.7% v.s. 74.1%). This demonstrates the effectiveness of the proposed kernel-guided enhancement network.\n\n_The design of loss function._ Here, we check the design of the loss function used in the HR enhancement branch. We first remove the recognition loss \\(\\mathcal{L}_{rec}\\) and the style loss \\(\\mathcal{L}_{sty}\\) separately. As can be seen in the 3rd, 4th, and 7th rows in Tab. VII, comparing with the combined one, the performance of using only one single loss is degraded. Next, we check the selection of style loss. Specifically, we consider three candidates (MSE, Charbonnier and L1) for the style loss function. As can be seen in the 5th, 6th, and 7th rows of Tab. VII, MSE loss outperforms Charbonnier loss [64] and L1 loss. The reason lies in that MSE penalizes large errors and is more tolerant to small errors, which is more suitable for HiREN to enhance the blurry or missed character details and keep the style unchanged [65]. Ergo, MSE is selected as the style loss in HiREN.\n\n#### Iv-D2 Hyper-parameter study\n\nHere, we provide the grid search results of the hyper-parameter \\(\\alpha\\) introduced in HiREN for balancing the two losses. The results are presented in Tab. VIII. As can be seen in Tab. VIII, the best performance is achieved when \\(\\alpha\\)=0.1 and 0.05.\n\n#### Iv-D3 The effect of loss quality estimation module\n\nHere, we compare the performances of different models w\/o the quality estimation module. As can be seen in Tab. IX, without \\(f_{QE}\\), all methods are degraded, which demonstrates the effect of the quality estimation module.\n\n## V Discussion\n\nIn this section, we discuss some issues to better demonstrate the advantages of HiREN and point out some limitations of the proposed method.\n\n### _Which kind of quality issues do HR images have?_\n\nWe conduct a visualization study to demonstrate the quality issues of HR images. As can be checked in Fig. 5, HR images are suffering from including but not limited to low-contrast (1st, 2nd and 6th cases), blurry (3rd and 4th cases) and motion blur (5th case). These unknown degradations obviously threaten the recognition of HR images and subsequently provide erroneous supervision to the recovery of the LR images.\n\n### _How does HiREN lift the quality of supervision information?_\n\nTo cope with various quality problems of HR images, HiREN generates HQ images through different strategies. In particular, HiREN makes the texts more prominent to solve low-contrast (e.g. the 1st and 2nd cases in Fig. 5). With respect to the blurry issue, HiREN makes the incorrectly recognized texts more distinguishable (e.g. \"e\" in the 3rd case and \"ri\" in the 4th case in Fig. 5). HiREN also tries to reduce the motion blur in the 5th case of Fig. 5. Although in some tough cases, HiREN fails to generate a correct HQ image (e.g. the 6th case in Fig. 5), our quality estimation module weights its loss to a small value to suppress the erroneous supervision information.\n\n\\begin{table}\n\\begin{tabular}{c|c c c c c} \\hline \\hline Method & SRCNN & TSRN & TG & TFGSR \\\\ \\hline without \\(f_{QE}\\) & 30.2\\% & 44.2\\% & 51.0 & 51.9\\% \\\\ with \\(f_{QE}\\) & **30.4**\\% & **45.0**\\% & **51.1** & **52.4**\\% \\\\ \\hline \\hline \\end{tabular}\n\\end{table} TABLE IX: Ablation study on the quality estimation module. The metric is the recognition accuracy of CRNN on the test set of TextZoom.\n\n\\begin{table}\n\\begin{tabular}{c|c c c|c} \\hline \\hline \\multirow{2}{*}{ID} & \\multirow{2}{*}{Kernel-guided} & \\multicolumn{3}{c|}{Loss functions} & \\multirow{2}{*}{\\(Acc_{train}\\)} \\\\ \\cline{2-2} \\cline{4-5}  & & \\(\\mathcal{L}_{rec}\\) & & \\(\\mathcal{L}_{sty}\\) \\\\ \\hline \\hline\n1 & \u2717 & \u2717 & \u2717 & 66.9 \\\\ \\hline\n2 & \u2717 & \u2713 & MSE & 72.7 \\\\\n3 & \u2713 & \u2713 & \u2717 & 66.1 \\\\\n4 & \u2713 & \u2717 & MSE & 67.4 \\\\\n5 & \u2713 & \u2713 & Charb & 67.5 \\\\\n6 & \u2713 & \u2713 & L1 & 67.3 \\\\\n7 & \u2713 & \u2713 & MSE & 74.1 \\\\ \\hline \\hline \\end{tabular}\n\\end{table} TABLE VII: The ablation studies of the HR enhancement branch. Here, \u2717 means the corresponding module is not applied, and Charbonnier Loss [64].\n\n\\begin{table}\n\\begin{tabular}{c|c c c c c c} \\hline \\hline \\multirow{2}{*}{Metric} & \\multicolumn{6}{c}{\\(\\alpha\\)} \\\\ \\cline{2-7}  & 0.5 & 0.2 & 0.1 & 0.05 & 0.025 & 0.01 & 0.005 \\\\ \\hline \\(Acc_{train}\\) & 73.6 & 73.4 & **74.1** & **74.1** & 72.3 & 72.2 & 71.2 \\\\ \\hline \\hline \\end{tabular}\n\\end{table} TABLE VIII: The determination of \\(\\alpha\\). The metric is \\(Acc_{train}\\).\n### _Error Analysis_\n\nIn this section, we perform an error analysis of HiREN to provide possible research directions for further works. Concretely, we provide some error cases in Fig. 6 to illustrate the limitations of recent works and HiREN. As can be seen in the 1st\\(\\sim\\)2nd cases, recent methods usually rely on a vocabulary [66], which makes the models guess the blurry pixels via the corpus that can be learned from the training dataset. This degrades the models' ability to recover numbers and punctuation. As a result, although HiREN recovers more characters than the original TPGSR, the word-level recovery still fails. Besides, as shown in the 3rd case, in some tough cases where the LR and HR images are extremely difficult to read, TPGSR and HiREN also fail to effectively do the recovery. This indicates the challenge of STISR.\n\n### _Limitations of HiREN_\n\nOn the one hand, HiREN may introduce some noise to the HR images and worsen their quality. However, such noise is very minor compared to the advantage brought by HiREN. Specifically, we find that 9,565 erroneously recognized images in TextZoom dataset are successfully enhanced by HiREN, which leads to correct recognition results, while only 128 images are deteriorated from correct to wrong. On the other hand, the training of the HR enhancement branch requires the feedback of a scene text recognizer and text-level annotations. This indicates that HiREN still needs some weak supervision information for supervision.\n\n## VI Conclusion\n\nIn this paper, we present a novel framework called HiREN to boost STISR performance. Different from existing works, HiREN aims at generating high-quality text images based on high-resolution images to provide more accurate supervision information for STISR. Concretely, recognizing the difficulty in catching the degradation from HQ to HR and obtaining the supervision information from HR images, we explore degradation kernel-guided super-resolution and the feedback of a recognizer as well as text-level annotations as weak supervision to train a HR enhancement branch. What is more, to suppress erroneous supervision information, a novel quality estimation module is designed to evaluate the qualities of images, which are used to weight their losses. Extensive experiments demonstrate the universality, high-performance and efficiency of HiREN. Our work provides a new solution for the STISR task.\n\nIn the future, we will try to explore more advanced models to further advance the proposed technique. One the one hand, we will try to further improve the recovery ability of the HR enhancement branch or address the vocabulary reliance issue. On the other hand, we plan to apply HiREN to self-supervised or unsupervised settings when the recognizer and text-level annotations are not trustworthy or text-level annotations are lack during training. Last but not least, we will extend the idea of the proposed quality enhancement branch to build a new noisy learning algorithm for STISR.\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2307.16410v1.pdf"}
{"id":2304.00044,"title":"On The Theory of Ring Afterglows","abstract":"Synchrotron and inverse Compton emission successfully explain the observed\nspectra of gamma-ray burst (GRB) afterglows. It is thought that most GRBs are\nproducts of extremely relativistic outflows and the afterglow marks the\ninteraction of that ejecta with the surrounding matter. Faster decay of\nafterglow light curves at late times is indicative of non-spherical geometries,\nand are usually interpreted as evidence for jet geometry. Recent numerical\nsimulations have shown that ring-like geometries are also permissible for\nrelativistic outflows. We therefore extend the standard theory of afterglow\nevolution to ring geometries. An analytic prescription for the light curves and\nspectra produced by relativistic toroidal blast waves is presented. We compare\nthese to their spherical and jet-like counterparts, and show that ring\nafterglows decay faster than spherical outflows but not as fast as jets.","authors":"Marcus DuPont, Andrew MacFadyen, Re'em Sari","published_date":"2023-03-31T18:02:12Z","link":"http:\/\/arxiv.org\/abs\/2304.00044v1","markdown":"# On The Theory of Ring Afterglows\n\n###### Abstract\n\nSynchrotron and inverse Compton emission successfully explain the observed spectra of gamma-ray burst (GRB) afterglows. It is thought that most GRBs are products of extremely relativistic outflows and the afterglow marks the interaction of that ejecta with the surrounding matter. Faster decay of afterglow light curves at late times is indicative of non-spherical geometries, and are usually interpreted as evidence for jet geometry. Recent numerical simulations have shown that ring-like geometries are also permissible for relativistic outflows. We therefore extend the standard theory of afterglow evolution to ring geometries. An analytic prescription for the light curves and spectra produced by relativistic toroidal blast waves is presented. We compare these to their spherical and jet-like counterparts, and show that ring afterglows decay faster than spherical outflows but not as fast as jets.\n\nGamma-Ray Bursts (629) -- Light curves (918) -- Relativistic Fluid Dynamics (1389) +\nFootnote \u2020: journal: ApJL\n\n0000-0002-8861-7885]Marcus DuPont\n\n0000-0002-4880-0885]Andrew MacFadyen\n\n0000-0002-0788-0885]Re'em Sari\n\n## 1 Introduction\n\nThe physics accounting for the variability and wide range in observed luminosities of gamma-ray bursts (GRBs), and the nature of their central engine are topics of deep debate. However, it is widely accepted that the dominant processes responsible for the X-ray, optical, and radio afterglow radiation are the synchrotron and inverse Compton mechanisms operating behind the blast wave that the GRB launches into the surrounding medium. Such radiative processes are expected to be applicable to just about any sufficiently relativistic outflow. This paved the way for the success of using the Blandford & McKee (1976) (BM) solution for modelling GRB afterglows and for distinguishing between isotropic and jet-like asymmetric outflows modelled as BM solutions truncated to within polar angle \\(\\theta_{0}\\)(see Piran, 2004, and references therein). Thus far, only afterglows for spherical and jet-like outflows have been considered and it is generally believed that most GRBs are caused by jetted relativistic outflows. Currently, the key indicators cited as evidence for GRB jets are: (a) the existence of an achromatic break in the afterglow light curve either due to lateral jet spreading (Rhoads, 1999; Sari et al., 1999) or an off-axis viewing of universal structured jets (e.g., Zhang & Meszaros, 2002; Rossi et al., 2002); (b) observed net polarizations that arise from asymmetric, relativistically beamed outflows (e.g., Gruzinov & Waxman, 1999; Sari, 1999; Yonetoku et al., 2011; Mandarakas et al., 2023); (c) extremely large energetics which require the outflow to be sufficiently collimated since the average _isotropic_ energy of \\(10^{55}\\) erg released by GRBs is much larger than what is physically allowed by a spherical explosion of a massive star (Taylor et al., 2004; Kumar & Zhang, 2015); (d) and measurements of proper motion of the flux centroid (Czerny et al., 1997; Taylor et al., 2004; Mooley et al., 2018).\n\nInsofar as shown by the the previous conditions and observations, many GRBs are only constrained to be _asymmetric_ outflows, but we argue they are not necessarily jet-like. This stance is valid since the current GRB afterglow catalogue is quite varied and many of them show breaks which do not fit the jet theory. Recently, it has been shown that relativistic outflows can have ring-like geometries, e.g. from the \"ellipsar\" mechanism (DuPont et al., 2022). Motivated by the result of DuPont et al. (2022), we consider in this Letter the dynamics and observational signatures of expanding relativistic rings, though we remain agnostic about the source and ener\ngies of said rings. Our work on ring afterglows is motivated by the many time-domain surveys in progress or being planned (Barthelmy et al., 2005; Shappee et al., 2014; Chambers et al., 2016; Kochanek et al., 2017; Ivezic et al., 2019; Bellm et al., 2019), which observe a wide array of astrophysical transients -- outside of just GRBs -- that expanding relativistic rings might help explain. These transients might include X-ray flashes (XRFs), Super Luminous Supernovae (SLSNe), trans-relativistic supernovae, and Fast Blue Optical Transients (FBOTs). Therefore, we are motivated to ask how the afterglow of expanding relativistic rings differs from their spherical and jet-like counterparts.\n\nIn this Letter, we calculate the light curves and spectra due to expanding relativistic rings. We invoke the same recipes described in Sari et al. (1998) and Sari et al. (1999), which have been successful at modeling many observed GRB afterglows. We derive temporal scalings for the relevant frequencies and spectral flux and comment on their differences from the spherical and jet-like afterglows.\n\nThis Letter is organized as follows: Section 2 describes the mathematical formalism for the dynamics and synchrotron radiation from the relativistic ring, Section 3 describes the resultant light curves of the ring-like outflows, and Section 4 discusses the relevance of our work.\n\n## 2 Formalism\n\n### Blast wave evolution\n\nIn the early phase of evolution before the expanding blast wave begins to decelerate, if it is expanding in a medium with density obeying \\(\\rho=Ar^{-k}\\), it has kinetic energy\n\n\\[E\\approx\\Gamma^{2}M=\\frac{A}{3-k}\\Gamma^{2}r^{3-k}\\Omega, \\tag{1}\\]\n\nwhere \\(M\\) is the swept up mass, \\(\\Gamma=(1-\\beta^{2})^{-1\/2}\\) is the Lorentz factor of the bulk flow, \\(\\beta\\) is velocity in units of \\(c\\), \\(A\\) is the mass-loading parameter, and \\(\\Omega\\) is the solid angle of the blast wave which obeys\n\n\\[\\Omega=\\begin{cases}4\\pi\\sin(\\theta_{0})&\\text{ring},\\\\ 8\\pi\\sin^{2}(\\theta_{0}\/2)&\\text{jets},\\\\ 4\\pi&\\text{sphere},\\end{cases} \\tag{2}\\]\n\nwhere \\(\\theta_{0}\\) is the half-opening angle of the blast wave(s) such that \\(\\Omega\\to 4\\pi\\) as \\(\\theta_{0}\\to\\pi\/2\\). For small opening angles, \\(\\Omega_{\\text{ring}}\\approx 4\\pi\\theta_{0}\\), which is a factor \\(2\/\\theta_{0}\\) larger than its double-sided jet-like counterpart, making relativistic rings more likely to be observed, as compared to a jet with the same opening angle. An illustration of the asymmetric geometries considered is shown in Figure 1. As evident from Figure 1 and from Equation 2, the solid angle for a ring complements the solid angle of a jet to \\(4\\pi\\) if \\(\\theta_{\\text{ring}}=\\pi\/2-\\theta_{\\text{jet}}\\).\n\nUsing conservation of energy, as the relativistic ring slows down such that \\(\\Gamma\\sim\\theta_{0}^{-1}\\), one finds \\(\\Gamma\\propto r^{-(3-k)}\\). This happens after an observer time of:\n\n\\[t_{\\text{b}}\\approx[\\zeta E_{\\text{iso}}\/4\\pi A]^{1\/\\zeta}(\\theta_{0}+\\theta_{ \\text{obs}})^{2(1+\\zeta)\/\\zeta}, \\tag{3}\\]\n\nwhere \\(E_{\\text{iso}}\\) is the isotropic-equivalent energy and \\(\\zeta\\equiv 3-k\\). Before this break time, the afterglow from rings and from jets are identical due to a lack of causal connectivity. The crux of this Letter is that after this break time, light curves from jets and from rings diverge and their distinguishing features are discernible in the current GRB catalogue. We will explore the previous point in later sections.\n\nAs the blast wave evolves, an observer sees photons at a time\n\n\\[t=t^{\\prime}(1-\\vec{\\beta}\\cdot\\hat{n})=t^{\\prime}(1-\\beta\\mu), \\tag{4}\\]\n\nwhere \\(t^{\\prime}\\) is the time in the emitter frame, \\(\\hat{n}\\) is a unit vector pointing from the observer to the emitting patch, and \\(\\mu\\equiv\\cos\\theta\\). Hereafter, all primed quantities signify values in the emitter frame. Assuming \\(\\Gamma\\gg 1\\) and the observer is nearly perfectly oriented with the emitting patch (i.e., \\(\\mu\\approx 1-\\theta^{2}\/2\\)), we have\n\n\\[t\\approx\\frac{t^{\\prime}}{2\\Gamma^{2}}[1+(\\Gamma\\theta)^{2}]\\approx\\frac{r}{2 \\Gamma^{2}}[1+(\\Gamma\\theta)^{2}], \\tag{5}\\]\n\nwhere we have used \\(t^{\\prime}\\approx r\\) for sufficiently relativistic flows which lie on the light sphere. Since the radiation is beamed into a typical angle \\(1\/\\Gamma\\), the quantity \\(\\Gamma\\theta\\) is of order unity, simplifying the observer time to \\(t\\approx r\/\\Gamma^{2}\\). From this, we arrive at the Lorentz factor as a function of observer time for the ring, \\(\\Gamma\\propto t^{-\\zeta\/(1+2\\zeta)}\\). Furthermore, the relativistic ring's radial evolution obeys \\(r\\propto t^{1\/(1+2\\zeta)}\\) after spreading begins.\n\n### Synchrotron Spectrum\n\nIn the observer frame, the characteristic peak frequency of the electrons is\n\n\\[\\nu_{m}=\\Gamma\\gamma_{e}^{2}\\frac{3eB^{\\prime}}{16m_{e}}\\propto\\Gamma^{4} \\propto t^{-4\\zeta\/(1+2\\zeta)}, \\tag{6}\\]\n\nwhere \\(\\gamma_{e}\\) is the electron Lorentz factor, \\(e\\) is elementary charge, \\(B^{\\prime}\\) is the magnetic field in the fluid frame, and \\(m_{e}\\) is the electron mass. Note that we have used the fact that the magnetic field in the down stream transforms from the usual jump condition \\(B^{\\prime}=\\Gamma\\sqrt{32\\pi\\rho\\epsilon_{B}}\\), where \\(\\epsilon_{B}\\) is fraction of total energy density due to magnetic fields, and the minimum Lorentz factor of the electrons obeys \\(\\gamma_{e}\\propto\\Gamma\\). In a time \\(t^{\\prime}\\), the electrons cool at a rate\n\\[\\langle P(\\gamma_{e})\\rangle=\\frac{4}{3}\\sigma_{T}u^{2}\\gamma_{e}^{2}U_{b}. \\tag{7}\\]\n\nIn the above equation, \\(\\sigma_{T}\\) is the Thompson cross section, \\([u^{\\mu}]=\\Gamma(1,\\vec{\\beta})\\) is the four-velocity in units where \\(c=1\\), and \\(U_{b}=B^{2}\/8\\pi\\) is the magnetic energy density. By inverting Equation 7, we solve for the cooling Lorentz factor,\n\n\\[\\gamma_{c}=\\frac{6\\pi m_{e}}{\\Gamma t^{\\prime}\\sigma_{T}B^{2}}=\\frac{6\\pi m_{ e}}{\\Gamma^{3}t\\sigma_{T}B^{2}}. \\tag{8}\\]\n\nIt then immediately follows that the cooling frequency obeys\n\n\\[\\nu_{c}=\\Gamma\\gamma_{c}^{2}\\frac{3eB^{\\prime}}{16m_{e}}\\propto\\Gamma^{-4}t^{ -2}\\propto t^{-2\/(1+2\\zeta)}. \\tag{9}\\]\n\nThe spectral flux from a radiating blast wave is given by\n\n\\[F_{\\nu}=\\frac{1+z}{4\\pi d_{L}^{2}}\\int_{V}\\delta^{2}j^{\\prime}_{\\nu}d^{3}\\vec{ x}, \\tag{10}\\]\n\nwhere \\(z\\) is redshift, \\(d_{L}\\) is luminosity distance, \\(\\delta=1\/\\Gamma(1-\\vec{\\beta}\\cdot\\hat{n})\\) is the Doppler beaming factor with respect to the observer, and \\(j^{\\prime}_{\\nu}\\) is the frequency-dependent emissivity. At peak emission, the emissivity is independent of \\(\\Gamma\\) and a highly relativistic flow along the line of sight to the observer gives \\(\\delta=2\\Gamma\\), so the peak spectral flux has the scaling\n\n\\[F_{\\nu,\\rm max}\\propto r^{3}\\Gamma^{2}\\propto t^{(3-2\\zeta)\/(1+2\\zeta)}. \\tag{11}\\]\n\nFor completeness, we do not assume that all synchrotron photons escape the plasma on their way to the observer, meaning some are self absorbed. Moreover, the self-absorption frequency is a difficult calculation, but by extrapolating from the Granot et al. (1999a) solution we can arrive at the simple scaling,\n\n\\[\\nu_{a}\\propto E^{1\/5}\\propto\\Gamma^{2\/5}r^{\\zeta\/5}\\propto t^{-\\zeta\/5(1+2 \\zeta)}. \\tag{12}\\]\n\nFrom this, we now have the necessary ingredients to compute light curves produced by relativistic rings.\n\n## 3 Light curves of relativistic rings\n\nWith the necessary constraints derived in the previous section, we now turn to explicit light curve calculations. Hereafter, we compute light curves for a constant density medium (i.e., \\(\\zeta=3\\)) to easily compare with the spherical and jet-like geometries derived in Sari et al. (1999).\n\nFigure 1: Cartoon illustrations of the two types of asymmetric geometries considered in this Letter. The left shows the conical, jet-like outflow along the poles of the source while the right shows the ring-like outflow in the equatorial plane of the source. The half-opening angle, \\(\\theta_{0}\\), is depicted for both geometries as well.\nWe start with the flux at low enough frequencies, such that some photons are self absorbed. Assuming that the time-averaged source emits at the characteristic \\(\\nu_{m}\\), if \\(\\nu_{a}\\ll\\nu_{m}\\), then because most of the electrons are emitting at typical synchrotron frequencies much larger than \\(\\nu_{a}\\), the spectral flux is proportional to \\(\\nu^{2}\\) as opposed to \\(\\nu^{5\/2}\\)(Katz, 1994). Thus, we have\n\n\\[F_{\\nu<\\nu_{a}}\\propto\\left(\\frac{\\nu}{\\nu_{a}}\\right)^{2}\\left( \\frac{\\nu_{a}}{\\nu_{m}}\\right)^{1\/3}F_{\\nu,\\max}\\propto r^{2}\\propto\\begin{cases} t^{2\/7}&\\text{ring,}\\\\ \\text{constant}&\\text{jet,}\\\\ t^{1\/2}&\\text{spherical,}\\end{cases} \\tag{13}\\] \\[F_{\\nu_{a}<\\nu<\\nu_{m}}\\propto\\left(\\frac{\\nu}{\\nu_{m}}\\right)^{ 1\/3}F_{\\nu,\\max}\\propto r^{3}\\Gamma^{2\/3}\\propto\\begin{cases}t^{1\/7}&\\text{ring,} \\\\ t^{-1\/3}&\\text{jet,}\\\\ t^{1\/2}&\\text{spherical,}\\end{cases} \\tag{14}\\]\n\nfor the flux below the self-absorption frequency and the intermediate flux between the self-absorption and characteristic frequency, respectively. This indicates that slopes would rise as long as the evolution were spherical or ring-like, but the slopes are different enough to perfectly distinguish between the two geometries. Moreover, there is a stark contrast from the latter geometries when compared with the \\(t^{-1\/3}\\) decay of the jet once it begins spreading. At high frequencies, the light curves follow\n\n\\[F_{\\nu_{m}<\\nu_{c}<\\nu}\\propto\\Gamma^{2}r^{3}\\left(\\frac{\\nu_{c} }{\\nu_{m}}\\right)^{-(p-1)\/2}\\left(\\frac{\\nu}{\\nu_{c}}\\right)^{-p\/2}\\propto \\begin{cases}t^{-2(3p-1)\/7}&\\text{ring,}\\\\ t^{-p}&\\text{jet,}\\\\ t^{-(3p-2)\/4}&\\text{spherical,}\\end{cases} \\tag{15}\\] \\[F_{\\nu_{m}<\\nu<\\nu_{c}}\\propto\\Gamma^{2}r^{3}\\left(\\frac{\\nu}{ \\nu_{m}}\\right)^{-(p-1)\/2}\\propto\\begin{cases}t^{-3(2p-1)\/7}&\\text{ring,}\\\\ t^{-p}&\\text{jet,}\\\\ t^{-3(p-1)\/4}&\\text{spherical,}\\end{cases} \\tag{16}\\]\n\nfor cooling electrons and for non-cooling electrons, respectively. In Equations 15 & 16, \\(p\\) is the electron distribution power-law index. Here we witness that ring afterglows possess two distinct cooling breaks analogous to whats been calculated for spherical outflows. Furthermore, our calculation evidences a very clear distinction between afterglows produced by relativistic rings and jets. A graphical depiction of this distinction is shown in Figure 2. We've shown _very_ distinct features such as differences in cooling breaks, and, more importantly, ring afterglows have shallower decay slopes than jets throughout the entirety of their evolution. The consequences of these revelations are discussed in the next section.\n\n## 4 Discussion\n\nWe have demonstrated that temporal evolution of ring afterglows is clearly distinct from their spherical and jet-like counterparts. While it is likely that classical GRBs are products of very energetic asymmetric flows, the geometry of said outflow is not well constrained. The jet model has been instrumental in its explanations of steep decays as resulting from highly collimated outflows. Yet, there exist observations which cannot be fit using the jet framework. Some light curves -- such as those produced by GRB 030329 (Stanek et al., 2003) or the more recent GRB 221009A (Williams et al., 2023) -- have very shallow breaks, which are hard to reconcile using top-hat\njet models. In particular, GRB 221009A was reported by Williams et al. (2023) to favor a broken power-law model in the X-ray with flux decay slopes steepening from \\(t^{-1.498\\pm 0.004}\\) to \\(t^{-1.672\\pm 0.008}\\) with a jet break time of \\(t_{b,\\rm X-ray}\\sim 8\\times 10^{4}\\,\\rm s\\). The timing of such steepening might be due to a jet with half-opening angle of \\(3.5^{\\circ}\\)(D'Avanzo et al., 2022). However, the light curve does not steepen beyond the decay index \\(\\alpha\\approx 1.7\\) -- where \\(F_{\\nu}\\propto t^{-\\alpha}\\) -- after the break, and observers cannot match this shallow X-ray decay index with what is predicted using a simple on-axis top-hat jet. For typical values \\(p\\cong 2.4\\), the top-hat jets predict \\(\\alpha>2\\), but rings predict \\(1.63<\\alpha<1.77\\), well within the required range for GRB 221009A. Therefore, one can interpret this GRB as stemming from either a more structured jet configuration or an expanding relativistic ring.\n\nThe notion of some astrophysical transients being sourced from expanding relativistic rings, rather than jets, have the following implications: (a) the probability of viewing ring afterglows is larger than that of jets by a factor \\(2\/\\theta_{0}\\). A blast wave with half-opening angle 0.1 radians, if oriented as a jet, would cover 0.5% of the sky while an expanding ring covers 10%, larger by a factor of 20. As a result, ring geometries, as compared to jet geometries, bring down the required source rates significantly; (b) as demonstrated by DuPont et al. (2022) relativistic rings can be born purely from geometrical and hydrodynamic effects as opposed to the more complex central engines required for producing classical jets; (c) the late-time evolution of the relativistic ring is much more stable than the jet since the spreading of the relativistic ring is effectively one dimensional and is therefore a candidate for light curves with shallower breaks (d) around the time of the ring break, when the emitting patch is no longer locally spherical, the specific intensity (surface brightness) is no longer symmetric about the line of sight to the observer for a general viewing angle (Granot et al., 1999, 2007; van Eerten et al., 2010). This fact can act as useful probe for the underlying dynamics of rings which can be further detailed by direct hydrodynamic simulations in the near future. Detailed analysis of scintillation patterns may be sensitive to the surface brightness distribution (Goodman, 1997) and may help distinguish jets from rings.\n\nThis Letter has considered synchrotron emission which is more readily observed in radio and optical frequencies. At higher frequencies, inverse Compton emission may dominate. Adding the inverse Compton component could be done in a similar way to Sari and Esin (2001), or its extension by Nakar et al. (2009) if Klein Nishina corrections are important.\n\nIn summary, we've considered the dynamics and observational signatures of expanding relativistic rings which preserve the notion of beaming, can account for shallow breaks as observed in many GRB light curves, and do not require a complex central engine to achieve their geometric configuration. Our investigation is inspired by the work of DuPont et al. (2022), where rings arise naturally, albeit with lower energy than needed to explain cosmological afterglows for the conditions considered in that work. Moreover, while the main focus of this work has been GRBs, we emphasize that the importance of our calculations are the unique features presented by the ring geometry, while the energetics can\n\nFigure 2: Pictorial light curves for the spherical, ring-like, and jet-like blast waves, respectively. The left and right panels show the typical light curve behavior at low (\\(\\sim\\) radio) and high (\\(\\sim\\) optical and X-ray) observed frequencies, respectively. The slopes are segmented in time between the break time, \\(t_{b}\\), and the times when the break frequencies \\(\\nu_{m}\\) and \\(\\nu_{c}\\) cross the observed frequency \\(\\nu\\). The vertical dashed line for \\(t_{c}\\) is broken at the jet curve in the left panel since \\(\\nu_{c}\\) is constant for that geometry and it therefore has no corresponding \\(t_{c}\\). In both frequency bands, we show the divergence in flux decay rate once the break time is reached with the low frequency band showing the clearest separation between the various phases of evolution.\nbe scaled appropriately and applied to a broader scope of astrophysical transients. Therefore, we suggest that ring-like outflows should be considered when interpreting observations of non-spherical explosions.\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2304.00044v1.pdf"}
{"id":2309.12494,"title":"Evidential uncertainty sampling for active learning","abstract":"Recent studies in active learning, particularly in uncertainty sampling, have\nfocused on the decomposition of model uncertainty into reducible and\nirreducible uncertainties. In this paper, the aim is to simplify the\ncomputational process while eliminating the dependence on observations.\nCrucially, the inherent uncertainty in the labels is considered, the\nuncertainty of the oracles. Two strategies are proposed, sampling by Klir\nuncertainty, which tackles the exploration-exploitation dilemma, and sampling\nby evidential epistemic uncertainty, which extends the concept of reducible\nuncertainty within the evidential framework, both using the theory of belief\nfunctions. Experimental results in active learning demonstrate that our\nproposed method can outperform uncertainty sampling.","authors":"Arthur Hoarau, Vincent Lemaire, Arnaud Martin, Jean-Christophe Dubois, Yolande Le Gall","published_date":"2023-09-21T21:26:50Z","link":"http:\/\/arxiv.org\/abs\/2309.12494v2","markdown":"# Evidential uncertainties on rich labels\n\n###### Abstract\n\nRecent research in active learning, and more precisely in uncertainty sampling, has focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, we propose to simplify the computational phase and remove the dependence on observations, but more importantly to take into account the uncertainty already present in the labels, _i.e._ the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which addresses the exploration-exploitation problem, and sampling by evidential epistemic uncertainty, which extends the reducible uncertainty to the evidential framework, both using the theory of belief functions.\n\nKeywords:Active Learning Uncertainty sampling Belief Functions\n\n## 1 Introduction\n\nFor reasons of efficiency, cost or energy reduction in machine learning or deep learning, one of the important issues is related to the amount of data and in some cases, to the amount of labelled data. Active learning [19] is a part of machine learning in which the learner can choose which observation to label in order to work with only a fraction of the labeled dataset to reduce the labeling cost. For this purpose, the learner uses a strategy that allows it to select only certain observations that will then be labeled. Among all the proposed strategies in the literature [1, 19] one of the best known is sampling by uncertainty [15].\n\nIn uncertainty sampling, the learner selects the instances for which it is most uncertain. The measures used to quantify this uncertainty, such as entropy, are up to now probabilistic. In this paper, we propose to use a broader framework of uncertainty that generalizes probabilities.\n\nAs proposed in recent papers [10, 11, 18] the uncertainty can be decomposed into two interesting terms: the epistemic and the aleatoric uncertainties. Aleatoric uncertainty arises from the stochastic property of the event and is therefore not reducible, whereas epistemic uncertainty is related to a lack of knowledge and can be reduced. Proposed calculations depend on the model prediction but also on the observations. We suggest in this paper, to get rid of the direct dependence on the observations and to use only the model output for similar results. This representation also addresses the exploration-exploitation\nproblem in active learning, with the possibility of choosing one or the other, or even a compromise as in [2].\n\nThe labeling process is often carried out by humans [7, 17]; without making any difference between a label given by someone who has hesitated for a long time and a label given by someone who has no doubt, and therefore uncertainty may already exist in the labels. This information is not taken into account in most models and sampling strategies. In the case of supervised classification, several models are now able to handle these uncertain labels [4, 5, 6, 23]. The main objective, in addition to not being dependent on observations and to address the problem of exploration-exploitation, is to take into account in the sampling, the uncertainty already present in the labels.\n\nGiven the above, we propose in this paper two uncertainty sampling strategies capable of representing a decomposition of the model uncertainties with regard to the uncertainty already present in the labels.\n\nThe first strategy is based upon two different uncertainties, the discord (how self-conflicting the information is) and non-specificity (how ignorant the information is) in the model output. The second strategy extends the epistemic uncertainty to the evidential framework and to several classes, thus simplifying the computation.\n\nThe paper is organized as follows; section 2 introduces some important notions of imperfect labeling and the modeling of these richer labels using the theory of belief functions. The usual uncertainty sampling approach [15] is also recalled and section 3 describes the separation between aleatoric and epistemic uncertainties. Section 4 presents the two new proposed strategies, section 5 shows an application on a real world dataset, then section 6 discusses and concludes the article. The experiments performed in this paper are described in supplementary materials, to avoid lengthy explanations, since the purpose of the paper does not lie in this part. Furthermore, uncertainties are mapped on 2D representations but the objective is to later serve active learning.\n\n## 2 Preliminaries\n\nIn this section, we introduce some general knowledge useful to understand the rest of the paper, starting with rich labels, modeled by the theory of belief functions and ending with the classical approach of sampling by uncertainty.\n\n#### 2.0.1 Imperfect labeling -\n\nMost of the datasets used for classification consider hard labels, with a binary membership where the observation is either a member of the class or not. In this paper, we refer as rich labels the elements of response provided by a source that may include several degrees of imprecision (_i.e._ \"_This might be a cat_\", \"_I don't know_\" or \"_I am hesitating between dog and cat, with a slight preference for cat_)\". Such datasets, offering uncertainty already present in the labels, exist [22] but are not numerous. These labels are called rich in this paper since they provide more information than hard labels and can be modeled using the theory of belief functions.\nTheory of belief functions -\n\nThe theory of belief functions [3; 20], is used in this study to model uncertainty and imprecision for labeling and prediction. Let \\(\\Omega=\\{\\omega_{1},\\ldots,\\omega_{M}\\}\\) be the frame of discernment for \\(M\\) exclusive and exhaustive hypotheses. It is assumed that only one element of \\(\\Omega\\) is true (closed-world assumption) [21]. The power set \\(2^{\\Omega}\\) is the set of all subsets of \\(\\Omega\\). A mass function assigns the belief that a source may have about the elements of the power set of \\(\\Omega\\), such that the sum of all masses is equal to 1.\n\n\\[m:2^{\\Omega}\\rightarrow[0,1],\\sum_{A\\in 2^{\\Omega}}m(A)=1. \\tag{1}\\]\n\nEach subset \\(A\\in 2^{\\Omega}\\) such as \\(m(A)>0\\) is called a _focal element_ of \\(m\\). The uncertainty is therefore represented by a mass \\(m(A)<1\\) on a focal element \\(A\\) and the imprecision is represented by a non-null mass \\(m(A)>0\\) on a focal element \\(A\\) such that \\(|A|>1\\).\n\nA mass function \\(m\\) is called _categorical mass function_ when it has only one focal element such that \\(m(A)=1\\). In the case where \\(A\\) is a set of several elements, the knowledge is certain but imprecise. For \\(|A|=1\\), the knowledge is certain and precise.\n\nOn decision level, the pignistic probability \\(BetP\\)[21] helps decision making on singletons:\n\n\\[BetP(\\omega)=\\sum_{A\\in 2^{\\Omega},\\ \\omega\\in A}\\frac{m(A)}{|A|}. \\tag{2}\\]\n\nIt is also possible to combine several mass functions (beliefs from different sources) into a single body of evidence. If the labels and therefore the masses are not independent, a simple average of the mass functions \\(m_{j}\\) derived from \\(N\\) sources can be defined as follows:\n\n\\[m(A)=\\frac{1}{N}\\sum_{j=1}^{N}m_{j}(A),\\ \\ A\\in 2^{\\Omega}. \\tag{3}\\]\n\nThere are other possible combinations that are more common than the mean, many of which are listed in [14].\n\n\\(\\bullet\\)**Example 1:** Let \\(\\Omega=\\{Cat,Dog\\}\\) be a frame of discernment. An observation labeled \"Cat\" by a source can be modeled in the framework of belief functions by the mass function \\(m_{1}\\) such as: \\(m_{1}(\\{Cat\\})=1\\) and \\(m_{1}(A)=0,\\ \\forall A\\in 2^{\\Omega}\\backslash\\{Cat\\}\\).\n\n\\(\\bullet\\)**Example 2:** An observation labeled \"Cat or Dog\" by a source can be modeled by the mass function \\(m_{2}\\) such as: \\(m_{2}(\\{Cat,Dog\\})=1\\) and \\(m_{2}(A)=0\\), \\(\\forall A\\in 2^{\\Omega}\\backslash\\{Cat,Dog\\}\\).\n\n\\(\\bullet\\)**Example 3:** The average mass function \\(\\bar{m}\\) of \\(m_{1}\\) and \\(m_{2}\\) is: \\(\\bar{m}(\\{Cat\\})=0.5\\), \\(\\bar{m}(\\{Cat,Dog\\})=0.5\\) and \\(\\bar{m}(A)=0\\) for all other subsets \\(A\\) in \\(2^{\\Omega}\\). Its pignistic probability \\(BetP\\), used for decision making is: \\(BetP(\\{Cat\\})=0.75\\) and \\(BetP(\\{Dog\\})=0.25\\).\n#### 2.1.1 Uncertainty sampling -\n\nActive learning iteratively builds a training set by selecting the best instances to label. The principle is, for a given performance or a given budget, to label as few observations as possible. Among all the strategies proposed in the literature [19] one of the best known methods is uncertainty sampling [13], where the function that defines the instances to be labeled maximizes the uncertainty related to the model prediction as described below.\n\nLet \\(\\mathcal{U}\\) be the uncertainty to label a new observation \\(x\\) for a given model and \\(\\Omega=\\{\\omega_{1},\\ldots,\\omega_{M}\\}\\) the set of the \\(M\\) possible classes. The uncertainty \\(\\mathcal{U}\\) can be calculated in several ways, a classical approach is to use Shannon's entropy:\n\n\\[\\mathcal{U}(x)=-\\sum_{\\omega\\in\\Omega}p(\\omega|x)\\text{log}[p(\\omega|x)], \\tag{4}\\]\n\nwith \\(p(\\omega|x)\\) the probability for \\(x\\) to belong to the class \\(\\omega\\), given by the model. Other uncertainty criteria exist, it is common to use the least confidence measure:\n\n\\[\\mathcal{U}(x)=1-\\max_{\\omega\\in\\Omega}[p(\\omega|x)]. \\tag{5}\\]\n\nMeasuring the uncertainty of a model to predict the class of some observations can be useful to find the areas of uncertainty in a space.\n\nFigure 1 represents three two-dimensional datasets, the classes are perfectly separated.\n\nGiven the model and one of the uncertainty criteria, we can compute the uncertainty of any point in space. For each dataset, the areas of uncertainty of the model are represented, with more red for more uncertainty. It is remarkable that these uncertainty areas can be compared to the decision boundaries of the model. Often, the closer the observation is to the decision boundary, the less confident the model is about its prediction.\n\nUncertainty sampling consists of choosing the observation for which the model is least certain of its prediction. This is one of the basis of active learning,\n\nFigure 1: Three 2-class datasets with areas of model uncertainty.\nhowever, other methods allow to extract more information about this uncertainty which leads to the decomposition into epistemic and aleatoric uncertainties.\n\n## 3 On the interest and limits of epistemic and aleatoric uncertainties for active learning\n\nIn this section, we introduce additional elements to decompose the uncertainty of the model so it can focus, in active learning, on the observations that will make it rapidly gain in performance.\n\nThe uncertainty \\(\\mathcal{U}(x)\\) can be separated into two uncertainties [9], one reducible and the other irreducible. The example1 of Figure 2 shows these two types of uncertainties, on 2a the result of a coin toss is uncertain and it is not possible to generate more knowledge to predict that the coin will flip heads or tails, this ignorance is called aleatoric uncertainty. On 2b either heads or tails is written in Finnish, it is an uncertainty that can be resolved by learning this language, it is called epistemic uncertainty.\n\nFootnote 1: This example is taken from Eyke Hullermeier\u2019s talk \u201cRepresentation and Quantification of Uncertainty in Machine Learning\u201d at the LFA2022 conference. In our example the word tails is written in Finnish, the word heads is called \u201cKruuna\u201d.\n\nBeing able to model these two uncertainties can help delimit where it is more interesting to provide knowledge and where it is useless. The total uncertainty \\(\\mathcal{U}(x)\\) is often represented as the sum of the epistemic uncertainty \\(\\mathcal{U}_{e}(x)\\) and the aleatoric uncertainty \\(\\mathcal{U}_{a}(x)\\): \\(\\mathcal{U}(x)=\\mathcal{U}_{e}(x)+\\mathcal{U}_{a}(x)\\).\n\nFor a two-class problem \\(\\Omega=\\{0,1\\}\\), it is proposed in [18] to model this uncertainty (here under the [15] formalism) by computing the plausibility \\(\\pi\\) of belonging to each of the two classes with the following formula, according to a probabilistic model \\(\\theta\\):\n\n\\[\\begin{split}\\pi(1|x)&=\\sup_{\\theta\\in\\Theta}\\,\\min[ \\pi_{\\Theta}(\\theta),p_{\\theta}(1|x)-p_{\\theta}(0|x)],\\\\ \\pi(0|x)&=\\sup_{\\theta\\in\\Theta}\\,\\min[\\pi_{\\Theta} (\\theta),p_{\\theta}(0|x)-p_{\\theta}(1|x)],\\end{split} \\tag{6}\\]\n\nwith \\(\\pi_{\\Theta}(\\theta)\\) depending on the likelihood \\(L(\\theta)\\) and the maximum likelihood \\(L(\\hat{\\theta})\\):\n\n\\[\\pi_{\\Theta}(\\theta)=\\frac{L(\\theta)}{L(\\hat{\\theta})}. \\tag{7}\\]\n\nFigure 2: Representation of aleatoric and epistemic uncertainties through the tossing of a coin and the word \u201cheads\u201d or \u201ctails\u201d written in Finnish.\nThe epistemic uncertainty is then high when the two classes are very plausible while the aleatoric uncertainty is high when the two classes are implausible:\n\n\\[\\begin{split}\\mathcal{U}_{e}(x)&=\\min[\\pi(1|x),\\pi(0|x )],\\\\ \\mathcal{U}_{a}(x)&=1-\\max[\\pi(1|x),\\pi(0|x)].\\end{split} \\tag{8}\\]\n\nThis calculation depends not only on the prediction of the model but also on the observations. To summarize, the fewer observations there are in a region, or the fewer decision elements there are to strongly predict a class, the higher the plausibility of the two classes, and the more reducible (and thus epistemic) the uncertainty is by adding knowledge.\n\nAn example is shown in Figure 3, a two-class dataset is shown in (a)a and the areas of model uncertainty are shown in (b)b according to the uncertainty sampling presented in the previous section. An horizontal line can be distinguished where the model uncertainty is highest. However, the sample represented in (a)a, shows that part of the uncertainty can be removed more easily by adding observations. In the same figure, three different datasets show how the sample can evolve by adding observations. Whatever the final distribution, the uncertainty on the left is not very reducible, while the uncertainty on the right can be modified by adding knowledge.\n\nThese two uncertainties can be calculated using equation (8), and are shown in Figure 4. The aleatoric uncertainty, and therefore irreducible, is represented in (a)a and the epistemic uncertainty, reducible, is represented in (b)b. The total uncertainty is then the sum of the two (c)c. The goal here is to use only the epistemic uncertainty, to know the areas where the model can learn new knowledge and where it will have more impact.\n\nFigure 3: Sample with areas of uncertainty according to the uncertainty sampling and three possible datasets based on the observations available in (a)a.\nUsing epistemic uncertainty as a sampling strategy is not reductive since it provides similar areas of uncertainty to those used previously, where epistemic and aleatoric uncertainty are indistinguishable.\n\nSuch information can be useful to find areas of reducible uncertainty, but it is not compatible with richer labels that also contain uncertainty. The way to compute this epistemic uncertainty is also dependent on the observations in addition to the model (_i.e._ the method could be oversimplified as: the model defines its zones of uncertainty, in which we look for the location with the smallest number of observations to define the reducible uncertainty.). Furthermore, the exploration-exploitation problem is not fully addressed. This leads to the next section in which two uncertainty sampling strategies for rich labels are proposed, they are also extended to several classes.\n\n## 4 Richer labels and multiple classes\n\nIn this section, we propose two uncertainty sampling strategies, with a simplified calculation phase, able to deal with richer labels and no longer directly dependent on the observations but only on the model prediction2. We also propose a natural extension for a number of classes higher than two. The first method uses discord and non-specificity to map uncertainty in order to address the exploration-exploitation problem. The second method extends the epistemic and aleatoric uncertainties to rich labels, also simplifying the computation phase.\n\nFootnote 2: The uncertainty is no longer directly dependent on the observations, but the model still is.\n\nFrom there, a label can be uncertain and imprecise, which means that additional information on ignorance is represented. Figure 5 shows how these labels are represented in this document, the darker the dot, the less ignorance the label contains (_e.g. I'm sure this is a dog_), the lighter the dot, the more ignorance it contains (_e.g. I have no idea between dog and cat_).\n\n### Discord and non-specificity: Klir uncertainty\n\nIn the framework of belief functions, discord and non-specificity are tools that allow to model uncertainty, we propose to use Klir's representation [12] for uncertainty sampling, some bridges can be made with epistemic and aleatoric uncertainty.\n\nFigure 4: Areas of uncertainty on (a)a for epistemic and aleatoric uncertainties.\n#### 3.1.2 Discord\n\nis here applied to the output of a model capable of making an uncertain and imprecise prediction3. It represents the amount of conflicting information in the model's prediction and is calculated with the following formula:\n\nFootnote 3: The Evidential \\(K\\)-nearest Neighbors model [5] is considered to illustrate the examples, which may vary depending on the model used.\n\n\\[D(m)=-\\sum_{A\\subseteq\\Omega}\\,m(A)\\,\\log_{2}(BetP(A)), \\tag{9}\\]\n\nwith \\(m\\) a mass function, or the output of the model (see section 2).\n\nFigure 6 represents three different cases where the discord varies, from high discordance where labels around the central point (the observation to label) highly disagree 6a, to low discordance where each of the labels is in agreement 6c.\n\n#### 3.1.3 Non-Specificity\n\nallows to quantify the degree of ignorance of the model, the higher it is, the more imprecise the response of the model, it is calculated with:\n\n\\[N(m)\\equiv\\sum_{A\\subseteq\\Omega}m(A)\\,\\log_{2}(|A|). \\tag{10}\\]\n\nThe same Figure 6 also represents three different cases of non-specificity, in 6d the non-specificity is low as there are relevant sources of information next to the observation to be labelled, in 6e the non-specificity increases the further away the elements are from the observation and in 6f the non-specificity is also high because the nearby sources of information are themselves ignorant.\n\n#### 3.1.4 Klir uncertainty\n\nis then derived from discord and non-specificity, it is used here for uncertainty sampling by adding the two previous formulas:\n\n\\[\\mathcal{U}_{m}(x)=N(x)+D(x), \\tag{11}\\]\n\nwith \\(N(x)\\) and \\(D(x)\\) respectively the non-specificity and discord of the model in \\(x\\). Klir [12] proposes to use the same weight for discord and non-specificity, but in [4] a parameter \\(\\lambda\\in[0,1]\\) is introduced and allows to bring more weight to non-specificity (we propose to use it for more exploration) or to discord (for more exploitation):\n\n\\[\\mathcal{U}_{m}(x)=\\lambda N(x)+(1-\\lambda)D(x). \\tag{12}\\]\n\nFigure 5: Observations on two dimensions with their rich labels, the darker the point, the more certain and precise its label.\nNote that this uncertainty is naturally extended to \\(|\\Omega|\\geq 2\\) classes.\n\nThis formula has the advantage of identifying the total uncertainty as well as the reducible one, but also of taking into account the uncertainty already present in the labels and of being adjustable for more exploration or exploitation. Figure 7 shows a dataset with two areas of uncertainty (a)a, on the right an area with a lack of data and on the left an area where labels are more ignorant. The uncertainty sampling, using Shannon's entropy (4) or the least confidence measure (5) is not able to see either of these two areas (b)b. The epistemic uncertainty (8) is able to distinguish the uncertainty related to the arrangement of the observations in space (_i.e._ the uncertainty on the right) but not the uncertainty related to the ignorance of the sources (c)c.\n\nThe proposal of using Klir uncertainty for sampling (discord and non-specificity) allows to represent each of these uncertainties. Figure 8 shows the areas of non-specificity (a)a, of discord (b)b and Klir uncertainty (c)c.\n\nKlir uncertainty can then be used for uncertainty sampling in active learning, it is also possible to vary the result for more exploration or more exploitation by modifying \\(\\lambda\\). Figure 9 shows the areas of uncertainty for different values of \\(\\lambda\\), more discord on the left to more non-specificity on the right.\n\nFigure 6: Three degrees of discord and three degrees of non-specificity in the center.\n\nFigure 7: An imperfectly labeled dataset (a)a with the areas of uncertainty according to uncertainty sampling and epistemic uncertainty.\nWe have proposed here to use Klir's uncertainty in sampling, which allows to represent some unknown uncertainties areas in active learning related to rich labels. The method is no longer dependent on the observations, but only on the prediction of the model and the exploration-exploitation problem is addressed thanks to the \\(\\lambda\\) parameter. Even though discord may recall aleatoric uncertainty (non-reducible) and non-specificity may recall epistemic uncertainty (reducible). These notions are not quite equivalent. Therefore, in the following section we also propose an extension of epistemic (and aleatoric) uncertainty for rich labels and for several classes.\n\n### Evidential epistemic uncertainty\n\nWe propose here to extend the notion of epistemic uncertainty to rich labels, by removing the dependence on observations, simplifying the computational phase, and allowing the model to detect new areas of uncertainty.\n\nThe epistemic uncertainty can be extended to rich labels by using the notion of plausibility within the framework of belief functions. It represents the total evidence that does not support the complementary event for a class \\(\\omega\\) or more generally for an element \\(A\\in 2^{\\Omega}\\). The plausibility \\(Pl\\) defines the belief that could be allocated to \\(A\\):\n\n\\[Pl(A)=\\sum_{A\\cap B\\neq\\emptyset}m(B). \\tag{13}\\]\n\nFigure 8: Areas of uncertainty corresponding to the dataset (a)a according to the non-specificity, the discord and the total uncertainty defined by Klir.\n\nFigure 9: Areas of Klir uncertainty, modifying the amount of non-specificity and discord. With \\(\\lambda=0.1\\), more discord is taken into account, with \\(\\lambda=0.5\\), discord and non-specificity are used as much and with \\(\\lambda=0.9\\), more non-specificity is taken into account.\nThe plausibility being the consistent evidence, the belief function \\(Bel\\) defines the total evidence directly supporting \\(A\\):\n\n\\[Bel(A)=\\sum_{B\\subseteq A,B\\neq\\emptyset}m(B). \\tag{14}\\]\n\nWe have \\(Pl(A)=1-Bel(\\bar{A})\\). Analogous to equation (8) and for two classes \\(\\Omega=\\{0,1\\}\\) the epistemic uncertainty is maximal when both classes are highly plausible. The proposed evidential epistemic and aleatoric uncertainties are defined as follows:\n\n\\[\\begin{split}\\mathcal{U}_{e}(x)&=\\min[Pl(1|x),Pl(0 |x)],\\\\ \\mathcal{U}_{a}(x)&=1-\\max[Pl(1|x),Pl(0|x)].\\end{split} \\tag{15}\\]\n\nThe equation for the aleatoric uncertainty can be rewritten depending on the belief \\(Bel\\):\n\n\\[\\mathcal{U}_{a}(x)=\\min[Bel(1|x),Bel(0|x)]. \\tag{16}\\]\n\nThe sum of the epistemic and aleatoric uncertainties is then the total evidential uncertainty: \\(\\mathcal{U}(x)=\\mathcal{U}_{e}(x)+\\mathcal{U}_{a}(x)\\). However, when the number of classes exceeds 2 the equation of the epistemic uncertainty cannot be simplified by the minimum plausibility:\n\n\\[\\begin{split}\\mathcal{U}_{e}(x)&\\neq\\min([Pl( \\omega|x)|\\omega\\in\\Omega]),\\\\ \\mathcal{U}_{a}(x)&\\neq 1-\\max([Pl(\\omega|x)|\\omega\\in \\Omega]).\\end{split} \\tag{17}\\]\n\nIt is preferable to first define the uncertainty related to one of the classes \\(\\omega\\), rewritten with the belief \\(Bel\\) to avoid having to manipulate \\(\\bar{\\omega}\\):\n\n\\[\\begin{split}\\mathcal{U}_{e}(\\omega|x)&=\\min[Pl( \\omega|x),Pl(\\bar{\\omega}|x)]\\\\ &=\\min[Pl(\\omega|x),1-Bel(\\omega|x)].\\end{split} \\tag{18}\\]\n\nThe evidential extension of the epistemic and aleatoric uncertainties for \\(|\\Omega|\\geq 2\\) classes is then:\n\n\\[\\begin{split}\\mathcal{U}_{e}(x)&=\\sum_{\\omega\\in \\Omega}\\min[Pl(\\omega|x),1-Bel(\\omega|x)],\\\\ \\mathcal{U}_{a}(x)&=\\sum_{\\omega\\in\\Omega}\\min[Bel( \\omega|x),1-Pl(\\omega|x)].\\end{split} \\tag{19}\\]\n\nThe example in Figure 10 shows a dataset of three classes with a zone of ignorance for some labels (between the green and red classes). Probabilistic (4)-(5) and epistemic (8) uncertainties cannot model the imprecision present in the labels, this less complete uncertainty zone is represented in 10b.\n\nThe previous uncertainty resulting from the sum of the discord and the non-specificity is presented in Figure 11. It manages both exploration 11a and exploitation 11b to give a better representation of the uncertainty 11c.\nFigure 11: Areas of uncertainty corresponding to the datasets (a)a according to the non-specificity, the discord and the total Klir uncertainty.\n\nFigure 12: Areas of uncertainty corresponding to the datasets (a)a according to the evidential epistemic uncertainty for green, red and blue classes.\n\nFigure 10: On the left, a sample of a dataset of three classes with an area of ignorance (labeled with imprecision) and on the right areas of uncertainty according to non-evidential uncertainty sampling.\n\nFigure 13: Areas of uncertainty for evidential epistemic and aleatoric uncertainties, according to (a)a.\nThe extension of the epistemic uncertainty, also introduced in this paper, is presented in the following experiments. First, the evidential epistemic areas of uncertainties for each of the three classes are presented in Figure 12. Then, the resulting evidential epistemic uncertainty of the model is deducted from equation (19) in Figure 13 along with the evidential aleatoric and total uncertainties.\n\n## 5 Sampling on real world dataset\n\nSome datasets have been labeled in an uncertain and imprecise way by users during crowdsourcing campaigns [22]. We therefore have access to really imperfectly labeled datasets with rich labels. Conventional methods for computing model uncertainty do not take into account the degrees of imprecision of these rich labels. The two proposed methods are illustrated on Credal Dog-2, one of these datasets. Figure 14 shows the dataset on the two first components of a Principal Component Analysis. This is a two-class dataset represented in 14a with true classes and in 14b with uncertain and imprecise rich labels given by contributors. Darker dots indicate higher certainty, and vice versa.\n\nFigure 16 shows the result of the first proposed method, sampling by Klir uncertainty, on the dataset with rich labels. The non-specificity is presented 15a and can be interpreted as the ignorance zones of the model. Discord is also represented 15b and the total uncertainty 15c is the sum of the two, it is this latter information that is used to sample on the model uncertainty.\n\nThe second proposed method, the extension of epistemic uncertainty, which is a reducible uncertainty applied to evidential reasoning, is presented in Figure 16. The irreducible aleatoric evidential uncertainty 16a is presented along with the reducible epistemic evidential uncertainty 16b. The total uncertainty 16c is the sum of the reducible and irreducible uncertainties. For active learning, it is not the total uncertainty, but the epistemic reducible uncertainty that is used.\n\n## 6 Discussion & Conclusion\n\nThe calculation of epistemic uncertainty (non-evidential) is demanding, and not necessarily accessible. It is, depending on the observations, necessary to go through several phases of computation, estimation of likelihood, maximum likelihood and optimization.\n\nIn this paper, we have proposed two new uncertainty sampling strategies and a new way to represent them. With these two proposed methods, the use of Klir uncertainty and the extended evidential epistemic uncertainty, a simple calculation on the output of the model allows to obtain the uncertainties. The objective is to also take into account the uncertainty present in richer labels, which is currently not possible. The first strategy is based on Klir's uncertainty, combining discord (how self-conflicting the information is) and non-specificity (how ignorant the information is) in the model output. The second strategy extends epistemic (reducible) uncertainty to the evidential framework and to several classes, simplifying the computational phase.\nThis simplicity obviously has a counterpart: the model must be able to deliver a mass function, to represent uncertainty and imprecision in the output. Such models exist but are not numerous, among them are the much quoted Evidential \\(K\\)-Nearest Neighbors [5], Evidential Decision Trees [4, 6], Evidential Random Forest and even Evidential Neural Networks [23]. The proposed methods are compatible with probabilistic models (since a probability is a special mass function) but the full depth of evidence modeling would be lost.\n\nThe novelty of this work lies in the representation of new information for uncertainty sampling, rather than in performance comparison. The next step is to apply these models to active learning, where the learning model has access to a very limited number of labeled observations, and must choose the most relevant observations to label in order to increase performance. The ability of the model to define these areas of uncertainty, and to categorize these uncertainties, is then relevant information.\n\nFigure 16: Areas of evidential epistemic uncertainty corresponding to 14b.\n\nFigure 14: Credal Dog-2 dataset, Brittany breed is in green and Beagle in red.\n\nFigure 15: Areas of uncertainty corresponding to the dataset 14b according to the non-specificity, the discord and to the total Klir uncertainty.","pdf_link":"http:\/\/arxiv.org\/pdf\/2309.12494v2.pdf"}
{"id":2309.07927,"title":"Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech\n  Recognition for Children VS. Adults","abstract":"Recent advancements in Automatic Speech Recognition (ASR) systems,\nexemplified by Whisper, have demonstrated the potential of these systems to\napproach human-level performance given sufficient data. However, this progress\ndoesn't readily extend to ASR for children due to the limited availability of\nsuitable child-specific databases and the distinct characteristics of\nchildren's speech. A recent study investigated leveraging the My Science Tutor\n(MyST) children's speech corpus to enhance Whisper's performance in recognizing\nchildren's speech. They were able to demonstrate some improvement on a limited\ntestset. This paper builds on these findings by enhancing the utility of the\nMyST dataset through more efficient data preprocessing. We reduce the Word\nError Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and\nfrom 13.23% to 8.61% with Whisper-Medium and show that this improvement can be\ngeneralized to unseen datasets. We also highlight important challenges towards\nimproving children's ASR performance. The results showcase the viable and\nefficient integration of Whisper for effective children's speech recognition.","authors":"Ahmed Adel Attia, Jing Liu, Wei Ai, Dorottya Demszky, Carol Espy-Wilson","published_date":"2023-09-12T06:58:18Z","link":"http:\/\/arxiv.org\/abs\/2309.07927v3","markdown":"Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children vs. Adults\n\n###### Abstract\n\nRecent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn't readily extend to ASR for children due to the limited availability of suitable child-specific databases and the distinct characteristics of children's speech. A recent study investigated leveraging the My Science Tutor (MyST) children's speech corpus to enhance Whisper's performance in recognizing children's speech. They were able to demonstrate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children's ASR performance. The results showcase the viable and efficient integration of Whisper for effective children's speech recognition.\n\nAhmed Adel Attia\\({}^{1}\\), Jing Liu\\({}^{1}\\), Wei Ai\\({}^{1}\\), Dorottya Demszky\\({}^{2}\\), Carol Espy-Wilson\\({}^{1}\\)\\({}^{1}\\)University of Maryland College Park, MD, USA\n\n\\({}^{2}\\)Stanford University, CA, USA Whisper, children ASR, My Science Tutor, MyST, CSLU kids, automatic speech recognition\n\n## 1 Introduction\n\nAutomatic Speech Recognition (ASR) has witnessed a boom in recent years through utilizing huge amounts of transcribed speech scrapped from the internet. Whisper [1] was able to approach human-level accuracy by utilizing 680K hours of speech data. XLS-R [2] pre-trains on 436K hours of untranscribed speech in a self-supervised manner and 65K hours of transcribed speech. These models were able to achieve state-of-the-art (SOTA) results by leveraging huge amounts of data. ASR models still underperform with low-resource languages and tasks. Recent works have attempted to explore how ASR models performance can be improved for low-resource languages [3, 4, 5, 6] but they haven't caught up with high-resource languages.\n\nChildren ASR is considered a low resource task and previous works have demonstrated the gap between children and adult ASR even in English. The main reason for that has been attributed to inter-speaker variability due to varying developmental rates and intra-speaker variability due to underdeveloped pronunciation skills [7, 8, 9, 10, 11, 12]. Current ASR models trained on adult speech are not capable of learning these variabilities as they are mostly unseen in the training data. Moreover, children's speech databases are limited and difficult to collect and transcribe [13].\n\nIn this work, we explore how Whisper can be fine-tuned on children's speech. We chose Whisper because of its massive training data which makes it more likely to generalize to unseen and uncommon speech patterns. Additionally, Whisper has been shown to be noise-robust [14]. We take advantage of the My Science Tutor (MyST) speech corpus [15] which is the largest publicly available children's speech corpus, provided free to academics for research purposes.\n\nA recent study [16] has attempted to adapt Whisper to the MyST corpus. They found that the quality of audio files as well as transcriptions in the MyST corpus varies, and were able to extract 65 hours of well-transcribed speech from the 197 hours of transcribed speech provided in MyST. We expand upon their work by outlining a more efficient data preprocessing scheme and extracting a total of 179.2 hours, which we show improves the performance and robustness of Whisper. Additionally, we maintain the train\/test\/development splits provided in the MyST corpus to ensure there's no overlap in speakers between data splits. We demonstrate tangible improvement on the MyST testset, reducing the Word Error Rate (WER) of the Small Whisper model from 13.93% to 9.11% and that of the Medium model from 13.23% to 8.61%. This also leads to improving to WER on the spontaneous part of the CSLU Kids dataset from 32.00% to 27.16% with the Small model, and from 37.04% to 16.53% with the Medium model without explicitly including this dataset in the training set.\n\nWe begin by giving a quick overview of Whisper in Section 2, followed by a description of the datasets used and our proposed preprocessing scheme in Section 3. We follow that by showcasing our experiments and training parameters in Section 4. Results and further discussion are in Section 5. We end with a conclusion outlining plans for future research in Section 6, and acknowledgments in Section 7.\n## 2 Model Description\n\nWhisper is a family of ASR models with varying sizes, namely, Tiny, Base, Small, Medium, and Large. Models from Tiny to Medium have an English-only variant and a multilingual variant. The training data for Whisper includes 438K hours of English-to-English transcription, 117K hours covering 96 languages not including English, and 125K hours of speech spoken in different languages, transcribed in English. To filter out low-quality transcription, the training set was passed through an initial model, and files with a high WER were flagged and manually inspected to remove automatically transcribed and mistranscribed files. This substantial amount of training data helped Whisper achieve near human-level transcription, especially in English, with their Large model achieving a WER of 2.82 on the Librispeech clean test set.\n\n## 3 Dataset Description and Processing\n\nWe mainly focus on the MyST corpus in this study. However, we also discuss how well the results on MyST can be generalized beyond this corpus. For that purpose, we use the CSLU kids database [17]. Additionally, we study how finetuning affects the performance on adult speech by testing our models on the test-clean subset of Librispeech. In this section, we describe each corpus.\n\n### My Science Tutor Dataset\n\nThe MyST corpus is the largest publicly available children's speech corpus. It consists of 393 hours of conversational children's speech, recorded from virtual tutoring sessions in physics, geography, biology, and other topics. The corpus spans 1,371 third, fourth, and fifth-grade students although age and gender information for each student are not available. Around 197 hours of the dataset were transcribed, although the quality of transcriptions varies. To the best of our knowledge, the MyST corpus was not included in Whisper's training set. Upon manual inspection, some transcriptions were assigned to the wrong files completely.\n\nProvided Transcription: Um, the wires are like a pathway energy goes through it into the motor and makes it work.\n\nActual Transcription: Um, because it's metal, and metal I think has energy.\n\nOther files appear to have been automatically transcribed with a lower-quality transcriber.\n\nProvided Transcription: No, I don't hearing even a candle burns.\n\nActual Transcription: No, I don't hear anything when the candle burns.\n\nAdditionally, some files have poor audio quality, with the children speaking too close to the microphone, which resulted in a high level of distortion in the audio files.\n\nTo identify these files, we follow a similar technique as in [1], by passing the entire dataset through Whisper-Large and flagging files with WER larger than 50%. Additionally, one and two-word files were removed altogether, because they lacked the context to distinguish between homophones, like \"to\", \"too\" and \"two\". All files with no speech activity, i.e. files labeled as \\(<\\)DISCARD\\(>\\) or \\(<\\)NO_SIGNAL\\(>\\) or \\(<\\)SILENCE\\(>\\), were also removed from the dataset. Table 1 shows the effect of different filtering steps on total dataset duration and WER. According to our results, around 5 hours of the training data is either mistranscribed or has low audio quality and is responsible for increasing the WER on the training data by about 3%. Similar results can be inferred about the test and development sets. Additionally, short files which accounted for only 4 hours of the training data increased the WER by more than 7%. We will publish the list of flagged files on GitHub and link to it in the camera-ready manuscript.\n\nFiles longer than 30 seconds in the training and development sets were also removed. That is because Whisper processes files in 30-second chunks, and any files longer than 30 seconds are truncated. However, it is not possible to accurately truncate the transcriptions without any timestamps present, so these files are unsuitable for loss calculation. Additionally, the majority of the files in the MyST corpus were too short, with the average file length in the training data being 8 seconds. That would mean that training batches are mostly padding, leading to inefficient training. To remedy this, files within a single recording session were concatenated to be close to but not longer than 30 seconds while maintaining the context of the conversation within the recording session.\n\nOur filtering technique removes 17.8 hours from the entire dataset, which leaves us with 179.2 hours of well-transcribed speech in total. We maintain the train\/development\/test split provided in the MyST database to avoid any overlap in speakers between the splits. We ended up with 132.5 hours in the training data, 20.9 in the development data, and 25.8 in the test data.\n\nThe text of the transcriptions was all upper case which destabilized the training. Consequently, all the text was mapped to be lowercase and further normalized using WhisperNormalizer1 Python package, which mapped tokens like \"you're\" to a standard \"you are\", as well as mapping all digit numbers to be spelled out. This ensured that only actual mistranscriptions would be penalized. This also reduces the\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|} \\hline\n**Filtration Method** & **Train** & **Test** & **Development** \\\\ \\hline\n**No Filteration** & 29.5 (145) & 26.2 (28.1) & 26.2 (25.5) \\\\ \\hline\n**Removing Files w. WER \\(>\\) 50\\%** & 26.8 (140) & 22.3 (26.7) & 22.3 (25.5) \\\\ \\hline\n**Removing Files w. WER \\(>\\) 50\\%** & 19.2 (132.5) & 14.2 (25.6) & 12.8 (21) \\\\ or **w. Less Than 3 Words** & & & \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: WER of Whisper-Large-v1 transcriptions of all three data splits of the MyST corpus before and after different levels of filtration (Duration of splits in hours).\ndiversity in transcription quality, which was noted to harm the performance, unlike diversity in audio quality[1].\n\nWhen contrasted with [18], their filtration method of removing all files longer than 20 seconds and shorter than 10 seconds yielded only 65 hours in total, which they partitioned into a 55-hour training set and a 10-hour test set and no development set. Their sets also suffered from overlapping speakers between the train and test sets. By sticking to the splits provided by MyST, our splits share no overlapping speakers, and have almost 3x the amount of data. To ensure fair comparison, the speech from all speakers in [16]'s test-set was removed from our training set, leaving us with a 125.7-hour training set.\n\n### CSLU Kids\n\nThe CSLU Kids speech corpus contains spontaneous and prompted speech from 1100 children between Kindergarten and Grade 10, with approximately 100 children per grade. In the scripted subset of the dataset, each child was prompted to read from a list of 319 scripts, that can either be simple words, sentences, or digit strings. Each utterance of spontaneous speech begins with a recitation of the alphabet followed by one minute of unprompted speech. The spontaneous speech in the CLSU corpus is distinct from the MyST corpus in that it is unstructured. Instead of talking about a particular topic, children were only given an open prompt like \"Tell me about your favorite movie.\" [17]. Below is a sample such of transcription.\n\n...usually just lay down on my bed, for now i don't like to i don't know, uh football okay first they are like standing on the ground and then they run and then they mm and if the girl pass the whole field you get a six points uh think it's twenty four i don't know think yeah they catch block and uh one uh the quarter back throws and the runners run uh it's blue uh and it has a big big big electric train set uh i have a workshop...\n\nThe majority of the recordings in the spontaneous section of the CLSU corpus were longer than 30 seconds, and are thus unsuitable for training. Instead, we use the scripted portion of the CSLU corpus to help the model adapt to the channel differences between MyST and CSLU recordings, but still consider the spontaneous section as an out of sample testset.\n\nThe transcriptions were of a high enough quality and filtering was not necessary, but they were all normalized to ensure a standard style of transcription. Files in the scripted portion of the dataset were shuffled and split into train, development, and test sets with an 80\/10\/10 split. The training set was 35 hours long, and the development and test sets were both around 4.8 hours long. Short files were combined to be close to 30 seconds as we did with the MyST corpus.\n\n### Librespecch: test-clean\n\nThe test-clean subset of the Librespecch corpus was used to test the ASR model's performance on Adult speech. It contains about 5.4 hours of speech read from Audiobooks from the LibriVox project. Since Librespecch was not used for training, we didn't combine the files, and we also didn't filter out any transcriptions to allow for reproducible and contrastable results. All transcriptions were normalized.\n\n## 4 Training Details and Hyperparameters\n\nWe followed the Huggingface Whipser finetuning tutorial 2. Our training scripts are available on Github 3, and we will link to the checkpoints on Huggingface in the camera-ready manuscript. Our evaluation script, which calculates the WER, was adapted from a code snippet by OpenAI4. All models were trained on Nvidia A6000 50GB GPU.\n\nFootnote 2: [https:\/\/huggingface.co\/blog\/fine-tune-whisper](https:\/\/huggingface.co\/blog\/fine-tune-whisper)\n\nFootnote 3: [https:\/\/github.com\/ahmedadelphia\/whisperKids](https:\/\/github.com\/ahmedadelphia\/whisperKids)\n\nFootnote 4: [https:\/\/github.com\/openai\/whisper\/discussions\/654](https:\/\/github.com\/openai\/whisper\/discussions\/654)\n\nFor the Small models, we used a learning rate of \\(1\\times 10^{-5}\\), batch size of 64, and 1 gradient accumulation step. For the Medium models, we used a learning rate of \\(1\\times 10^{-5}\\), batch size of 32, and 1 gradient accumulation step. All models were finetuned until convergence and the best checkpoints were used for evaluation.\n\n## 5 Results and Discussion\n\n### Whisper Zero-shot Models\n\nTable 3 shows the WER for different Whisper models without any finetuning. Looking at these results, the gap between children and adult speech becomes immediately clear. The WER for the scripted part of CSLU Kids is between 6 and 10 times that of Librispeech, and the WER for MyST is between 3 and 5 times. In general, English models perform better than multilingual models, with the exception of the Medium model. That could be because the Medium model is big enough to benefit from seeing more data in different languages. The bigger the model, the better the performance, with the exception of Large-V1 being slightly worse than Medium. In fact, the performance seems to saturate beyond Medium and the difference in performance between Medium and Large-V2 is negligible.\n\nWe note that the zero-shot WER reported here is smaller than that reported in [16]. We attribute this to the fact that they used a different normalizer than the one Whisper was trained with, which we validated by inspecting their datasets which are publicly accessible on Huggingface Based on these results, we finetune the Small and Medium models, both the English and multilingual variants.\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|} \\hline\n**Dataset** & **Training** & **Development** & **Testing** & **Filterted2** & **Age Group** \\\\  & **Duration** & **Duration** & **Duration** & **Filtration** & **Filtered2** & **Age Group** \\\\ \\hline\n**MyST** & 125 & 20.9 & 25.8 & \u2713 & 8-11 Years \\\\\n**CSLU Kids - Scripted** & 35 & 4.8 & 4.8 & X & 6-11 Years \\\\\n**Librespecch- testclean** & 0 & 0 & 5.4 & X & Adult \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 2: Summary of the Datasets Used. Durations in hours.\n### Finetuned Whisper Models\n\nIn this section, we showcase the performance of our finetuned models and contrast them with the models from [16], whose models are publicly available on Huggingface. We report the best-performing variants here. We tested all models on testsets from four corpora, listed in Table 2.\n\nLooking at the results in Table 4, it is clear that Whisper can and does improve its performance on the MyST dataset as well as CSLU, proving that transformer ASR models have the capacity to improve their performance on children's speech. We establish strong SOTA performance of 8.61% for the MyST testset. To the best of our knowledge, our best performance on the CSLU scripted dataset of 1.97% beats the current SOTA of 5.52% [19]. We also show improvement on unseen datasets, since both our models trained on just MyST, or a combination of MyST and CSLU scripted data show improvement on CSLU spontaneous speech without any training on speech from this dataset. Our best performing model on the CSLU spontaneous dataset scores 16.53% WER, which is about half the WER of zeroshot Whisper. Additionally, our models \"forget\" less about the adult speech than the baseline, with our models seeing a degradation of only about 1%.\n\nMedium models outperformed Small models, and generalized better to unseen datasets. The English-only variant of the Small model showed significant improvement over the multilingual variant in seen and unseen datasets. The Medium multilingual variant performed slightly better on the MyST dataset when finetuned exclusively on it, but the English-only variant generalized better to unseen data. Multilingual models in both sizes had higher WER for Librispeech.\n\nLooking at the results for the scripted portion of the CSLU corpus, it is clear that the lack of context in these script harm the performance of the models that weren't trained on speech from this dataset. However, the performance improved significantly when speech from this dataset was included in the training data, mainly because of the lack of variability on the scripts, unlike the more diverse MyST or CSLU spontaneous datasets. We also attribute the gap in performance between the MyST and CSLU spontaneous datasets to the fact that speech in the MyST corpus is more structured than the CSLU spontaneous dataset. This shows that one of the reasons behind the gap in performance between adult and children's ASR is that the decoder in Whisper, which acts as an audio-condtional language model, is not well adapted to the variablility found in children's speech, where they can suddenly change topic several times in a short period.\n\n## 6 Conclusions and Future Work\n\nIn this paper, we outlined how Whisper, a SOTA ASR system can be finetuned on children's speech using MyST, the largest publically available conversational children's speech corpus. We showcased a way to filter out mistranscribed files from the corpus and established a strong baseline for children's speech recognition. Our finetuning reduced the WER by 4 to 5% and reduced the gap between adult and children's speech. We also outlined some of the challenges that faces children ASR, namely the fact that audio-conditional language models are not well adapted to the variability in children's speech.\n\nIn the future, we will explore the noise robustness of Whisper. Specifically we will look at babble noise and other typical classroom nonspeech sounds and how they can affect performance, and how to improve such robustness in children's ASR. We will also explore whether these models are biased towards a certain gender, racial group or age group.\n\nThe authors of [20] developed grade-specific ASR models, and proposed grouping different age groups separately, instead of under the umbrella term \"children speech\". Their suggested grouping was kindergarten; 1st grade; 2nd and 3rd grade; and 4th grade and above, and they noted that it is possible to achieve adult-like performance with the latter group. We aim to expand upon their work in the future, exploring whether their results can be replicated with large transformer ASR models and whether such bias against youger children can be mitigated.\n\n## 7 Acknowledgments\n\nThe authors of this paper thank Wayne Ward for sharing his experience with Whisper, MyST and other children databases.\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Training Data**} & \\multirow{2}{*}{**MyST**} & **CSLU Kids** & **CSLU Kids** & **Librespeech** \\\\  & & & **Scripted** & **Spontaneous** & **testclean** \\\\ \\hline \\multicolumn{5}{|c|}{**Small**} \\\\ \\hline\n**ML - Zeroshot** & - & 14.06 & 25.15 & 36.36 & 3.39 \\\\\n**EN - Zeroshot** & - & 13.93 & 21.31 & 32.00 & **3.05** \\\\ \\hline\n**EN - [16]** & MyST55H & 13.23 & 31.26 & 28.63 & 5.40 \\\\\n**ML** & MyST & 11.80 & 55.51 & 28.53 & 6.23 \\\\\n**ML** & MyST + CSLU & 12.11 & 2.74 & 32.72 & 7.97 \\\\\n**EN** & MyST & **9.11** & 33.85 & 28.47 & **4.18** \\\\\n**EN** & MyST + CSLU & 9.21 & **2.59** & **27.16** & 4.74 \\\\ \\hline \\multicolumn{5}{|c|}{**Medium**} \\\\ \\hline\n**ML - Zeroshot** & - & 13.23 & 18.57 & 31.85 & 3.02 \\\\\n**EN - Zeroshot** & - & 12.90 & 18.62 & 37.04 & **2.76** \\\\ \\hline\n**EN - [16]** & MyST55H & 14.40 & 28.31 & 26.76 & 8.66 \\\\\n**ML** & MyST & **8.61** & 30.10 & 24.26 & 5.32 \\\\\n**ML** & MyST + CSLU & 8.99 & **1.97** & 20.28 & 4.28 \\\\\n**EN** & MyST & 8.91 & 47.94 & 25.56 & 3.95 \\\\\n**EN** & MyST + CSLU & 8.85 & 2.38 & **16.53** & **3.52** \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 4: WER on different test sets for different Whisper Models. EN stands for English-only model and ML stands for multilingual model.\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|} \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**MyST**} & **CSLU Kids** & **CSLU Kids** & **Librespeech** \\\\  & & **Scripted** & **Spontaneous** & **testclean** \\\\ \\hline\n**Tiny** & 21.16 & 74.98 & 57.01 & 7.49 \\\\\n**Tiny.en** & 18.34 & 61.04 & 45.29 & 5.59 \\\\\n**Base** & 18.54 & 40.20 & 43.71 & 4.98 \\\\\n**Base.en** & 15.57 & 33.18 & 38.57 & 4.15 \\\\\n**Small** & 14.06 & 25.15 & 36.36 & 3.39 \\\\\n**Small.en** & 13.93 & 21.31 & 32.00 & 3.05 \\\\\n**Medium** & 12.90 & 18.62 & 37.04 & 2.76 \\\\\n**Medium.en** & 13.23 & 18.57 & 31.85 & 3.02 \\\\\n**Large-V1** & 14.15 & 21.50 & 45.18 & 2.98 \\\\\n**Large-V2** & 12.80 & 17.22 & 29.39 & 2.82 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 3: Zero-shot WER on different test sets for different Whisper Models Without Finetuning.","pdf_link":"http:\/\/arxiv.org\/pdf\/2309.07927v3.pdf"}
{"id":2309.0009,"title":"Benford's Law under Zeckendorf expansion","abstract":"In the literature, Benford's Law is considered for base-b expansions where\nb>1 is an integer. In this paper, we investigate the distribution of leading\n\"digits\" of a sequence of positive integers under other expansions such as\nZeckendorf expansion, and declare what Benford's Law should be under\ngeneralized Zeckendorf expansion.","authors":"Sungkon Chang, Steven J. Miller","published_date":"2023-08-31T19:16:07Z","link":"http:\/\/arxiv.org\/abs\/2309.00090v1","markdown":"# Benford's Law under Zeckendorf expansion\n\n###### Abstract\n\nIn the literature, Benford's Law is considered for base-\\(b\\) expansions where \\(b>1\\) is an integer. In this paper, we investigate the distribution of leading \"digits\" of a sequence of positive integers under other expansions such as Zeckendorf expansion, and declare what Benford's Law should be under generalized Zeckendorf expansion.\n\n## 1 Introduction\n\nIntroduced in [2, 18] is a probability distribution of the leading decimal digits of a sequence of positive integers, known as _Benford's Law_, and the exponential sequences such as \\(\\{3^{n}\\}\\) are standard examples of sequences that satisfy Benford's Law. Given \\(d\\in\\{1,2,3,\\ldots,9\\}\\), the probability of having the leading digit \\(d\\) in the decimal expansion of \\(3^{n}\\) is \\(\\log_{10}\\frac{d+1}{d}\\), and this distribution is Benford's Law. In fact, given a block \\(B\\) of digits of any length, the probability of having the leading block \\(B\\) in the decimal expansion of \\(3^{n}\\) is given by a similar logarithmic formula as well, and this is known as _strong Benford's Law;_ see Example 1.9. It is indeed a special property that a sequence has convergent proportions for each leading digit. For example, the proportion of odd integers \\(2n-1\\leq M\\) with leading digit \\(d\\) oscillates, and does not converge as \\(M\\to\\infty\\); see Section 4.10.\n\nIn the literature, Benford's Law is considered for base-\\(b\\) expansions where \\(b>1\\) is an integer. For example, the probabilities of the binary expansions of integer powers of \\(3\\) having the leading binary digits \\(100_{2}\\) and \\(101_{2}\\) are \\(\\log_{2}\\frac{2^{2}+1}{2^{2}}\\) and \\(\\log_{2}\\frac{2^{2}+2}{2^{2}+1}\\), respectively; for later reference, we may rewrite the values as follows:\n\n\\[\\log_{2}\\frac{1+2^{-2}}{1}\\approx 0.322,\\quad\\log_{2}\\frac{1+2^{-1}}{1+2^{-2}} \\approx 0.264. \\tag{1}\\]\n\nIn this paper, we shall consider the distribution of leading \"digits\" of a sequence of positive integers under other expansions such as Zeckendorf expansion [19]. For example, let \\(\\{F_{n}\\}_{n=1}^{\\infty}\\) for \\(n\\geq 1\\) be the shifted Fibonacci sequence, i.e., \\(F_{n+2}=F_{n+1}+F_{n}\\) for all \\(n\\in\\mathbb{N}\\) and \\(F_{1}=1\\) and \\(F_{2}=2\\), and consider two Zeckendorf expansions: \\(3^{5}=F_{12}+F_{5}+F_{2}\\) and \\(3^{8}=F_{18}+F_{16}+F_{14}+F_{11}+F_{7}+F_{5}\\). Similar to the way the binary expansions are denoted, we may write\n\n\\[3^{5}=100000010010_{F},\\quad 3^{8}=101010010001010000_{F}\\]\n\nwhere \\(1\\)'s are inserted at the \\(k\\)th place from the right if \\(F_{k}\\) is used in the expansions.\n\n**Definition 1.1**.: Let \\(A=\\{0,1\\}\\). Given \\(\\{s,n\\}\\subset\\mathbb{N}\\), let \\(n=\\sum_{k=1}^{M}a_{k}F_{M-k+1}\\) be the Zeckendorf expansion of \\(n\\) (where \\(a_{1}=1\\)). We define \\(\\mathrm{LB}_{s}(n):=(a_{1},\\ldots,a_{s})\\in A^{s}\\) if \\(M\\geq s\\); otherwise, \\(\\mathrm{LB}_{s}(n)\\) is undefined. The tuple \\(\\mathrm{LB}_{s}(n)\\) is called _the leading block of \\(n\\) with length \\(s\\) under Zeckendorf expansion_.\nFor example, \\(\\mathrm{LB}_{3}(3^{5})=(1,0,0)\\), \\(\\mathrm{LB}_{3}(3^{8})=(1,0,1)\\), and \\(\\mathrm{LB}_{6}(3^{8})=(1,0,1,0,1,0)\\). Since \\(\\mathrm{LB}_{2}(n)=(1,0)\\) for all integers \\(n\\geq 2\\), it is only meaningful to consider the first three or more Zeckendorf digits. We prove Theorem 1.3 in this note.\n\n**Definition 1.2**.: Given a conditional statement \\(P(n)\\) where \\(n\\in\\mathbb{N}\\), and a subset \\(A\\) of \\(\\mathbb{N}\\), let us define\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in A:P(n)\\text{ is true}\\,\\right\\}:=\\lim_{n\\to \\infty}\\frac{\\#\\{k\\in A:P(k)\\text{ is true},\\ k\\leq n\\}}{\\#\\{k\\in A:k\\leq n\\}}.\\]\n\nFor example, if \\(A=\\{n\\in\\mathbb{N}:n\\equiv 2\\mod 3\\}\\), then \\(\\mathrm{Prob}\\left\\{\\,n\\in A:n\\equiv 1\\mod 5\\,\\right\\}=\\frac{1}{5}\\). If \\(A\\) is finite, the limit always exists.\n\nLet \\(\\phi\\) be the Golden ratio. The following is an analogue of Benford's Law under binary expansion demonstrated in (1).\n\n**Theorem 1.3**.: _Let \\(a>1\\) be an integer._\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{3}(a^{n})=(1,0, 0)\\,\\right\\} =\\,\\log_{\\phi}(1+\\phi^{-2})\\approx.672,\\] \\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{3}(a^{n})=(1,0, 1)\\,\\right\\} =\\,\\log_{\\phi}\\frac{\\phi}{1+\\phi^{-2}}\\approx.328.\\]\n\nIn particular, they exist! Although the probabilities are different from the binary cases, the structure of the log expressions in Theorem 1.3 is quite similar to that of the binary expansions in (1), i.e., the denominators of the quotients express the leading digits in power expansions with respect to their bases. The exponential sequences \\((a^{n})_{n=1}^{\\infty}\\) where \\(a>1\\) is an integer are standard sequences that satisfy Benford's Law under base-\\(b\\) expansion. Motivated from these standard examples, we define Benford's Law under Zeckendorf expansion to be the above distribution of the leading blocks \\((1,0,0)\\) and \\((1,0,1)\\) under Zeckendorf expansion; see Definition 3.6.\n\nThe exponential sequences \\(\\{a^{n}\\}_{n=1}^{\\infty}\\) are standard sequences for so-called _strong Benford's Law under base-\\(b\\) expansion_ as well; see Example 1.9. We introduce below the probability of the leading Zeckendorf digits of \\(a^{n}\\) with arbitrary length, which is a generalization of Theorem 1.3; this result is rewritten in Theorem 3.8 with more compact notation.\n\n**Definition 1.4**.: Let \\(A=\\{0,1\\}\\), and let \\(s\\geq 2\\) be an integer. Let \\(\\mathbf{b}=(b_{1},b_{2},\\ldots,b_{s})\\in A^{s}\\) such that \\(b_{1}=1\\) and \\(b_{k}b_{k+1}=0\\) for all \\(1\\leq k\\leq s-1\\). We define \\(\\widetilde{\\mathbf{b}}\\) to be a tuple \\((\\widetilde{b}_{1},\\ldots,\\widetilde{b}_{s})\\in A^{s}\\) as follows. If \\(1+\\sum_{k=1}^{s}b_{k}F_{s-k+1}<F_{s+1}\\), then \\(\\widetilde{b}_{k}\\) for \\(1\\leq k\\leq s\\) are defined to be integers in \\(A\\) such that \\(1+\\sum_{k=1}^{s}b_{k}F_{s-k+1}=\\sum_{k=1}^{s}\\widetilde{b}_{k}F_{s-k+1}\\) and \\(\\widetilde{b}_{k}\\widetilde{b}_{k+1}=0\\) for all \\(1\\leq k\\leq s-1\\). If \\(1+\\sum_{k=1}^{s}b_{k}F_{s-k+1}=F_{s+1}\\), then \\(\\widetilde{b}_{1}:=\\widetilde{b}_{2}:=1\\), and \\(\\widetilde{b}_{k}:=0\\) for all \\(3\\leq k\\leq s\\).\n\nFor the case of \\(1+\\sum_{k=1}^{s}b_{k}F_{s-k+1}<F_{s+1}\\), the existence of the tuple \\(\\widetilde{\\mathbf{b}}\\) is guaranteed by Zeckendorf's Theorem.\n\n**Theorem 1.5**.: _Let \\(a>1\\) and \\(s\\geq 2\\) be integers. Let \\(\\mathbf{b}\\) and \\(\\widetilde{\\mathbf{b}}\\) be tuples defined in Definition 1.4. Then,_\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{s}(a^{n})=\\mathbf{b}\\,\\right\\} =\\,\\log_{\\phi}\\frac{\\sum_{k=1}^{s}\\widetilde{b}_{k}\\phi^{-(k-1)}}{\\sum_{k=1}^{ s}b_{k}\\phi^{-(k-1)}}.\\]\nFor example,\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{6}(a^{n})=(1,0,0,0,1,0)\\right\\} =\\,\\log_{\\phi}\\frac{1+\\phi^{-3}}{1+\\phi^{-4}}\\approx 0.157\\] \\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{6}(a^{n})=(1,0,1,0,1,0)\\right\\} =\\,\\log_{\\phi}\\frac{1+\\phi^{-1}}{1+\\phi^{-2}+\\phi^{-4}}\\] \\[=\\,\\log_{\\phi}\\frac{\\phi}{1+\\phi^{-2}+\\phi^{-4}}\\approx 0.119.\\]\n\nAs in Benford's Law under Zeckendorf expansion, we define the probability distributions described in Theorem 3.8 to be _strong Benford's Law under Zeckendorf expansion_; see Definition 3.9.\n\nExponential sequences are standard examples for Benford's Laws, but some exponential sequences do not satisfy Benford's Law under some base-\\(b\\) expansion. Let us demonstrate examples under Zeckendorf expansion. Let \\(\\{G_{n}\\}_{n=1}^{\\infty}\\) be the sequence given by \\(G_{k}=F_{2k}+F_{k}\\) for \\(k\\in\\mathbb{N}\\). Then, given an integer \\(s>1\\), the \\(s\\) leading Zeckendorf digits of \\(G_{k}\\) is \\(100\\cdots 00_{F}\\) as \\(k\\to\\infty\\) since the gap \\(2k-k=k\\) between the indices of \\(F_{2k}\\) and \\(F_{n}\\) approaches \\(\\infty\\). Thus, \\(\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{s}(G_{n})=(1,0,0,\\ldots,0) \\right\\}=1\\) for all \\(s\\in\\mathbb{N}\\), and the probabilities of other digits of length \\(s\\) are all (asymptotically) \\(0\\). Similar probability distributions occur for the Lucas sequence \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) given by \\(K_{k+2}=K_{k+1}+K_{k}\\) for \\(k\\in\\mathbb{N}\\) and \\((K_{1},K_{2})=(2,1)\\). Given \\(s\\in\\mathbb{N}\\), the probabilities of having leading Zeckendorf digits of length \\(s\\) are entirely concentrated on one particular string of digits. For example, \\(\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{10}(K_{n})=(1,0,0,0,1,0,0,0,1,0)\\right\\}=1\\), and the probabilities of having other digits of length \\(10\\) is all (asymptotically) \\(0\\); see Example 5.10 for full answers.\n\nGeneralized Zeckendorf expansions are introduced in [10, 17]. In Section 6, we prove Theorem 6.9 on the probability of the leading digits of \\(a^{n}\\) with arbitrary length under generalized Zeckendorf expansion, and define these probability distributions to be strong Benford's Law under generalized Zeckendorf expansion; see Definition 6.10. As in the concept of _absolute normal numbers_[12], we introduce in Definition 6.15 the notion of _absolute Benford's Law_, which is the property of satisfying strong Benford's Law under all generalized Zeckendorf expansions. For example, the sequence given by \\(K_{n}=\\left\\lfloor\\frac{\\phi}{\\sqrt{5}}(\\frac{89}{55})^{n}\\right\\rfloor\\) for \\(n\\in\\mathbb{N}\\) satisfies strong Benford's Law under all generalized Zeckendorf expansions; see Example 6.18. Its first fifteen values are listed below:\n\n\\[(1,1,3,4,8,12,21,34,55,89,144,233,377,610,988).\\]\n\nThey are nearly equal to the Fibonacci terms as \\(\\frac{89}{55}\\) is the \\(10\\)th convergent of the continued fraction of \\(\\phi\\). The differences amplify as we look at higher terms, and even under Zeckendorf expansion, this sequence satisfies strong Benford's Law.\n\nIt is also natural to consider sequences that have different distributions, and in this note we investigate other distributions of leading digits under generalized Zeckendorf expansions as well. In the following paragraphs, we shall explain this approach using base-\\(10\\) expansion. The results for other expansions are introduced in Section 5 and 6.\nStrong Benford's Law for the sequence \\(\\{3^{n}\\}_{n=1}^{\\infty}\\) under decimal expansion follows from the equidistribution of the fractional part of \\(\\log_{10}(3^{n})\\) on the interval \\((0,1)\\). We realized that the function \\(\\log_{10}(x)\\) is merely a tool for calculating the leading digits, and that other distributions of leading digits naturally emerge as we modified the function \\(\\log_{10}(x)\\).\n\nWe noticed that the frequency of leading digits converges when a continuation of the sequence \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\) has convergent behavior over the intervals \\([n,n+1]\\), and we phrase it more precisely below.\n\n**Definition 1.6**.: Let \\(\\{H_{n}\\}_{n=1}^{\\infty}\\) be an increasing sequence of positive integers. A continuous function \\(h:[1,\\infty)\\to\\mathbb{R}\\) is called a _uniform continuation of \\(\\{H_{n}\\}_{n=1}^{\\infty}\\)_ if \\(h(n)=H_{n}\\) for all \\(n\\in\\mathbb{N}\\), and the following sequence of functions \\(h_{n}:[0,1]\\to[0,1]\\) uniformly converges to an increasing (continuous) function:\n\n\\[h_{n}(p)=\\frac{h(n+p)-h(n)}{h(n+1)-h(n)}.\\]\n\nIf \\(h\\) is a uniform continuation of \\(\\{H_{n}\\}_{n=1}^{\\infty}\\), let \\(h_{\\infty}:[0,1]\\to[0,1]\\) denote the increasing continuous function given by \\(h_{\\infty}(p)=\\lim_{n\\to\\infty}h_{n}(p)\\).\n\nTheorem 1.8 below is a version specialized for decimal expansion. The proof of this theorem is similar to, and much simpler than the proof of Theorem 5.6 for Zeckendorf expansion, and we leave it to the reader.\n\n**Definition 1.7**.: If \\(\\alpha\\in\\mathbb{R}\\), we denote the fractional part of \\(\\alpha\\) by \\(\\operatorname{frc}(\\alpha)\\). Given a sequence \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) of real numbers, we say, \\(\\operatorname{frc}(K_{n})\\)_is equidistributed_ if \\(\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{frc}(K_{n})\\leq \\beta\\,\\big{\\}}=\\beta\\) for all \\(\\beta\\in[0,1]\\).\n\nFor example, consider the sequence \\(\\{\\operatorname{frc}(n\\pi)\\}_{n=1}^{\\infty}\\) where \\(\\pi\\approx 3.14\\) is the irrational number. Then, by Weyl's Equidistribution Theorem, \\(\\operatorname{frc}(n\\pi)\\) is equidistributed on the interval \\([0,1]\\). The sequence \\((\\sin^{2}(n))_{n=1}^{\\infty}\\) is an example of sequences that have \\(\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\sin^{2}(n)\\leq\\beta\\,\\big{\\}}\\) defined for each \\(\\beta\\in[0,1]\\), and the probability is \\(\\frac{1}{\\pi}\\cos^{-1}(1-2\\beta)\\). Thus, it is not equidistributed on \\([0,1]\\).\n\n**Theorem 1.8**.: _Let \\(h:[1,\\infty)\\to\\mathbb{R}\\) be a uniform continuation of the sequence \\(\\{10^{k-1}\\}_{n=1}^{\\infty}\\). Then, there is a sequence \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) of positive integers approaching \\(\\infty\\) (see Theorem 6.19 for the description of \\(K_{n}\\)) such that \\(\\operatorname{frc}\\big{(}h^{-1}(K_{n})\\big{)}\\) is equidistributed._\n\n_Let \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) be a sequence of positive integers approaching \\(\\infty\\) such that \\(\\operatorname{frc}\\big{(}h^{-1}(K_{n})\\big{)}\\) is equidistributed. Let \\(d\\) be a positive integer of \\(s\\) decimal digits. Then, the probability of the \\(s\\) leading decimal digits of \\(K_{n}\\) being \\(d\\) is equal to_\n\n\\[{h_{\\infty}}^{-1}\\left(\\frac{(d+1)-10^{s-1}}{9\\cdot 10^{s-1}}\\right)-{h_{ \\infty}}^{-1}\\left(\\frac{d-10^{s-1}}{9\\cdot 10^{s-1}}\\right).\\]\n\n**Example 1.9**.: Let \\(h:[1,\\infty)\\to\\mathbb{R}\\) be the function given by \\(h(x)=10^{x-1}\\). Then, \\(h\\) is a uniform continuation of the sequence \\(\\{10^{n-1}\\}\\), and \\(h_{\\infty}(p)=\\frac{1}{9}(10^{p}-1)\\). By Theorem 6.19, the\nsequence \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) with the equidistribution property is given by \\(K_{n}=\\lfloor 10^{n+\\operatorname{frc}(n\\pi)}\\rfloor\\), but there are simpler sequences such as \\(\\{3^{n}\\}_{n=1}^{\\infty}\\) that have the property.\n\nBy Theorem 1.8, the probability of the \\(s\\) leading decimal digits of \\(K_{n}\\) being \\(d\\) is equal to\n\n\\[\\log_{10}\\frac{d+1}{10^{s-1}}-\\log_{10}\\frac{d}{10^{s-1}}\\,=\\,\\log_{10}\\left(1 +\\frac{1}{d}\\right)\\]\n\nwhere \\(d\\in\\mathbb{N}\\) has \\(s\\) decimal digits. This distribution is known as strong Benford's Law under base-10 expansion, and we may say that strong Benford's Law under base-10 expansion arises from the logarithmic continuation of \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\). For this reason, we call \\(h(x)\\,a\\)_Benford continuation of the base-10 sequence_.\n\n**Example 1.10**.: Let \\(h:[1,\\infty)\\to\\mathbb{R}\\) be the function whose graph is the union of the line segments from \\((n,10^{n-1})\\) to \\((n+1,10^{n})\\) for all \\(n\\in\\mathbb{N}\\). Let \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) be the sequence given by \\(K_{n}=\\left\\lfloor 10^{n+\\log_{10}(9\\operatorname{frc}(n\\pi)+1)}\\right\\rfloor\\) as described in Theorem 6.19. Then, the fractional part \\(\\operatorname{frc}\\left(h^{-1}(K_{n})\\right)\\) is equidistributed. The limit function \\(h_{\\infty}\\) defined in Theorem 1.8 is given by \\(h_{\\infty}(p)=p\\) for \\(p\\in[0,1]\\), and given a decimal expansion \\(d\\) of length \\(s\\), the probability of the \\(s\\) leading decimal digits of \\(K_{n}\\) being \\(d\\) is (uniformly) equal to \\(1\/(9\\cdot 10^{s-1})\\) by Theorem 1.8.\n\nThe first ten values of \\(K_{n}\\) are\n\n\\[(22,354,4823,60973,737166,8646003,99203371,219467105,\\,3469004940,47433388230).\\]\n\nFor example, if we look at many more terms of \\(K\\), then the first two digits \\(22\\) of \\(K_{1}\\) will occur as leading digits with probability \\(1\/90\\approx 0.011\\), and the probability for the digits \\(99\\) is also \\(1\/90\\). As in constructing a normal number, it's tricky to construct a sequence of positive integers with this property, and prove that it has the property. Let us note here that the \\(s\\) leading decimal digits of the sequence \\(\\{n\\}_{n=1}^{\\infty}\\) has frequency close to \\(1\/(9\\cdot 10^{s-1})\\), but it oscillates and does not converge as more terms are considered; see Theorem 4.10 for a version under Zeckendorf expansion. In Example 5.4, we demonstrate the \"line-segment\" continuation of the Fibonacci sequence.\n\nIn Example 5.7, we use a more refined \"line segment continuation\", and demonstrate a uniform continuation that generates the distribution of leading blocks that satisfies strong Benford's Law up to the 4th digits, but does not satisfy the law for the leading blocks of length \\(>4\\).\n\nTheorem 1.8 suggests that given a uniform continuation \\(h\\) of the sequence \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\), we associate certain distributions of leading digits, coming from the equidistribution property. It's natural to consider the converse that given a sequence \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) with \"continuous distribution of leading digits\" of arbitrary length, we associate a certain uniform continuation of \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\). Theorem 1.11 below is a version for base-10 expansion. In Section 5, we introduce results on this topic for the Fibonacci sequence \\(\\{F_{n}\\}_{n=1}^{\\infty}\\). The proof of Theorem 1.11 is similar to, and simpler than Theorem 5.18 for the Fibonacci expansion, and leave it to the reader.\n**Theorem 1.11**.: _Let \\(\\{K_{n}\\}_{n=1}^{\\infty}\\) be a sequence of positive integers approaching \\(\\infty\\). Let \\(h_{K}^{*}:[0,1]\\to[0,1]\\) be the function given by \\(h_{K}^{*}(0)=0\\), \\(h_{K}^{*}(1)=1\\), and_\n\n\\[h_{K}^{*}(\\tfrac{1}{9}(\\beta-1))\\,=\\,\\lim_{s\\to\\infty}\\operatorname{Prob}\\big{\\{} \\,n\\in\\mathbb{N}:\\text{The s leading decimal digits of $K_{n}$ is $\\leq\\left\\lfloor 10^{s-1}\\beta\\right\\rfloor$}\\,\\big{\\}} \\tag{2}\\]\n\n_where \\(\\beta\\) varies over the real numbers in the interval \\([1,10)\\) and we assume that the RHS of (2) is defined for all \\(\\beta\\in[1,10)\\). If \\(h_{K}^{*}\\) is an increasing continuous function, then there is a uniform continuation \\(h\\) of the sequence \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\) such that \\({h_{\\infty}}^{-1}=h_{K}^{*}\\), and \\(\\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{ \\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{ \\operatorname{\\operatorname{\\operatorname{\\operatorname{\\operatorname{   \\operatorname{       \\cdot\nThe remainder of this paper is organized as follows. In Section 2, the notations for sequences and coefficient functions are introduced. In Section 3, the distribution of leading blocks of exponential sequences under Zeckendorf expansion is introduced, and Benford's Law and strong Benford's Law under Zeckendorf expansion are declared. Introduced in Section 4 are the method of calculating the distribution results introduced in Section 3, and also the distribution results for monomial sequences \\(\\{n^{a}\\}_{n=1}^{\\infty}\\). In Section 5, we introduce a general approach to the distributions of leading blocks under Zeckendorf expansion that are different from that of Benford's Law. The approach establishes the correspondence between the continuations of the Fibonacci sequences and the distributions of leading blocks under Zeckendorf expansion. In Section 6, we introduce definitions and results that generalize the contents of Sections 3, 4, and 5 for generalized Zeckendorf expansions. The absolute Benford's Law mentioned earlier in this section is properly introduced in Section 6 as well. In Section 7, the Benford behavior introduced in Theorem 1.14 is generalized for the setting of two generalized Zeckendorf expansions.\n\n## 2 Notation and definitions\n\n**Notation 2.1**.: Let \\(\\mathbb{N}_{0}:=\\mathbb{N}\\cup\\{0\\}\\), and let \\(\\Omega_{n}:=\\{k\\in\\mathbb{N}:k\\leq n\\}\\). For simpler notation, let us use a capital letter for a sequence of numbers, and use the infinite tuple notation for listing its values, e.g., \\(Q=(2,4,6,8,\\ldots)\\). We use the usual subscript notation for individual values, e.g., \\(Q_{3}=6\\).\n\n**Definition 2.2**.: Tuples \\((c_{1},c_{2},\\ldots,c_{t})\\in\\mathbb{N}_{0}^{t}\\) where \\(t\\in\\mathbb{N}\\) are called _coefficient functions of length_\\(t\\) if \\(c_{1}>0\\). If \\(\\epsilon\\) is a coefficient function of length \\(t\\), we denote the \\(k\\)th entry by \\(\\epsilon(k)\\) (if \\(k\\leq t\\)), and its length \\(t\\) by \\(\\operatorname{len}(\\epsilon)\\). For a coefficient function \\(\\epsilon\\), let \\(\\epsilon*Q\\) denote \\(\\sum_{k=1}^{t}\\epsilon(k)Q_{t-k+1}\\) where \\(t=\\operatorname{len}(\\epsilon)\\), and let \\(\\epsilon\\cdot Q\\) denote \\(\\sum_{k=1}^{t}\\epsilon(k)Q_{k}\\).\n\nIf \\(\\epsilon=(4,1,6,2)\\) and \\(Q\\) is a sequence, then \\(\\epsilon*Q=4Q_{4}+Q_{3}+6Q_{2}+2Q_{1}\\), and \\(\\epsilon\\cdot Q=4Q_{1}+Q_{2}+6Q_{3}+2Q_{4}\\).\n\n## 3 Benford's Law for Zeckendorf expansions\n\nLet \\(a\\) and \\(b\\) be two integers \\(>1\\) such that \\(\\gcd(a,b)=1\\). The sequence \\(K\\) be the sequence given by \\(K_{n}=a^{n}\\) is a standard example of sequences that satisfy Benford's Law under base-\\(b\\) expansion. We shall declare the behavior of the leading digits of the Zeckendorf expansion of \\(a^{n}\\) to be Benford's Law under Zeckendorf expansion.\n\nLet us begin with formulating Zeckendorf's Theorem in terms of coefficient functions.\n\n**Definition 3.1**.: Let \\(\\mathscr{F}\\) be the set of coefficient functions \\(\\epsilon\\) such that \\(\\epsilon(k)\\leq 1\\) for all \\(k\\leq\\operatorname{len}(\\epsilon)\\), and \\(\\epsilon(k)\\epsilon(k+1)=0\\) all \\(k\\leq\\operatorname{len}(\\epsilon)-1\\). Let \\(F\\) be the shifted Fibonacci sequence such that \\(F_{n+2}=F_{n+1}+F_{n}\\) for all \\(n\\in\\mathbb{N}\\) and \\((F_{1},F_{2})=(1,2)\\). Let \\(\\phi\\) be the golden ratio, let \\(\\omega:=\\phi^{-1}\\), and let \\(\\widehat{F}=(1,\\omega,\\omega^{2},\\ldots)\\) be the sequence given by \\(\\widehat{F}_{n}=\\omega^{n-1}\\).\nRecall the product notation from Definition 2.2.\n\n**Theorem 3.2** ([19], Zeckendorf's Theorem).: _For each positive integer \\(n\\), there is a unique coefficient function \\(\\epsilon\\in\\mathscr{F}\\) such that \\(n=\\epsilon*F\\)._\n\nRecall the example \\(3^{5}=F_{12}+F_{5}+F_{2}\\). If \\(\\epsilon=(1,0,0,0,0,0,0,1,0,0,1,0)\\), then \\(\\epsilon\\in\\mathscr{F}\\) and \\(3^{5}=c*F\\).\n\n**Definition 3.3**.: The expression \\(n=c*F\\) where \\(n\\in\\mathbb{N}\\) and \\(\\epsilon\\in\\mathscr{F}\\) is called _the \\(\\mathscr{F}\\)-expansion of \\(n\\)_ or _the Zeckendorf expansion of \\(n\\)_.\n\n### Benford's Law\n\nIf \\(\\epsilon\\in\\mathscr{F}\\) and \\(\\operatorname{len}(\\epsilon)\\geq 2\\), then \\((\\epsilon(1),\\epsilon(2))=(1,0)\\) is always the case, and hence, the probability of having \\((\\epsilon(1),\\epsilon(2))=(1,0)\\) is \\(1\\). For the purpose of demonstration, we consider the first three entries of \\(\\epsilon\\).\n\nTo denote arbitrarily many _leading blocks of coefficient functions_, which are defined in Definition 3.4 below, we shall use the boldface font and subscripts, e.g., \\(\\mathbf{b}_{1}\\) and \\(\\mathbf{b}_{2}\\), and in particular, \\(\\mathbf{b}_{k}\\) for \\(k=1,2\\) are not numbers, but tuples. The reader must not be confused with the entries of a sequence \\(Q\\), e.g., \\(Q_{1}\\) and \\(Q_{2}\\), which are numbers, and we use the regular font for sequences.\n\n**Definition 3.4**.: A coefficient function of length \\(s\\) is also called _a leading block of length \\(s\\)_ in the context of investigating the frequency of leading blocks, and it is denoted with boldface fonts, e.g. \\(\\mathbf{b}=(1,0,0,1)\\in\\mathscr{F}\\), \\(\\mathbf{b}(3)=0\\), and \\(\\mathbf{b}(4)=1\\). Let \\(\\mathscr{F}_{3}:=\\{\\mathbf{b}_{1},\\mathbf{b}_{2}\\}\\) where \\(\\mathbf{b}_{1}=(1,0,0)\\), \\(\\mathbf{b}_{2}=(1,0,1)\\) are leading blocks of length \\(3\\), and the set is called _the set of leading blocks of length \\(3\\) under \\(\\mathscr{F}\\)-expansion_. If \\(\\mathbf{b}\\in\\mathscr{F}_{3}\\) and \\(\\mathbf{b}=\\mathbf{b}_{1}\\), then define \\(\\widetilde{\\mathbf{b}}:=\\mathbf{b}_{2}\\), and and if \\(\\mathbf{b}\\in\\mathscr{F}_{3}\\) and \\(\\mathbf{b}=\\mathbf{b}_{2}\\), then define \\(\\widetilde{\\mathbf{b}}:=(1,1,0)\\).\n\nThe block \\(\\widetilde{\\mathbf{b}}=(1,1,0)\\) is not a member of \\(\\mathscr{F}\\), and hence, does not occur as the leading block of an \\(\\mathscr{F}\\)-expansion, but it's convenient to use for Theorem 3.5, where we rely on the equality \\(\\widetilde{\\mathbf{b}}\\cdot(1,\\omega^{1},\\omega^{2})=\\phi\\); see Definitions 2.2 and 3.1. The block \\(\\widetilde{\\mathbf{b}}\\) makes the statements of Definition 3.6 below more aesthetic, and the principle of defining an exclusive block such as \\((1,1,0)\\) for other generalized Zeckendorf expansions will be explained in Definition 3.7 and Section 6.\n\nThe following is a special version of Corollary 4.7, and it is Theorem 1.3 written in terms of the dot product and blocks. Recall the notation \\(\\operatorname{LB}_{s}\\) from Definition 1.1, the set \\(\\mathscr{F}_{3}\\) from Definition 3.4, the sequence \\(\\widehat{F}\\) from Definition 3.1, and the dot product from Definition 2.2.\n\n**Theorem 3.5**.: _Let \\(K\\) be a sequence given by \\(K_{n}=a^{n}\\) where \\(a>1\\) is an integer. Then, given \\(\\mathbf{b}\\in\\mathscr{F}_{3}\\),_\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{3}(K_{n})= \\mathbf{b}\\,\\right\\}\\;=\\;\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat {F}}{\\mathbf{b}\\cdot\\widehat{F}}.\\]\nMotivated from the distribution of these standard sequences, we introduce the following definition.\n\n**Definition 3.6**.: A sequence \\(K\\) of positive integers is said to _satisfy \\(\\mathscr{F}\\)-Benford's Law_ or _satisfy Benford's Law under \\(\\mathscr{F}\\)-expansion_ if given \\(\\mathbf{b}\\in\\mathscr{F}_{3}\\),\n\n\\[\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,\\colon\\operatorname{LB}_{3}(K_{n })=\\mathbf{b}\\,\\big{\\}}\\,=\\,\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot \\widehat{F}}{\\mathbf{b}\\cdot\\widehat{F}}.\\]\n\nLet us demonstrate how the structure of the formulas in Definition 3.6 compares with the one for base-10 expansion. Consider the two leading blocks \\(\\mathbf{c}_{1}=(2,1,2)\\) and \\(\\mathbf{c}_{2}=(2,1,3)\\) for base-10 expansion. Let \\(b=10\\). Then, strong Benford's Law for decimal expansion requires the probability of having the leading block \\(\\mathbf{c}_{1}\\) to be \\(\\log_{10}\\frac{213}{212}\\), which is equal to\n\n\\[\\log_{b}\\frac{\\mathbf{c}_{2}\\cdot(1,b^{-1},b^{-2})}{\\mathbf{c}_{1}\\cdot(1,b^ {-1},b^{-2})}\\,=\\,\\log_{b}\\frac{b^{2}\\mathbf{c}_{2}\\cdot(1,b^{-1},b^{-2})}{b^ {2}\\mathbf{c}_{1}\\cdot(1,b^{-1},b^{-2})}\\,=\\,\\log_{b}\\frac{\\mathbf{c}_{2}\\cdot (b^{2},b,1)}{\\mathbf{c}_{1}\\cdot(b^{2},b,1)}\\,=\\,\\log_{10}\\frac{213}{212}.\\]\n\nThe first expression in terms of the negative powers of \\(b\\) is analogous to the ones in Definition 3.6.\n\n### Strong Benford's Law\n\nUnder base-\\(b\\) expansion, a sequence \\(K\\) is said to satisfy strong Benford's Law if the probability of the first \\(M\\) leading digits of \\(K_{n}\\) satisfies a certain logarithmic distribution, and exponential sequences \\(\\{a^{n}\\}_{n=1}^{\\infty}\\) where \\(a>1\\) is an integer are standard examples that satisfy strong Benford's Law under base-\\(b\\) expansion. In Corollary 4.7, we calculate the distribution of leading blocks of arbitrary length of the Zeckendorf expansions of exponential sequence \\(\\{a^{n}\\}_{n=1}^{\\infty}\\). We declare this distribution to be _strong Benford's Law under Zeckendorf expansion_. We state the formal definition below.\n\nRecall the convolution \\(*\\) from Definition 2.2.\n\n**Definition 3.7**.: Given an integer \\(s\\geq 2\\), let \\(\\mathscr{F}_{s}:=\\{\\mathbf{b}_{1},\\mathbf{b}_{2},\\ldots,\\mathbf{b}_{\\ell}\\}\\) be the finite set of the leading blocks of length \\(s\\) occurring in the \\(\\mathscr{F}\\)-expansions of the positive integers such that \\(1+\\mathbf{b}_{k}*F=\\mathbf{b}_{k+1}*F\\) for all \\(k\\leq\\ell-1\\). The leading block \\(\\mathbf{b}_{\\ell}\\) is called _the largest leading block of length \\(s\\) under \\(\\mathscr{F}\\)-expansion_.\n\nIf \\(s\\) is even, then let \\(\\mathbf{b}_{\\ell+1}:=(1,0,1,0,\\ldots,1,0,1,1)\\), and if \\(s\\) is odd, then it is \\(\\mathbf{b}_{\\ell+1}:=(1,0,1,0,\\ldots,1,1,0)\\). If \\(\\mathbf{b}=\\mathbf{b}_{k}\\in\\mathscr{F}_{s}\\), then we denote \\(\\mathbf{b}_{k+1}\\) by \\(\\widetilde{\\mathbf{b}}\\).\n\nNotice that the existence of \\(\\widetilde{\\mathbf{b}}\\) defined above is guaranteed by Zeckendorf's Theorem. Let us demonstrate examples of \\(\\mathbf{b}\\) and \\(\\widetilde{\\mathbf{b}}\\). Let \\(\\mathbf{b}=(1,0,0,0,1,0)\\in\\mathscr{F}_{6}\\). Then, \\(\\widetilde{\\mathbf{b}}=(1,0,0,1,0,0)\\in\\mathscr{F}_{6}\\), and \\(1+\\mathbf{b}*F=\\widetilde{\\mathbf{b}}*F\\). If we list the coefficient functions in \\(\\mathscr{F}_{6}\\) with respect to the lexicographical order, then \\(\\widetilde{\\mathbf{b}}\\) is the immediate successor of \\(\\mathbf{b}\\) if \\(\\mathbf{b}\\neq(1,0,1,0,1,0)\\).\n\nFor each case of \\(s\\) being even or odd, the largest leading block \\(\\mathbf{b}\\) of length \\(s\\) satisfies \\(1+\\mathbf{b}*F=\\widetilde{\\mathbf{b}}*F\\). If \\(\\mathbf{b}^{\\prime}=(1,0,1,0,1,0)\\), then \\(\\widetilde{\\mathbf{b}}^{\\prime}=(1,0,1,0,1,1)\\), and below we shall demonstrate\nthat the equality \\(\\widetilde{\\mathbf{b}}^{\\prime}\\cdot\\widehat{F}=\\sum_{k=0}^{2}\\omega^{2k}+\\omega^ {5}=\\phi\\) makes the sum of the probabilities in Theorem 3.8 and Definition 3.9 be 1.\n\nLet us compare this setup with the case of base-10 expansion. Let \\(\\mathbf{c}=(4,5,6,7,8,9)\\) be the leading block of length 6 for base-10 expansion, and let the sequence \\(H\\) given by \\(H_{n}=10^{n-1}\\) be the \"base\" sequence. Then, \\(1+\\mathbf{c}*H=\\widetilde{\\mathbf{c}}*H\\) where \\(\\widetilde{\\mathbf{c}}=(4,5,6,7,9,0)\\). If we list all the coefficient functions of length 6, with respect to the lexicographical order, that are legal for base-10 expansion, then \\(\\widetilde{\\mathbf{c}}\\) is the immediate successor of \\(\\mathbf{c}\\). If \\(\\mathbf{c}^{\\prime}=(9,10,9,9,9,9)\\), then we let \\(\\widetilde{\\mathbf{c}}^{\\prime}=(9,10,0,0,0,0)\\), and \\(\\sum_{n=1}^{6}\\widetilde{\\mathbf{c}}^{\\prime}(n)10^{n-1}=1+\\mathbf{c}^{\\prime }*H=10^{6}\\). If strong Benford's Law under base-10 expansion is satisfied, the probability of having the leading block \\(\\mathbf{c}^{\\prime}\\) under base-10 expansion is\n\n\\[\\log_{10}\\frac{\\widetilde{\\mathbf{c}}^{\\prime}*H}{\\mathbf{c}^{\\prime}*H}\\,=\\, \\log_{10}\\frac{\\widetilde{\\mathbf{c}}^{\\prime}\\cdot\\widehat{H}}{\\mathbf{c}^{ \\prime}\\cdot\\widehat{H}}\\,=\\,1-\\log_{10}\\mathbf{c}^{\\prime}\\cdot\\widehat{H}\\]\n\nwhere \\(\\widehat{H}\\) is the sequence given by \\(\\widehat{H}_{n}=10^{-(n-1)}\\).\n\nRecall the sequence \\(\\widehat{F}\\) from Definition 3.1.\n\n**Theorem 3.8**.: _Let \\(K\\) be a sequence of positive integers given by \\(K_{n}=ab^{n}(1+o(1))\\) where a and \\(b\\) are positive real numbers such that \\(\\log_{\\phi}b\\) is irrational. Then, given \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\) where \\(s\\geq 2\\),_\n\n\\[\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\big{\\}}\\,=\\,\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat {F}}{\\mathbf{b}\\cdot\\widehat{F}}.\\]\n\nProof.: It follows immediately from Corollary 4.7. \n\nLet us demonstrate below that the probabilities add up to 1 for \\(s=6\\), but the argument is sufficiently general to be extended for all cases of \\(s\\). Let \\(\\mathscr{F}_{6}=(\\mathbf{b}_{1},\\ldots,\\mathbf{b}_{\\ell})\\) such that \\(\\mathbf{b}_{k+1}=\\widetilde{\\mathbf{b}}_{k}\\) for all \\(1\\leq k\\leq\\ell\\). Then, \\(\\mathbf{b}_{1}=(1,0,0,0,0,0)\\) and \\(\\mathbf{b}_{\\ell}=(1,0,1,0,1,0)\\). Then, \\(\\mathbf{b}_{\\ell+1}=(1,1,0,0,0,0)\\), and\n\n\\[\\sum_{k=1}^{\\ell}\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}_{k}\\cdot\\widehat{F}} {\\mathbf{b}_{k}\\cdot\\widehat{F}}\\,=\\,\\sum_{k=1}^{\\ell}\\log_{\\phi}(\\mathbf{b}_ {k+1}\\cdot\\widehat{F})-\\log_{\\phi}(\\mathbf{b}_{k}\\cdot\\widehat{F})\\,=\\,\\log_{ \\phi}(\\mathbf{b}_{\\ell+1}\\cdot\\widehat{F})-\\log_{\\phi}1\\,=\\,1.\\]\n\n**Definition 3.9**.: Let \\(K\\) be a sequence of positive integers approaching \\(\\infty\\). Then, \\(K\\) is said to _satisfy strong Benford's Law under \\(\\mathscr{F}\\)-expansion_ if given \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\) where \\(s\\geq 2\\),\n\n\\[\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\big{\\}}\\,=\\,\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat{ F}}{\\mathbf{b}\\cdot\\widehat{F}}.\\]\n\n**Example 3.10**.: Let \\(K\\) be a sequence satisfying strong Benford's Law under \\(\\mathscr{F}\\)-expansion, e.g., \\(\\{2^{n}\\}_{n=1}^{\\infty}\\); see Theorem 3.8. Let \\(\\mathbf{b}=(1,0,0,0,1,0)\\), so \\(\\widetilde{\\mathbf{b}}=(1,0,0,1,0,0)\\). Then,\n\n\\[\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{6}(K_{n})= \\mathbf{b}\\,\\big{\\}}\\,=\\,\\log_{\\phi}\\frac{1+\\omega^{3}}{1+\\omega^{4}}\\approx 0.157.\\]\nCalculations\n\nNotice that \\(\\log_{b}(x)\\) makes it convenient to calculate the distribution of the leading digits of exponential sequences \\(\\{a^{n}\\}_{n=1}^{\\infty}\\) under base-\\(b\\) expansion where \\(b>1\\) is an integer. In this section, we introduce an analogue of \\(\\log_{b}(x)\\) for Zeckendorf expansion in Section 4.1, and use it for various calculations.\n\nAs mentioned in the introduction, these functions are merely a tool for calculating the leading digits, and in Section 5, we consider other continuations, and demonstrate their connections to different distributions of leading digits.\n\n### An analytic continuation of the Fibonacci sequence\n\nBelow we introduce an analytic continuation of the Fibonacci sequence.\n\n**Definition 4.1**.: Let \\(\\alpha=\\frac{\\phi}{\\sqrt{5}}\\), and define \\(\\mathfrak{F}:\\mathbb{R}\\to\\mathbb{R}\\) be the function given by\n\n\\[\\mathfrak{F}(x)=\\alpha(\\phi^{x}+\\phi^{-x}\\cos(\\pi x)\\phi^{-2}).\\]\n\nWe call the function \\(a\\)_Benford continuation of the Fibonacci sequence_.\n\nNotice that \\(F_{n}=\\frac{1}{\\sqrt{5}}(\\phi^{n+1}-(-1\/\\phi)^{n+1})=\\frac{\\phi}{\\sqrt{5}}( \\phi^{n}+(-1)^{n}\\phi^{-(n+2)})\\). Thus, \\(\\mathfrak{F}\\) is a real analytic continuation of \\(F_{n}\\), so \\(\\mathfrak{F}(n)=F_{n}\\) for all \\(n\\in\\mathbb{N}\\). It is an increasing function on \\([1,\\infty)\\). Let \\(\\mathfrak{F}^{-1}\\) denote the inverse function of \\(\\mathfrak{F}:[1,\\infty)\\to\\mathbb{R}\\). Comparing it with the case of base-10 expansion, we find that \\(10^{x-1}\\) is an analytic continuation of the sequence \\(\\{10^{n-1}\\}_{n=1}^{\\infty}\\), and its inverse is \\(1+\\log_{10}(x)\\), which is the main object for the equidistribution for Benford's Law under base-10 expansion. The equidistribution property described in Theorem 4.5 is associated with strong Benford's Law under \\(\\mathscr{F}\\)-expansion, and the name of the function is due to this connection.\n\n**Lemma 4.2**.: _For real numbers \\(x\\geq 1\\), we have \\(\\mathfrak{F}(x)=\\alpha\\phi^{x}+O(\\phi^{-x})\\), and_\n\n\\[\\mathfrak{F}^{-1}(x)\\;=\\;\\log_{\\phi}(x)-\\log_{\\phi}(\\alpha)+O(1\/x^{2}).\\]\n\nProof.: Let \\(y=\\alpha\\phi^{x}+\\alpha\\phi^{-x}\\cos(\\pi x)\\phi^{-2}\\) and let \\(w=\\alpha\\phi^{-x}\\cos(\\pi x)\\phi^{-2}=O(\\phi^{-x})\\). Since \\(y=\\alpha\\phi^{x}+o(1)\\), we have \\(w=O(1\/y)\\). Then, \\(y=\\alpha\\phi^{x}+w\\) implies\n\n\\[x \\;=\\;\\log_{\\phi}(y-w)-\\log_{\\phi}\\alpha\\;=\\;\\log_{\\phi}(y)-\\log _{\\phi}\\alpha+\\log_{\\phi}(1-w\/y)\\] \\[\\;=\\;\\log_{\\phi}(y)-\\log_{\\phi}\\alpha+O(|w|\/y)\\;=\\;\\log_{\\phi}(y )-\\log_{\\phi}\\alpha+O(1\/y^{2}).\\]\n### Equidistribution\n\nRecall the set \\(\\mathscr{F}_{s}\\) of leading blocks from Definition 3.7. In this section, having a leading block \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\) is interpreted in terms of the fractional part of the values of \\(\\widetilde{\\mathfrak{F}}^{-1}\\).\n\n**Definition 4.3**.: Given \\(\\epsilon\\in\\mathbb{N}_{0}^{t}\\) and an integer \\(s\\leq t\\), let \\(\\epsilon|s:=(\\epsilon(1),\\ldots,\\epsilon(s))\\).\n\nRecall \\(\\widehat{F}\\) from Definition 3.1 and the product notation from Definition 2.2.\n\n**Lemma 4.4**.: _Let \\(K\\) be a sequence of positive real numbers approaching \\(\\infty\\), and let \\(s\\) be an integer \\(\\geq 2\\). Let \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\), and let \\(A_{\\mathbf{b}}:=\\{n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})=\\mathbf{b}\\}\\). Then, there are real numbers \\(\\gamma_{n}=o(1)\\) and \\(\\widetilde{\\gamma}_{n}=o(1)\\) such that \\(n\\in A_{\\mathbf{b}}\\) if and only if_\n\n\\[\\log_{\\phi}\\mathbf{b}\\cdot\\widehat{F}+\\gamma_{n}\\;\\leq\\;\\operatorname{frc} \\bigl{(}\\widetilde{\\mathfrak{F}}^{-1}(K_{n})\\bigr{)}\\;<\\;\\log_{\\phi}\\widetilde {\\mathbf{b}}\\cdot\\widehat{F}+\\widetilde{\\gamma}_{n} \\tag{3}\\]\n\n_where \\(\\widetilde{\\gamma}_{n}=0\\) if \\(\\mathbf{b}\\) is the largest block of length \\(s\\)._\n\nProof.: Suppose that \\(n\\in\\mathbb{N}\\) is sufficiently large, so that \\(\\mathbf{b}^{\\prime}:=\\operatorname{LB}_{s}(K_{n})\\) exists. By Zeckendorf's Theorem, there is \\(\\mu\\in\\mathscr{F}\\) such that \\(K_{n}=\\mu*F\\), so \\(m:=\\operatorname{len}(\\mu)\\geq s\\), and \\(\\mathbf{b}^{\\prime}=\\mu|s\\). There are \\(\\epsilon\\in\\mathscr{F}\\) of length \\(m\\) and a coefficient function \\(\\check{\\epsilon}\\) of length \\(m\\) such that \\(\\epsilon|s=\\mathbf{b}^{\\prime}\\), \\(\\check{\\epsilon}|s=\\widetilde{\\mathbf{b}}^{\\prime}\\), \\(\\epsilon(k)=\\check{\\epsilon}(k)=0\\) for all \\(k>s\\), so \\(\\epsilon*F\\leq K_{n}<\\check{\\epsilon}*F\\). Recall \\(\\alpha\\) from Definition 4.1. Then,\n\n\\[\\epsilon*F\\;=\\;\\alpha\\sum_{k=1}^{s}\\epsilon(k)\\phi^{m-k+1}+O(1)\\;=\\;\\alpha\\phi ^{m}(1+o(1))\\sum_{k=1}^{s}\\epsilon(k)\\omega^{k-1}\\;=\\;\\alpha\\phi^{m}(1+o(1)) \\,\\mathbf{b}^{\\prime}\\cdot\\widehat{F}.\\]\n\nBy Lemma 4.2,\n\n\\[\\widetilde{\\mathfrak{F}}^{-1}(\\epsilon*F)\\;=\\;m+\\log_{\\phi}(\\mathbf{b}^{ \\prime}\\cdot\\widehat{F})+\\gamma_{n},\\quad\\gamma_{n}\\;=\\;o(1).\\]\n\nSimilarly, we have \\(\\widetilde{\\mathfrak{F}}^{-1}(\\check{\\epsilon}*F)=m+\\log_{\\phi}(\\widetilde {\\mathbf{b}}^{\\prime}\\cdot\\widehat{F})+\\widetilde{\\gamma}_{n}\\) where \\(\\widetilde{\\gamma}_{n}=o(1)\\). If \\(\\mathbf{b}^{\\prime}\\) is the largest block of length \\(s\\), then \\(\\check{\\epsilon}*F=F_{m+1}\\), and hence, \\(\\widetilde{\\mathfrak{F}}^{-1}(\\check{\\epsilon}*F)=m+1\\), which implies \\(\\widetilde{\\gamma}_{n}=0\\). In general, \\(\\check{\\epsilon}*F\\leq F_{m+1}\\), so \\(\\widetilde{\\mathfrak{F}}^{-1}(\\check{\\epsilon}*F)\\leq m+1\\).\n\nThus, if \\(n\\in A_{\\mathbf{b}}\\), then \\(\\mathbf{b}^{\\prime}=\\mathbf{b}\\), and\n\n\\[\\epsilon*F\\leq K_{n}\\;<\\;\\check{\\epsilon}*F\\Rightarrow\\widetilde{ \\mathfrak{F}}^{-1}(\\epsilon*F)\\leq\\widetilde{\\mathfrak{F}}^{-1}(K_{n})\\;<\\; \\widetilde{\\mathfrak{F}}^{-1}(\\check{\\epsilon}*F)\\] \\[\\qquad\\Rightarrow\\log_{\\phi}\\mathbf{b}\\cdot\\widehat{F}+\\gamma_{n }\\;\\leq\\;\\operatorname{frc}\\bigl{(}\\widetilde{\\mathfrak{F}}^{-1}(K_{n}) \\bigr{)}\\;<\\;\\log_{\\phi}\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}+\\widetilde{ \\gamma}_{n}.\\]\n\nThere is no difficulty in reversing this argument, and we leave the proof of the converse to the reader. \n\n**Theorem 4.5**.: _Let \\(K\\) be an increasing sequence of positive integers such that \\(\\operatorname{frc}\\bigl{(}\\widetilde{\\mathfrak{F}}^{-1}(K_{n})\\bigr{)}\\) is equidistributed. Then, \\(K\\) satisfies strong Benford's Law under the \\(\\mathscr{F}\\)-expansion._\nProof.: Notice that \\(\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})=\\mathbf{b} \\,\\right\\}\\) where \\(s\\geq 2\\) is equal to the probability of \\(n\\) satisfying (3). Let \\(t\\in\\mathbb{N}\\). Then, there is an integer \\(M_{t}\\) such that \\(\\left|\\gamma_{n}\\right|\\) and \\(\\left|\\widetilde{\\gamma}_{n}\\right|\\) are \\(<1\/t\\) for all \\(n\\geq M_{t}\\). Thus, by Lemma 4.4,\n\n\\[\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\operatorname{LB}_{s}(K_{n})=B\\, \\right\\}+o(1)\\]\n\n\\[\\leq\\,\\operatorname{Prob}\\left\\{k\\in\\Omega_{n}:\\log_{\\phi}\\mathbf{b}\\cdot \\widehat{F}-\\frac{1}{t}\\,\\leq\\,\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}} ^{-1}(K_{n})\\right)\\,<\\,\\log_{\\phi}\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}+ \\frac{1}{t}\\right\\}+o(1)\\]\n\n\\[\\Rightarrow\\,\\limsup_{n}\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}: \\operatorname{LB}_{s}(K_{n})=\\mathbf{b}\\,\\right\\}\\,\\leq\\,\\log_{\\phi}\\frac{ \\widetilde{\\mathbf{b}}\\cdot\\widehat{F}}{\\mathbf{b}\\cdot\\widehat{F}}+\\frac{2}{ t}.\\]\n\n\\[\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\right\\}+o(1)\\]\n\n\\[\\geq\\,\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\log_{\\phi}\\mathbf{b}\\cdot \\widehat{F}+\\frac{1}{t}\\leq\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}}^{-1 }(K_{n})\\right)\\,<\\,\\log_{\\phi}\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}-\\frac{1} {t}\\,\\right\\}+o(1)\\]\n\n\\[\\Rightarrow\\,\\liminf_{n}\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}: \\operatorname{LB}_{s}(K_{n})=\\mathbf{b}\\,\\right\\}\\,\\geq\\,\\log_{\\phi}\\frac{ \\widetilde{\\mathbf{b}}\\cdot\\widehat{F}}{\\mathbf{b}\\cdot\\widehat{F}}-\\frac{2}{ t}.\\]\n\nSince \\(\\liminf\\) and \\(\\limsup\\) are independent of \\(t\\), we prove that \\(\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\right\\}=\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}}{ \\mathbf{b}\\cdot\\widehat{F}}\\).\n\nThe converse of Theorem 4.5 is true as well, i.e., if \\(K\\) satisfies strong Benford's Law under \\(\\mathcal{F}\\)-expansion, then \\(\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}}^{-1}(K_{n})\\right)\\) is equidistributed. We shall prove it in Section 5.\n\nThe following lemma is useful, and it is probably known.\n\n**Lemma 4.6**.: _Let \\(h:\\mathbb{N}\\to\\mathbb{R}\\) be a function such that \\(\\operatorname{frc}(h(n))\\) is equidistributed, and let \\(E:\\mathbb{N}\\to\\mathbb{R}\\) be a function such that \\(E(n)\\to 0\\) as \\(n\\to\\infty\\). Then, \\(\\operatorname{frc}(h(n)+E(n))\\) is equidistributed._\n\n**Corollary 4.7**.: _Let \\(K\\) be a sequence of positive integers given by \\(K_{n}=ab^{n}(1+o(1))\\) where \\(a\\) and \\(b\\) are positive real numbers such that \\(\\log_{\\phi}b\\) is irrational. Then, \\(\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}}^{-1}(K_{n})\\right)\\) is equidistributed, and hence, given \\(\\mathbf{b}\\in\\mathcal{F}_{s}\\) where \\(s\\geq 2\\),_\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\right\\}\\,=\\,\\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat {F}}{\\mathbf{b}\\cdot\\widehat{F}}.\\]\n\nProof.: By Lemma 4.2,\n\n\\[\\widehat{\\mathfrak{F}}^{-1}(K_{n})\\,=\\,n\\log_{\\phi}b-\\log_{\\phi}(a\/\\alpha)+ \\log_{\\phi}(1+o(1))+o(1).\\]\n\nSince \\(\\log_{\\phi}b\\) is irrational, by Weyl's Equidistribution Theorem, \\(\\operatorname{frc}\\left(n\\log_{\\phi}b\\right)\\) is equidistributed, and by the lemma, \\(\\operatorname{frc}\\left(n\\log_{\\phi}b+o(1)\\right)\\) is equidistributed. Shifting it by a constant \\(-\\log_{\\phi}(a\/\\alpha)\\) does not change the equidistribution property, and this concludes the proof. \n\nFor example, if \\(K\\) is a sequence given by \\(K_{n}=\\sum_{k=1}^{N}a_{k}\\,b_{k}^{\\,n}\\) where \\(a_{k},b_{k}\\in\\mathbb{Z}\\), \\(a_{1}>0\\), and \\(b_{1}>|b_{k}|\\) for all \\(k\\geq 2\\), then \\(K_{n}=a_{1}b_{1}^{n}(1+o(1))\\), and \\(\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}}^{-1}(K_{n})\\right)\\) is equidistributed. Many increasing sequences \\(K\\) of positive integers given by a linear recurrence with constant positive integer coefficients satisfy \\(K_{n}=ab^{n}(1+o(1))\\) where \\(\\log_{\\phi}(b)\\) is irrational, and hence, \\(\\operatorname{frc}\\left(\\widehat{\\mathfrak{F}}^{-1}(K_{n})\\right)\\) is equidistributed.\n### The leading blocks of integer powers\n\nLet \\(a\\) be a positive integer, and let \\(K\\) be the sequence given by \\(K_{n}=n^{a}\\). Then, \\(K\\) does not satisfy Benford's Law under the base-10 expansion, but it has a close relationship with Benford's Law [14]. In this section, we show that both statements are true under \\(\\mathscr{F}\\)-expansion as well. Recall \\(\\Omega_{n}\\) from Notation 2.1 and \\(\\mathscr{F}_{3}\\) from Definition 3.4, and let \\(\\mathbf{b}_{1}:=(1,0,0)\\in\\mathscr{F}_{3}\\). We also introduce the oscillating behavior of \\(\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\operatorname{LB}_{3}(K_{k})= \\mathbf{b}_{1}\\,\\right\\}\\) as \\(n\\to\\infty\\), and hence, \\(\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{3}(K_{n})= \\mathbf{b}_{1}\\,\\right\\}\\) does not exist.\n\n**Example 4.8**.: Let \\(K\\) be the sequence given by \\(K_{n}=n\\), and let \\(t>0\\) be a large integer. Given a sufficiently large positive random integer \\(n<F_{t+1}\\), let \\(n=\\mu*F\\) be the \\(\\mathscr{F}\\)-expansion, and \\(M:=\\operatorname{len}(\\mu)\\). Notice that \\(\\operatorname{LB}_{3}(n)=\\mathbf{b}_{1}\\) if and only if \\(n=F_{M}+m\\) where \\(0\\leq m<F_{M-2}\\). Thus, there are \\(F_{M-2}\\) integers \\(n\\) in \\([1,F_{t+1})\\) such that \\(F_{M}\\leq n<F_{M+1}\\) and \\(\\operatorname{LB}_{3}(n)=\\mathbf{b}_{1}\\). Thus,\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in\\Omega_{F_{t+1}}:\\operatorname{LB}_{3}(n)= \\mathbf{b}_{1}\\,\\right\\}\\,=\\,\\left(\\frac{1}{F_{t+1}}\\sum_{M=3}^{t}F_{M-2} \\right)+o(1)=\\left(\\frac{1}{F_{t+1}}\\sum_{M=3}^{t}\\alpha\\phi^{M-2}+o(1)\\right) +o(1)\\\\ =\\,\\frac{1}{\\alpha\\phi^{t+1}+o(1)}\\frac{\\alpha\\phi^{t-1}}{\\phi-1} +o(1)\\,=\\,\\frac{1}{\\phi^{2}(\\phi-1)}+o(1)\\,=\\,\\phi-1+o(1)\\]\n\nas function of \\(t\\). However, by Theorem 4.10, we have\n\n\\[\\limsup_{n}\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}: \\operatorname{LB}_{3}(k)=\\mathbf{b}_{1}\\,\\right\\}\\,=\\,\\frac{\\phi+1}{\\phi+2} \\approx.724,\\] \\[\\liminf_{n}\\operatorname{Prob}\\left\\{\\,k\\in\\Omega_{n}: \\operatorname{LB}_{3}(k)=\\mathbf{b}_{1}\\,\\right\\}\\,=\\,\\phi-1\\approx.618.\\]\n\nThus, \\(\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{3}(n)= \\mathbf{b}_{1}\\,\\right\\}\\) does not exist.\n\nRecall \\(\\mathfrak{F}\\) from Definition 4.1, and its inverse \\(\\mathfrak{F}^{-1}\\). We use the function \\(\\mathfrak{F}\\) to more generally handle the distribution of the leading blocks of \\(\\{n^{a}\\}_{n=1}^{\\infty}\\) with any length. Given a positive integer \\(m\\), let \\(A_{m}=\\{n\\in\\mathbb{N}:n<F_{m}^{1\/a}\\}\\).\n\n**Lemma 4.9**.: _If \\(\\beta\\in[0,1]\\), then_\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in A_{m}:\\operatorname{frc}\\left(\\mathfrak{F}^{- 1}(n^{a})\\right)\\leq\\beta\\,\\right\\}\\,=\\,\\frac{\\phi^{\\beta\/a}-1}{\\phi^{1\/a}-1}+ O(m\\phi^{-m\/a}).\\]\n\nProof.: Let \\(m\\in\\mathbb{N}\\), and let \\(n\\in A^{\\prime}_{m+1}:=A_{m+1}-A_{m}\\), so that \\(F_{m}\\leq n^{a}<F_{m+1}\\) and \\(m\\leq\\mathfrak{F}^{-1}(n^{a})<m+1\\). Thus, given a real number \\(\\beta\\in[0,1]\\),\n\n\\[\\left\\{\\,n\\in A^{\\prime}_{m+1}:\\operatorname{frc}\\left(\\mathfrak{F}^{- 1}(n^{a})\\right)\\leq\\beta\\,\\right\\} \\,=\\,\\left\\{\\,n\\in A^{\\prime}_{m+1}:m\\leq\\mathfrak{F}^{-1}(n^{a}) \\leq m+\\beta\\,\\right\\}\\] \\[\\,=\\,\\left\\{\\,n\\in A^{\\prime}_{m+1}:\\operatorname{frc}\\left( \\mathfrak{F}^{-1}(n^{a})\\right)\\leq\\beta\\,\\right\\} \\,=\\,\\mathfrak{F}(m+\\beta)^{1\/a}-\\mathfrak{F}(m)^{1\/a}+O(1)\\] \\[\\,=\\,\\alpha^{1\/a}\\phi^{(m+\\beta)\/a}-\\alpha^{1\/a}\\phi^{m\/a}+O(1)\\] \\[\\,=\\,\\alpha^{1\/a}\\phi^{(m+\\beta)\/a}\\gamma-\\alpha^{1\/a}\\phi^{m\/a} \\gamma+O(m),\\quad\\gamma=\\frac{\\phi^{1\/a}}{\\phi^{1\/a}-1}.\\]\nThis proves that\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in A_{m+1}:\\mathrm{frc}\\left(\\widehat{ \\mathfrak{F}}^{-1}(n^{a})\\right)\\leq\\beta\\,\\right\\} \\,=\\,\\frac{\\alpha^{1\/a}\\phi^{(m+\\beta)\/a}\\gamma-\\alpha^{1\/a}\\phi^{m\/ a}\\gamma+O(m)}{F_{m+1}^{1\/a}+O(1)}\\] \\[\\,=\\,\\frac{\\phi^{\\beta\/a}\\gamma-\\gamma+O(m\\phi^{-m\/a})}{\\phi^{1\/a} +O(\\phi^{-m\/a})}\\,=\\,\\frac{\\phi^{\\beta\/a}-1}{\\phi^{1\/a}-1}+O(m\\phi^{-m\/a}).\\]\n\nRecall from Lemma 4.4 that\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in A_{m}:\\mathrm{LB}_{3}(n^{a})=\\mathbf{b}_{1}\\, \\right\\}=\\mathrm{Prob}\\left\\{\\,n\\in A_{m}:\\mathrm{frc}\\left(\\widehat{ \\mathfrak{F}}^{-1}(n^{a})\\right)\\leq\\delta_{1}+o(1)\\,\\right\\}\\]\n\nwhere \\(\\delta_{1}:=\\log_{\\phi}\\frac{\\widehat{\\mathbf{b}}_{1}:\\widehat{F}}{\\mathbf{b }_{1}:\\widehat{F}}\\). Thus, as \\(m\\to\\infty\\), by Lemma 4.9,\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in A_{m}:\\mathrm{LB}_{3}(n^{a})=\\mathbf{b}_{1}\\, \\right\\}\\,\\to\\,\\frac{\\phi^{\\delta_{1}\/a}-1}{\\phi^{1\/a}-1}\\,=\\,\\frac{(1+\\omega ^{2})^{1\/a}-1}{\\phi^{1\/a}-1}\\]\n\nwhere \\(\\omega=\\phi^{-1}\\). Let us show that\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in A_{m}:\\mathrm{LB}_{3}(n^{a})=\\mathbf{b}_{1}\\, \\right\\}\\,\\not\\to\\,\\delta_{1}\\]\n\nas \\(m\\to\\infty\\). We claim that the ratio \\(\\frac{(1+\\omega^{2})^{1\/a}-1}{\\phi^{1\/a}-1}\\) is not equal to \\(\\delta_{1}=\\log_{\\phi}(1+\\omega^{2})\\). Since \\(a\\in\\mathbb{N}\\), the ratio is an algebraic number over \\(\\mathbb{Q}\\). However, by the Gelfand-Schneider Theorem, \\(\\log_{\\phi}(1+\\omega^{2})\\) is a transcendental number. Thus, \\(K\\) does not satisfy Benford's Law under the \\(\\mathscr{F}\\)-expansion.\n\nHowever, as noted in [14] for base-\\(b\\) expansions, we have\n\n\\[\\lim_{a\\to\\infty}\\lim_{m\\to\\infty}\\mathrm{Prob}\\left\\{\\,n\\in A_{m}:\\mathrm{ LB}_{3}(n^{a})=\\mathbf{b}_{1}\\,\\right\\}\\,=\\,\\lim_{a\\to\\infty}\\frac{\\phi^{ \\delta_{1}\/a}-1}{\\phi^{1\/a}-1}\\,=\\,\\delta_{1}\\,=\\,\\log_{\\phi}(1+\\omega^{2}).\\]\n\nEven though the leading blocks of \\(K_{n}\\) do not satisfy Benford's Law under \\(\\mathscr{F}\\)-expansion, the limiting behavior of high power sequences for special values of \\(n\\) resembles Benford's Law.\n\nRecall \\(\\Omega_{n}\\) from Definition 2.1. Let us use Lemma 4.9 to prove that \\(\\mathrm{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\mathrm{frc}\\left(\\widehat{\\mathfrak{F}}^ {-1}(K_{k})\\right)\\leq\\beta\\,\\right\\}\\) oscillates, and does not converge.\n\n**Theorem 4.10**.: _Let \\(\\beta\\) be a real number in \\([0,1]\\), and let \\(\\,r:=(\\phi^{\\beta\/a}-1)\/(\\phi^{1\/a}-1)\\). Given an integer \\(n>1\\), let \\(\\widehat{\\mathfrak{F}}^{-1}(n^{a})=m+p\\) where \\(\\,p=\\mathrm{frc}\\left(\\widehat{\\mathfrak{F}}^{-1}(n^{a})\\right)\\) and \\(\\,m\\in\\mathbb{N}\\). Then,_\n\n\\[P_{n}:=\\mathrm{Prob}\\left\\{\\,k\\in\\Omega_{n}:\\mathrm{frc}\\left(\\widehat{ \\mathfrak{F}}^{-1}(K_{k})\\right)\\leq\\beta\\,\\right\\}\\,=\\,\\,\\begin{cases}\\frac{r+ \\phi^{p\/a}-1}{\\phi^{p\/a}}+O(m\\phi^{-m\/a})&\\text{if }0\\leq p\\leq\\beta\\\\ \\frac{r+\\phi^{p\/a}-1}{\\phi^{p\/a}}+O(m\\phi^{-m\/a})&\\text{if }\\beta<p<1\\end{cases}.\\]\n\n_In particular,_\n\n\\[\\limsup P_{n}=r\\phi^{1\/a-\\beta\/a}=\\beta+O(1\/a),\\quad and\\quad\\liminf P_{n}=r= \\beta+O(1\/a).\\]\nProof.: Let \\(m\\) be a sufficiently large positive integer, and let \\(n\\in A_{m+1}-A_{m}\\). Let \\(n=\\mathfrak{F}(m+p)^{1\/a}\\) for \\(p\\in[0,1)\\). If \\(p\\leq\\beta\\), then, \\(\\operatorname{frc}\\left(\\mathfrak{F}^{-1}(n^{a})\\right)=\\operatorname{frc} \\left(\\mathfrak{F}^{-1}\\mathfrak{F}(m+p)\\right)=\\operatorname{frc}(m+p)=p\\leq\\beta\\), and if \\(p>\\beta\\), then, \\(\\operatorname{frc}\\left(\\mathfrak{F}^{-1}(n^{a})\\right)=p>\\beta\\). Thus,\n\n\\[\\left\\{n\\in A_{m+1}-A_{m}:\\operatorname{frc}\\left(\\mathfrak{F}^{-1}(n^{a}) \\right)\\leq\\beta\\right\\}\\;=\\;\\left\\{n\\in A_{m+1}-A_{m}:n\\leq\\mathfrak{F}(m+ \\beta)^{1\/a}\\right\\}.\\]\n\nIf \\(n\\leq\\mathfrak{F}(m+\\beta)^{1\/a}\\), i.e., \\(p\\leq\\beta\\), then by Lemma 4.9\n\n\\[P_{n} =\\;\\frac{1}{n}\\left(\\operatorname{Prob}\\left\\{\\;k\\in A_{m}: \\operatorname{frc}\\left(\\mathfrak{F}^{-1}(k^{a})\\right)\\leq\\beta\\;\\right\\}\\; \\#A_{m}+n-\\mathfrak{F}(m)^{1\/a}+O(1)\\right)\\] \\[=\\;\\frac{r\\mathfrak{F}(m)^{1\/a}+O(m)+\\mathfrak{F}(m+p)^{1\/a}- \\mathfrak{F}(m)^{1\/a}}{\\mathfrak{F}(m+p)^{1\/a}+O(1)}\\] \\[=\\;\\frac{r+O(m\\phi^{-m\/a})+\\phi^{p\/a}-1}{\\phi^{p\/a}+O(\\phi^{-m\/a })}\\;=\\;\\frac{r+\\phi^{p\/a}-1}{\\phi^{p\/a}}+O(m\\phi^{-m\/a})\\]\n\nIf \\(n>\\mathfrak{F}(m+\\beta)^{1\/a}\\), i.e., \\(p>\\beta\\), then\n\n\\[P_{n}=\\frac{r+\\phi^{\\beta\/a}-1}{\\phi^{p\/a}}+O(m\\phi^{-m\/a})\\;=\\; \\frac{r\\phi^{1\/a}}{\\phi^{p\/a}}+O(m\\phi^{-m\/a}).\\] \\[\\text{Thus, }\\limsup P_{n}=\\frac{r+\\phi^{\\beta\/a}-1}{\\phi^{\\beta\/a }}=\\frac{r\\phi^{1\/a}}{\\phi^{\\beta\/a}},\\quad\\liminf P_{n}=\\frac{r\\phi^{1\/a}}{ \\phi^{1\/a}}=r.\\]\n\nThus, \\(\\operatorname{Prob}\\left\\{\\;n\\in\\mathbb{N}:\\operatorname{frc}\\left( \\mathfrak{F}^{-1}(K_{n})\\right)\\leq\\beta\\;\\right\\}\\) does not converge, but \\(\\operatorname{frc}\\left(\\mathfrak{F}^{-1}(K_{n})\\right)\\) is almost equidistributed for large values of \\(a\\).\n\n**Example 4.11**.: Let \\(\\mathbf{b}\\) and \\(\\widetilde{\\mathbf{b}}\\) be the blocks defined in Example 3.10, and let \\(K\\) be the sequence given by \\(K_{n}=n^{2}\\). By Lemma 4.4, if \\(D:=\\{n\\in\\mathbb{N}:\\operatorname{LB}_{6}(K_{n})=\\mathbf{b}\\}\\), then for \\(n\\in D\\),\n\n\\[\\log_{\\phi}(1+\\omega^{4})+o(1)\\;<\\;\\operatorname{frc}\\left(\\mathfrak{F}^{-1}(K _{n})\\right)\\;<\\;\\log_{\\phi}(1+\\omega^{3})+o(1)\\]\n\nwhere the upper and lower bounds are functions of \\(n\\in D\\). Let \\(\\beta=\\log_{\\phi}(1+\\omega^{4})\\) and \\(\\widetilde{\\beta}=\\log_{\\phi}(1+\\omega^{3})\\). Recall \\(\\Omega_{n}\\) from Definition 2.1. Then,\n\n\\[\\operatorname{Prob}\\left\\{\\;k\\in\\Omega_{n}:\\operatorname{LB}_{6}( K_{k})=\\mathbf{b}\\;\\right\\}=\\] \\[\\operatorname{Prob}\\left\\{\\;k\\in\\Omega_{n}:\\operatorname{frc} \\left(\\mathfrak{F}^{-1}(K_{n})\\right)<\\widetilde{\\beta}\\;\\right\\}\\;-\\; \\operatorname{Prob}\\left\\{\\;k\\in\\Omega_{n}:\\operatorname{frc}\\left(\\mathfrak{F} ^{-1}(K_{n})\\right)<\\beta\\;\\right\\}\\;+\\;o(1).\\]\nLet \\(r=(\\phi^{\\beta\/2}-1)\/(\\phi^{1\/2}-1)\\) and \\(\\widetilde{r}=(\\phi^{\\widetilde{h}\/2}-1)\/(\\phi^{1\/2}-1)\\), and let \\(n=\\mathfrak{F}(m+p)^{1\/a}\\) where \\(p=\\operatorname{frc}\\big{(}\\mathfrak{F}^{-1}(n^{a})\\big{)}\\in[0,1)\\). Then, by Theorem 4.10, we have\n\n\\[\\operatorname{Prob}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{LB}_{6}(K_{k})= \\mathbf{b}\\,\\big{\\}}\\;=\\;\\begin{cases}\\frac{\\widetilde{r}+\\phi^{\\rho^{\\beta 2}}-1}{\\phi^{\\beta\/2}}-\\frac{r+\\phi^{\\rho^{\\beta 2}}-1}{\\phi^{\\beta\/2}}+o(1)&\\text{ if }p \\leq\\beta\\;,\\\\ \\frac{\\widetilde{r}+\\phi^{\\rho^{\\beta 2}}-1}{\\phi^{\\rho^{2}}}-\\frac{r+\\phi^{\\rho^{ \\beta 2}}-1}{\\phi^{\\rho^{2}}}+o(1)&\\text{ if }\\beta<p\\leq\\widetilde{\\beta}\\\\ \\frac{\\widetilde{r}+\\phi^{\\widetilde{h}\/2}-1}{\\phi^{\\rho^{2}}}-\\frac{r+\\phi^ {\\rho^{\\beta 2}}-1}{\\phi^{\\rho^{2}}}+o(1)&\\text{ if }p>\\widetilde{\\beta}\\;.\\end{cases}\\]\n\n\\[\\Rightarrow\\limsup_{n}\\operatorname{Prob}\\big{\\{}\\,k\\in\\Omega_{n}: \\operatorname{LB}_{6}(K_{k})=\\mathbf{b}\\,\\big{\\}}\\;=\\;\\frac{\\widetilde{r}+ \\phi^{\\widetilde{h}\/2}-1}{\\phi^{\\widetilde{h}\/2}}-\\frac{r+\\phi^{\\beta\/2}-1}{ \\phi^{\\widetilde{h}\/2}}\\approx 0.1737\\]\n\n\\[\\liminf_{n}\\operatorname{Prob}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{LB}_{6}(K _{k})=\\mathbf{b}\\,\\big{\\}}\\;=\\;\\frac{\\widetilde{r}+\\phi^{\\widetilde{h}\/2}-1} {\\phi^{\\beta\/2}}-\\frac{r+\\phi^{\\beta\/2}-1}{\\phi^{\\beta\/2}}\\approx 0.1419.\\]\n\n## 5 Other continuations\n\nReflecting upon Lemma 4.4 and Theorem 4.5, we realized that we could consider different continuations of the Fibonacci sequence \\(F\\), and ask which sequence satisfies the equidistribution property, and which distributions its leading blocks follow. Let us demonstrate the idea in Example 5.4. The claims in this example can be proved using Theorem 5.6. Recall the Benford continuation \\(\\mathfrak{F}\\) from Definition 4.1.\n\n**Definition 5.1**.: Given \\(n\\in\\mathbb{N}\\), let \\(\\mathfrak{F}_{n}:[0,1]\\to[0,1]\\) be the increasing function given by\n\n\\[\\mathfrak{F}_{n}(p)\\,:=\\,\\frac{\\mathfrak{F}(n+p)-\\mathfrak{F}(n)}{\\mathfrak{F }(n+1)-\\mathfrak{F}(n)}=\\frac{\\mathfrak{F}(n+p)-\\mathfrak{F}(n)}{F_{n-1}}\\;=\\; \\phi(\\phi^{p}-1)+o(1)\\]\n\nwhere \\(F_{0}:=1\\). Let \\(\\mathfrak{F}_{\\infty}:[0,1]\\to[0,1]\\) be the increasing function given by \\(\\mathfrak{F}_{\\infty}(p)=\\phi(\\phi^{p}-1)\\).\n\nRecall uniform continuations of sequences from Definition 1.6.\n\n**Lemma 5.2**.: _The function \\(\\mathfrak{F}\\) is a uniform continuation of \\(F\\)._\n\nProof.: Notice that \\(\\mathfrak{F}_{n}(p)=\\phi(\\phi^{p}-1)+\\gamma(n,p)\\) where \\(\\big{|}\\gamma(n,p)\\big{|}<C\/\\phi^{n}\\) where \\(C\\) is independent of \\(p\\) and \\(n\\). Thus, it uniformly converges to \\(\\phi(\\phi^{p}-1)\\). \n\n**Lemma 5.3**.: _Let \\(p\\in[0,1]\\) be a real number. Then, \\(\\mathfrak{F}(n+\\mathfrak{F}_{n}{}^{-1}(p))=F_{n}+(F_{n+1}-F_{n})p\\)._\n\nProof.: Let \\(p^{\\prime}=\\mathfrak{F}_{n}{}^{-1}(p)\\). Then, \\(\\mathfrak{F}_{n}(p^{\\prime})=p\\), and hence, \\(\\frac{\\mathfrak{F}(n+p^{\\prime})-\\mathfrak{F}(n)}{F_{n+1}-F_{n}}=p\\). The assertion follows from the last equality. \n\n**Example 5.4**.: Let \\(f:[1,\\infty)\\to\\mathbb{R}\\) be the increasing continuous function whose graph is the union of the line segments from \\((n,F_{n})\\) to \\((n+1,F_{n+1})\\) for \\(n\\in\\mathbb{N}\\). Then, \\(f_{\\infty}(p)=p\\) for all \\(p\\in[0,1]\\). Let \\(K\\) be the sequence given by \\(K_{n}=\\big{|}\\mathfrak{F}(n+\\mathfrak{F}_{n}{}^{-1}(\\operatorname{frc}(n\\pi))) \\big{|}\\). Then, by Lemma 5.3,\n\n\\[f^{-1}\\big{(}\\mathfrak{F}(n+\\mathfrak{F}_{n}{}^{-1}(\\operatorname{frc}(n\\pi))) \\big{)}=n+\\operatorname{frc}(n\\pi)\\;\\Rightarrow\\;\\operatorname{frc}\\big{(}f^{-1 }(K_{n})\\big{)}=\\operatorname{frc}(n\\pi)+o(1),\\]\nwhich is equidistributed.\n\nRecall \\(\\mathcal{F}_{s}\\) from Definition 3.7 where \\(s\\geq 2\\), and let \\(\\mathbf{b}\\in\\mathcal{F}_{s}\\). Recall \\(\\widehat{F}\\) from Definition 3.1 and the product notation from Definition 2.2. Then, by Theorem 5.6,\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{s}(K_{n})=\\mathbf{b}\\,\\right\\} \\;=\\;\\phi(\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}-\\mathbf{b}\\cdot\\widehat{F}) \\;=\\;\\phi^{-s+2}(\\widetilde{\\mathbf{b}}\\ast\\overline{F}-\\mathbf{b}\\ast \\overline{F})\\]\n\nwhere \\(\\overline{F}\\) is the sequence given by \\(\\overline{F}_{n}=\\phi^{n-1}\\). If \\(\\mathbf{b}(s)=0\\), then \\(\\omega^{s-2}(\\widetilde{\\mathbf{b}}\\ast\\overline{F}-\\mathbf{b}\\ast\\overline{ F})=\\omega^{s-2}\\), and if \\(\\mathbf{b}(s)=1\\), then \\(\\omega^{s-2}(\\widetilde{\\mathbf{b}}\\ast\\overline{F}-\\mathbf{b}\\ast\\overline{ F})=\\omega^{s-1}\\). For example, if \\(s=6\\), then\n\n\\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{6}(K_{n})\\;=\\;(1,0,0,1,0)\\right\\}\\;=\\;\\omega^{5}\\] \\[\\mathrm{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\mathrm{LB}_{6}(K_{n})\\;=\\;( 1,0,1,0,1,0)\\right\\}\\;=\\;\\omega^{4}.\\]\n\nIt's nearly a uniform distribution.\n\nLet us show that the probabilities add up to \\(1\\). Notice that \\(\\#\\mathcal{F}_{s}=F_{s-1}\\), \\(\\#\\{\\mathbf{b}\\in\\mathcal{F}_{s}:\\mathbf{b}(s)=0\\}=F_{s-2}\\), and and \\(\\#\\{\\mathbf{b}\\in\\mathcal{F}_{s}:\\mathbf{b}(s)=1\\}=F_{s-3}\\). Then, by Binet's Formula, the following sum is equal to \\(1\\):\n\n\\[\\sum_{\\mathbf{b}\\in\\mathcal{F}_{s}}\\omega^{s-2}(\\widetilde{\\mathbf{b}}\\ast \\overline{F}-\\mathbf{b}\\ast\\overline{F})\\;=\\;\\frac{F_{s-2}}{\\phi^{s-2}}+\\frac{ F_{s-3}}{\\phi^{s-1}}=1.\\]\n\nBy Lemma 5.3, we have \\(K_{n}=\\left\\{F_{n}+(F_{n+1}-F_{n})\\mathrm{frc}(n\\pi)\\right\\}\\) for \\(n\\in\\mathbb{N}\\), and the following are the first ten values of \\(K_{n}\\):\n\n\\[(1,2,3,6,11,19,33,36,64,111).\\]\n\nLet us introduce and prove the main results on continuations.\n\n**Lemma 5.5**.: _Let \\(f\\) be a uniform continuation of \\(F\\), and let \\(K\\) be a sequence of positive real numbers approaching \\(\\infty\\). Then, \\(\\mathrm{frc}\\left(f^{-1}(\\left\\lfloor K_{n}\\right\\rfloor)\\right)=\\mathrm{frc} \\left(f^{-1}(K_{n})\\right)+o(1)\\)._\n\nProof.: Let \\(n\\in\\mathbb{N}\\). Then, \\(F_{m}\\leq\\left\\lfloor K_{n}\\right\\rfloor\\leq K_{n}<F_{m+1}\\) for \\(m\\in\\mathbb{N}\\) depending on \\(n\\). Let \\(K_{n}=f(m+p)\\) and \\(\\left\\lfloor K_{n}\\right\\rfloor=f(m+p^{\\prime})\\) where \\(p,p^{\\prime}\\in[0,1]\\) are real numbers, which depend on \\(n\\). Then, \\(F_{m}+f_{m}(p^{\\prime})(F_{m+1}-F_{m})+O(1)=F_{m}+f_{m}(p)(F_{m+1}-F_{m})\\), and hence, \\(f_{m}(p^{\\prime})+o(1)=f_{m}(p)\\). Thus,\n\n\\[f^{-1}(K_{n})\\;=\\;m+p=m+{f_{m}}^{-1}\\left(f_{m}(p^{\\prime})+o(1)\\right)\\;=\\;m +{f_{m}}^{-1}\\left(f_{\\infty}(p^{\\prime})+o(1)\\right).\\]\n\nBy the uniform convergence,\n\n\\[=\\;m+{f_{\\infty}}^{-1}\\left(f_{\\infty}(p^{\\prime})+o(1)\\right)+o(1)\\;=\\;m+{f_{ \\infty}}^{-1}\\left(f_{\\infty}(p^{\\prime})\\right)+o(1)\\;=\\;m+p^{\\prime}+o(1).\\]\n\nTherefore, \\(\\mathrm{frc}\\left(f^{-1}(K_{n})\\right)=\\mathrm{frc}\\left(f^{-1}(\\left\\lfloor K _{n}\\right\\rfloor)\\right)+o(1)\\).\n\n**Theorem 5.6**.: _Let \\(f:[1,\\infty)\\rightarrow\\mathbb{R}\\) be a uniform continuation of \\(F\\). Then there is a sequence \\(K\\) of positive integers approaching \\(\\infty\\), e.g., \\(K_{n}=\\left\\lfloor\\widehat{\\mathfrak{F}}\\left(n+\\widehat{\\mathfrak{F}}_{n}^{-1 }\\circ f_{n}(\\mathrm{frc}(n\\pi)\\right)\\right\\rfloor\\), such that \\(\\mathrm{frc}\\left(f^{-1}(K_{n})\\right)\\) is equidistributed._\n_Let \\(K\\) be a sequence of of positive integers approaching \\(\\infty\\) such that \\(\\operatorname{frc}\\bigl{(}f^{-1}(K_{n})\\bigr{)}\\) is equidistributed. Let \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\) where \\(s\\geq 2\\). Then,_\n\n\\[\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s} (K_{n})=\\mathbf{b}\\,\\big{\\}} =\\,{f_{\\infty}}^{-1}\\circ\\mathfrak{F}_{\\infty}(\\log_{\\phi} \\widetilde{\\mathbf{b}}\\cdot\\widehat{F})-{f_{\\infty}}^{-1}\\circ\\mathfrak{F}_{ \\infty}(\\log_{\\phi}\\mathbf{b}\\cdot\\widehat{F})\\] \\[=\\,{f_{\\infty}}^{-1}\\bigl{(}\\phi(\\widetilde{\\mathbf{b}}\\cdot \\widehat{F}-1)\\bigr{)}-{f_{\\infty}}^{-1}\\bigl{(}\\phi(\\mathbf{b}\\cdot\\widehat{F} -1)\\bigr{)}\\,. \\tag{4}\\]\n\nProof.: Let \\(x\\geq 1\\) be a real number, and let \\(F_{n}\\leq x<F_{n+1}\\) for \\(n\\in\\mathbb{N}\\). Since \\(\\mathfrak{F}\\) and \\(f\\) are increasing continuations of \\(F\\), there are two unique real numbers \\(p\\) and \\(p^{\\prime}\\) in \\([0,1]\\) such that \\(x=\\mathfrak{F}(n+p)=f(n+p^{\\prime})\\). We claim that\n\n\\[f^{-1}(x)=n+{f_{n}}^{-1}(\\mathfrak{F}_{n}(p)), \\tag{5}\\]\n\nand \\(\\mathfrak{F}^{-1}(x)=n+\\mathfrak{F}_{n}^{-1}(f_{n}(p^{\\prime}))\\). To prove the claim, note\n\n\\[\\mathfrak{F}(n+p)=f(n+p^{\\prime}) \\,\\Rightarrow\\,{F_{n}}+\\mathfrak{F}_{n}(p)({F_{n+1}}-{F_{n}}) \\,=\\,{F_{n}}+{f_{n}}(p^{\\prime})({F_{n+1}}-{F_{n}})\\] \\[\\,\\Rightarrow\\,p^{\\prime}={f_{n}}^{-1}(\\mathfrak{F}_{n}(p)),\\,p= \\mathfrak{F}_{n}^{-1}(f_{n}(p^{\\prime})).\\]\n\nThen \\(f(n+p^{\\prime})=x\\) and \\(\\mathfrak{F}(n+p)=x\\) imply the claim.\n\nLet \\(\\overline{K}\\) and \\(K\\) be the sequences given by \\(\\overline{K}_{n}=\\mathfrak{F}\\bigl{(}n+\\mathfrak{F}_{n}^{-1}\\circ f_{n}( \\operatorname{frc}(n\\pi))\\bigr{)}\\) and \\(K_{n}=\\left\\lfloor\\overline{K}_{n}\\right\\rfloor\\). Given \\(n\\in\\mathbb{N}\\), let \\(p_{n}=\\mathfrak{F}_{n}^{-1}\\circ f_{n}(\\operatorname{frc}(n\\pi))\\). Then,\n\n\\[f^{-1}(\\overline{K}_{n})\\,=\\,n+{f_{n}}^{-1}\\bigl{(}\\mathfrak{F}_{n}(p_{n}) \\bigr{)}\\,=\\,n+\\operatorname{frc}(n\\pi)\\,.\\]\n\nThus, \\(\\operatorname{frc}\\Bigl{(}f^{-1}(\\overline{K}_{n})\\Bigr{)}\\) is equidistributed. If we further assume that \\(f\\) is a uniform continuation, then, by Lemmas 4.6 and 5.5, \\(\\operatorname{frc}\\Bigl{(}f^{-1}(\\left\\lfloor\\overline{K}_{n}\\right\\rfloor) \\Bigr{)}=\\operatorname{frc}\\bigl{(}f^{-1}(K_{n})\\bigr{)}\\) is equidistributed as well.\n\nLet \\(K\\) be a sequence of of positive integers approaching \\(\\infty\\) such that \\(\\operatorname{frc}\\bigl{(}f^{-1}(K_{n})\\bigr{)}\\) is equidistributed. Let \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\), and let \\(A_{\\mathbf{b}}:=\\{n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})=\\mathbf{b}\\}\\). Let \\(n\\in A_{\\mathbf{b}}\\), and \\(F_{m}\\leq K_{n}<F_{m+1}\\) for \\(m\\in\\mathbb{N}\\) depending on \\(n\\). Let \\(K_{n}=\\mathfrak{F}(m+p)=f(m+p^{\\prime})\\) where \\(p\\) and \\(p^{\\prime}\\) are real numbers in \\([0,1]\\) depending on \\(n\\).\n\nThen, by Lemma 4.4,\n\n\\[\\log_{\\phi}\\mathbf{b}\\cdot\\widehat{F}+o(1)\\,\\,<\\,\\operatorname{ frc}\\bigl{(}\\mathfrak{F}^{-1}(K_{n})\\bigr{)}\\,<\\,\\log_{\\phi}\\widetilde{\\mathbf{b}} \\cdot\\widehat{F}+o(1)\\] \\[\\Rightarrow \\log_{\\phi}\\mathbf{b}\\cdot\\widehat{F}+o(1)\\,<\\,\\operatorname{ frc}\\bigl{(}m+\\mathfrak{F}_{n}^{-1}(f_{n}(p^{\\prime}))\\bigr{)}\\,<\\,\\log_{\\phi} \\widetilde{\\mathbf{b}}\\cdot\\widehat{F}+o(1)\\] \\[\\Rightarrow {f_{n}}^{-1}\\circ\\mathfrak{F}_{n}(\\log_{\\phi}\\mathbf{b}\\cdot \\widehat{F}+o(1))\\,<\\,p^{\\prime}\\,<\\,{f_{n}}^{-1}\\circ\\mathfrak{F}_{n}(\\log_{ \\phi}\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}+o(1))\\] \\[\\Rightarrow {f_{\\infty}}^{-1}\\circ\\mathfrak{F}_{\\infty}(\\log_{\\phi}\\mathbf{b} \\cdot\\widehat{F})+o(1)\\,<\\,{\\operatorname{frc}\\bigl{(}f^{-1}(K_{n})\\bigr{)}} \\,<\\,{f_{\\infty}}^{-1}\\circ\\mathfrak{F}_{\\infty}(\\log_{\\phi}\\widetilde{ \\mathbf{b}}\\cdot\\widehat{F})+o(1).\\]\n\nSince \\(\\operatorname{frc}\\bigl{(}f^{-1}(K_{n})\\bigr{)}\\) is equidistributed, the above inequalities imply the assertion (4). \n\nLet us demonstrate a continuation, for which the distribution of leading blocks of length \\(4\\) coincides with that of strong Benford's Law, but the distribution does not coincide for higher length blocks.\n**Example 5.7**.: Consider \\(\\mathscr{F}_{4}=\\{\\mathbf{b}_{1},\\mathbf{b}_{2},\\mathbf{b}_{3}\\}\\), i.e.,\n\n\\[\\mathbf{b}_{1}=(1,0,0,0),\\ \\mathbf{b}_{2}=(1,0,0,1),\\ \\mathbf{b}_{3}=(1,0,1,0).\\]\n\nLet \\(p_{k}=\\log_{\\phi}(\\mathbf{b}_{k}\\cdot\\widehat{F})<1\\) for \\(k=1,2,3\\), and let \\(p_{0}=0\\) and \\(p_{4}=1\\). For each \\(n\\in\\mathbb{N}\\), define \\(f_{n}:[0,1]\\to[0,1]\\) to be the function whose graph is the union of line segments from \\((p_{k},\\widehat{\\mathfrak{s}}_{\\infty}(p_{k}))\\) to \\((p_{k+1},\\widehat{\\mathfrak{s}}_{\\infty}(p_{k+1}))\\) for \\(k=0,1,2,3\\). Notice that \\(f_{n}\\) is defined independently of \\(n\\), and that it defines a uniform continuation \\(f:[1,\\infty)\\to[1,\\infty)\\) such that \\(f_{\\infty}=f_{n}\\) for all \\(n\\in\\mathbb{N}\\) as follows: Given \\(x\\in[1,\\infty)\\), find \\(n\\in\\mathbb{N}\\) such that \\(n\\leq x<n+1\\), and define \\(f(x)=F_{n}+f_{n}(x-n)(F_{n+1}-F_{n})\\).\n\nNote that \\(f_{\\infty}(p_{k})=\\widehat{\\mathfrak{s}}_{\\infty}(p_{k})\\), i.e., \\(f_{\\infty}{}^{-1}(\\widehat{\\mathfrak{s}}_{\\infty}(p_{k}))=p_{k}\\) for \\(k=0,1,2,3\\). By Theorem 5.6, if \\(\\operatorname{\\mathrm{missing}}\\left(f^{-1}(K_{n})\\right)\\) is equidistributed, we have\n\n\\[\\operatorname{\\mathrm{missing}}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{4}( K_{n})=\\mathbf{b}_{k}\\,\\right\\}\\ =\\ p_{k+1}-p_{k}\\ =\\ \\log_{\\phi}\\frac{\\widetilde{\\mathbf{b}}_{k}\\cdot\\widehat{F}}{\\mathbf{b}_{k} \\cdot\\widehat{F}}\\]\n\nwhere \\(\\widetilde{\\mathbf{b}}_{3}=(1,0,1,1)\\) as defined Definition 3.7. However, the leading blocks of length \\(>4\\) do not satisfy Benford's Law under \\(\\mathscr{F}\\)-expansion.\n\nThe following is an example where \\(f_{\\infty}\\) is analytic.\n\n**Example 5.8**.: Let \\(f:[1,\\infty)\\to\\mathbb{R}\\) be the function given by \\(f(n+p)=F_{n}+(F_{n+1}-F_{n})p^{2}\\) where \\(n\\in\\mathbb{N}\\) and \\(p\\in[0,1)\\). Then, \\(f_{\\infty}(p)=p^{2}\\).\n\nLet \\(K\\) be the sequence given by \\(K_{n}=\\left\\lfloor\\widehat{\\mathfrak{s}}(n+\\widehat{\\mathfrak{s}}_{n}{}^{-1} (p^{2}))\\right\\rfloor\\), and let \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\). Then, Theorem 5.6,\n\n\\[\\operatorname{\\mathrm{missing}}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_ {n})=\\mathbf{b}\\,\\right\\}=\\sqrt{\\phi(\\widetilde{\\mathbf{b}}\\cdot\\widehat{F}- 1)}-\\sqrt{\\phi(\\mathbf{b}\\cdot\\widehat{F}-1)}.\\]\n\n### Converse\n\nLet's consider the converse of Theorem 5.6, i.e., given a sequence \\(K\\) of positive integers approaching \\(\\infty\\), let us construct a uniform continuation \\(f\\), if possible, such that \\(\\operatorname{\\mathrm{missing}}\\left(f^{-1}(K_{n})\\right)\\) is equidistributed. Recall the set \\(\\mathscr{F}_{s}\\) from Definition 3.7.\n\n**Definition 5.9**.: A sequence \\(K\\) of positive integers approaching \\(\\infty\\) is said to have _strong leading block distribution under \\(\\mathscr{F}\\)-expansion_ if \\(\\operatorname{\\mathrm{missing}}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_ {n})=\\mathbf{b}\\,\\right\\}\\) exists for each integer \\(s\\geq 2\\) and each \\(\\mathbf{b}\\in\\mathscr{F}_{s}\\).\n\n**Example 5.10**.: Let \\(K\\) be the Lucas sequence, i.e., \\(K=(2,1,3,4,\\ldots)\\) and \\(K_{n+2}=K_{n+1}+K_{n}\\). Recall that \\(F_{n}=\\frac{1}{10}(5+\\sqrt{5})\\phi^{n}(1+o(1))\\) and \\(K_{n}=\\frac{1}{2}(\\sqrt{5}-1)\\phi^{n}(1+o(1))\\), and let \\(\\alpha=\\frac{1}{10}(5+\\sqrt{5})\\) and \\(a=\\frac{1}{2}(\\sqrt{5}-1)\\). Then, by Lemma 4.2,\n\n\\[\\operatorname{\\mathrm{missing}}\\left(\\widehat{\\mathfrak{s}}^{-1}(K_{n})\\right) =-\\log_{\\phi}(a\/\\alpha)+o(1)\\approx.328+o(1).\\]\n\nBy Lemma 4.4, the leading block of \\(K_{n}\\) being \\(\\mathbf{b}_{1}=(1,0,0)\\) is determined by whether \\(0\\leq\\operatorname{\\mathrm{missing}}\\left(\\widehat{\\mathfrak{s}}^{-1}(K_{n}) \\right)<\\log_{\\phi}(1+\\omega^{2})\\approx.67\\). Thus, \\(\\operatorname{\\mathrm{missing}}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{3}(K_ {n})=\\mathbf{b}_{1}\\,\\right\\}=1\\), and \\(\\operatorname{\\mathrm{missing}}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{3}(K_ {n})=\\mathbf{b}_{2}\\,\\right\\}=0\\).\nIn fact, the sequence \\(K\\) has strong leading block distribution. Recall \\(\\widehat{F}\\) from Definition 3.1, and let us claim that \\(\\mathbf{b}\\cdot\\widehat{F}\\neq\\frac{\\alpha}{a}=\\frac{1}{10}(5+3\\sqrt{5})\\) for all \\(s\\in\\mathbb{N}\\) and \\(\\mathbf{b}\\in\\mathcal{F}_{s}\\). Notice that\n\n\\[\\frac{\\alpha}{\\alpha}-1=\\sum_{k=1}^{\\infty}\\omega^{4k}. \\tag{6}\\]\n\nThe equality (6) is called _the Zeckendorf expansion of a real number in \\((0,1)\\)_ since it is a power series expansion in \\(\\omega\\) where no consecutive powers are used; a formal definition is given in Definition 5.11 below. By the uniqueness of Zeckendorf expansions of the real numbers in \\((0,1)\\), the above infinite sum in (6) is not equal to any finite sum \\(\\mathbf{b}\\cdot\\widehat{F}-1\\) where \\(\\mathbf{b}\\in\\mathcal{F}_{s}\\); see Theorem 5.13.\n\nLet \\(s\\) be an integer \\(\\geq 2\\), and let \\(\\mathcal{F}_{s}=\\{\\mathbf{b}_{1},\\ldots,\\mathbf{b}_{\\ell}\\}\\). Then, there is \\(k\\in\\mathbb{N}\\) such that \\(\\mathbf{b}_{k}\\cdot\\widehat{F}\\;<\\;\\frac{\\alpha}{a}\\;<\\;\\mathbf{b}_{k+1}\\cdot \\widehat{F}.\\) This implies that\n\n\\[\\log_{\\phi}(\\mathbf{b}_{k}\\cdot\\widehat{F})\\;<\\;\\log_{\\phi}(\\tfrac{\\alpha}{a} )\\;<\\;\\log_{\\phi}(\\mathbf{b}_{k+1}\\cdot\\widehat{F}).\\]\n\nSince \\(\\operatorname{\\mathrm{frc}}\\big{(}\\widehat{y}^{-1}(K_{n})\\big{)}=\\log_{\\phi}( \\alpha\/a)+o(1)\\) for all \\(n\\in\\mathbb{N}\\), by Lemma 4.4, we have \\(\\operatorname{\\mathrm{Prob}}\\big{\\{}\\;n\\in\\mathbb{N}\\colon\\operatorname{LB}_{ s}(K_{n})=\\mathbf{b}_{k}\\;\\big{\\}}=1\\). For example, consider the case of \\(s=9\\), and notice that \\(\\omega^{4}+\\omega^{8}<\\frac{\\alpha}{a}-1<\\omega^{4}+\\omega^{7}\\) by (6). Then, we have \\(\\mathbf{b}\\cdot\\widehat{F}<\\frac{\\alpha}{a}<\\widetilde{\\mathbf{b}}\\cdot \\widehat{F}\\) where\n\n\\[\\mathbf{b}=(1,0,0,0,1,0,0,0,1)\\;\\;\\text{and}\\;\\;\\widetilde{\\mathbf{b}}=(1,0,0,0,1,0,0,1,0),\\]\n\nand the probability of having the leading block \\(\\mathbf{b}\\) in the values of the Lucas sequence is \\(1\\).\n\nRecall uniform continuations from Definition 1.6. Since the distribution of the leading blocks of the Lucas sequence \\(K\\) is concentrated on one particular block in \\(\\mathcal{F}_{s}\\) for each \\(s\\), there does not exist a uniform continuation \\(f\\), described in Theorem 5.6, whose equidistribution is associated with the leading block distributions of the Lucas sequence \\(K\\). For a uniform continuation to exist, the values of the leading block distributions must be put together into a continuous function, and below we formulate the requirement more precisely.\n\n**Definition 5.11**.: Let \\(\\mathbf{I}\\) denote the interval \\((0,1)\\) of real numbers. An infinite tuple \\(\\mu\\in\\prod_{k=1}^{\\infty}\\mathbb{N}_{0}\\) is called a _Zeckendorf expression for \\(\\mathbf{I}\\)_ if \\(\\mu(k)\\leq 1\\), \\(\\mu(k)\\mu(k+1)=0\\), and for all \\(j\\in\\mathbb{N}_{0}\\), the sequence \\(\\{\\mu(j+n)\\}_{n=1}^{\\infty}\\) is not equal to the sequence \\(\\{1+(-1)^{n+1}\\}\/2\\}_{n=1}^{\\infty}=(1,0,1,0,\\ldots)\\). Let \\(\\mathcal{F}^{*}\\) be the set of Zeckendorf expressions for \\(\\mathbf{I}\\).\n\nGiven \\(s\\in\\mathbb{N}\\) and \\(\\mu\\in\\mathcal{F}^{*}\\), let \\(\\mu|s:=(\\mu(1),\\ldots,\\mu(s))\\). Given \\(s\\in\\mathbb{N}\\) and \\(\\{\\mu,\\tau\\}\\subset\\mathcal{F}^{*}\\), we declare \\(\\mu|s<\\tau|s\\) if \\(\\mu|s\\cdot\\widehat{F}<\\tau|s\\cdot\\widehat{F}\\), which coincides with the lexicographical order on \\(\\mathcal{F}\\).\n\n**Notation 5.12**.: Given a sequence \\(Q\\) of real numbers, and \\(\\mu\\in\\prod_{k=1}^{\\infty}\\mathbb{N}_{0}\\), we define \\(\\mu\\cdot Q:=\\sum_{k=1}^{\\infty}\\mu(k)Q_{k}\\), which may or may not be a convergent series.\n\n**Theorem 5.13** ([10], Zeckendorf Theorem for \\(\\mathbf{I}\\)).: _Given a real number \\(\\beta\\in\\mathbf{I}\\), there is a unique \\(\\mu\\in\\mathcal{F}^{*}\\) such that \\(\\beta=\\sum_{k=1}^{\\infty}\\mu(k)\\omega^{k}=(\\mu\\cdot\\widehat{F})\\omega\\)._\nFor the uniqueness of \\(\\mu\\) in the theorem, we require the infinite tuples such as \\((0,1,0,1,0,\\ldots)\\) to be not a member of \\(\\mathcal{F}^{*}\\) since \\(\\sum_{k=1}^{\\infty}\\omega^{2k}=\\omega\\), which is analogous to \\(0.0999\\ldots=0.1\\) in decimal expansion.\n\n**Proposition 5.14** ([10]).: _Let \\(\\{\\mu,\\tau\\}\\subset\\mathcal{F}^{*}\\). Then, \\(\\mu\\cdot\\widehat{F}<\\tau\\cdot\\widehat{F}\\) if and only if \\(\\mu|s<\\tau|s\\) for some \\(s\\in\\mathbb{N}\\)_\n\nGiven a sequence with strong leading block distribution, we shall construct a function on \\(\\mathbf{I}\\) in Definition 5.16 below, and it is well-defined by Lemma 5.15.\n\n**Lemma 5.15**.: _Given a real number \\(\\beta\\in\\mathbf{I}\\), there is a unique \\(\\mu\\in\\mathcal{F}^{*}\\) such that \\(\\mu(1)=1\\) and \\(\\phi(\\mu\\cdot\\widehat{F}-1)=\\beta\\)._\n\nProof.: Let \\(\\widehat{F}^{*}\\) be the sequence defined by \\(\\widehat{F}^{*}_{n}=\\omega^{n}\\). Given a real number \\(\\beta\\in\\mathbf{I}\\), we have \\(0<\\omega+\\beta\\omega^{2}<1\\). By Theorem 5.13, there are is \\(\\mu\\in\\mathcal{F}^{*}\\) such that \\((\\mu\\cdot\\widehat{F})\\omega=\\mu\\cdot\\widehat{F}^{*}=\\omega+\\beta\\omega^{2}\\), which implies \\(\\phi(\\mu\\cdot\\widehat{F}-1)=\\beta\\). We claim that \\(\\mu(1)=1\\). If \\(\\mu(1)=0\\), then by Proposition 5.14, \\(\\omega+\\beta\\omega^{2}=\\mu\\cdot\\widehat{F}^{*}=(0,\\ldots)\\cdot\\widehat{F}^{*}< \\omega=(1,0,0,\\ldots)\\cdot\\widehat{F}^{*}\\), which implies a false statement \\(\\beta\\omega^{2}<0\\). Thus, \\(\\mu(1)=1\\). \n\nRecall from Definition 5.11 the definition of inequalities on tuples.\n\n**Definition 5.16**.: Let \\(K\\) be a sequence of positive integers with strong leading block distribution under \\(\\mathcal{F}\\)-expansion such that given \\(\\mu\\in\\mathcal{F}^{*}\\) and an integer \\(s\\geq 2\\) such that \\(\\mu(1)=1\\), the following limit exists:\n\n\\[\\lim_{s\\to\\infty}\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{ s}(K_{n})\\leq\\mu|s\\,\\big{\\}} \\tag{7}\\]\n\nwhere \\(\\mu|s\\) is identified in \\(\\mathcal{F}_{s}\\).\n\nLet \\(f^{*}_{K}:[0,1]\\to[0,1]\\) be the function given by \\(f^{*}_{K}(0)=0\\), \\(f^{*}_{K}(1)=1\\), and \\(f^{*}_{K}(\\phi(\\mu\\cdot\\widehat{F}-1))\\) is equal to the value in (7). If \\(f^{*}_{K}\\) is continuous and increasing, then \\(K\\) is said to _have continuous leading block distribution under \\(\\mathcal{F}\\)-expansion_.\n\n**Lemma 5.17**.: _Let \\(K\\) be a sequence with continuous leading block distribution under \\(\\mathcal{F}\\)-expansion, and let \\(f^{*}_{K}\\) be the function defined in Definition 5.16. Let \\(\\mu\\in\\mathcal{F}^{*}\\) such that there is \\(t\\in\\mathbb{N}\\) such that \\(\\mu(1)=1\\) and \\(\\mu(k)=0\\) for all \\(k>t\\). Then, \\(f^{*}_{K}(\\phi(\\mu|t\\cdot\\widehat{F}-1))\\ \\leq\\ \\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\, \\mathrm{LB}_{t}(K_{n})\\leq\\mu|t\\,\\big{\\}}\\)._\n\nProof.: Notice that if \\(s>t\\), then\n\n\\[\\{n\\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{s}(K_{n})\\leq\\mu|s\\}\\subset\\{n \\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{t}(K_{n})\\leq\\mu|t\\}\\] \\[\\Rightarrow \\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{s}(K _{n})\\leq\\mu|s\\,\\big{\\}}\\ \\leq\\ \\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{t}(K_{n})\\leq\\mu|t \\,\\big{\\}}\\] \\[\\lim_{s\\to\\infty}\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,: \\,\\mathrm{LB}_{s}(K_{n})\\leq\\mu|s\\,\\big{\\}}=f^{*}_{K}(\\phi(\\mu\\cdot\\widehat{F} -1))\\ \\leq\\ \\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\,\\mathrm{LB}_{t}(K_{n})\\leq\\mu|t \\,\\big{\\}}\\]\n\nSince \\(\\mu|t\\cdot\\widehat{F}=\\mu\\cdot\\widehat{F}\\),\n\n\\[\\Rightarrow f^{*}_{K}(\\phi(\\mu|t\\cdot\\widehat{F}-1))\\ \\leq\\ \\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}\\,:\\, \\mathrm{LB}_{t}(K_{n})\\leq\\mu|t\\,\\big{\\}}\\,.\\]\nRecall uniform continuations from Definition 1.6.\n\n**Theorem 5.18**.: _Let \\(K\\) be a sequence with continuous leading block distribution under \\(\\mathscr{F}\\)-expansion. Let \\(f_{K}^{*}\\) be the function defined in Definition 5.16. Then, there is a uniform continuation \\(f\\) of \\(F\\) such that \\({f_{\\infty}}^{-1}=f_{K}^{*}\\) and \\(\\operatorname{\\mathrm{frc}}\\big{(}f^{-1}(K_{n})\\big{)}\\) is equidistributed._\n\nProof.: Let \\(f:[1,\\infty)\\to\\mathbb{R}\\) be the function given by \\(f(x)=F_{n}+(F_{n+1}-F_{n})(f_{K}^{*})^{-1}(p)\\) where \\(x=n+p\\) and \\(p=\\operatorname{\\mathrm{frc}}(x)\\). Then, \\(f\\) is a uniform continuation of \\(F_{n}\\) since \\((f_{K}^{*})^{-1}\\) is independent of \\(n\\). Then, \\({f_{\\infty}}=(f_{K}^{*})^{-1}\\), i.e., \\({f_{\\infty}}^{-1}=f_{K}^{*}\\).\n\nLet \\(\\beta\\in(0,1)\\) be a real number, and below we show that \\(\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{\\mathrm{ frc}}\\big{(}f^{-1}(K_{n})\\big{)}\\leq\\beta\\,\\big{\\}}\\) exists, and it is equal to \\(\\beta\\). Recall \\(\\mathfrak{F}\\) from Definition 4.1 and \\(\\mathfrak{F}_{n}\\) from Definition 5.1. Let \\(n\\in\\mathbb{N}\\), and let \\(m\\in\\mathbb{N}\\) such that \\(F_{m}\\leq K_{n}<F_{m+1}\\). Then, \\(K_{n}=f(m+p_{n}^{\\prime})=\\mathfrak{F}(m+p_{n})\\) where \\(p_{n},p_{n}^{\\prime}\\in[0,1]\\), i.e., \\(f_{\\infty}(p_{n}^{\\prime})=\\mathfrak{F}_{m}(p_{n})\\). By Theorem 5.13 and Lemma 5.15, there is a unique \\(\\mu\\in\\mathscr{F}^{*}\\) such that \\(f_{\\infty}(\\beta)=\\phi(\\mu\\cdot\\widehat{F}-1)\\) and \\(\\mu(1)=1\\). Recall \\(\\mathfrak{F}_{\\infty}\\) from Definition 5.1. Notice that\n\n\\[\\operatorname{\\mathrm{frc}}\\big{(}f^{-1}(K_{n})\\big{)}\\,=\\,p_{n}^ {\\prime}\\,\\leq\\,\\beta\\,{\\Rightarrow}\\,{f_{\\infty}}^{-1}(\\mathfrak{F}_{m}(p_{n }))\\,\\leq\\,\\beta\\,\\Rightarrow\\,p_{n}\\,{\\leq}\\,{\\mathfrak{F}_{m}}^{-1}(f_{ \\infty}(\\beta))\\] \\[\\Rightarrow \\operatorname{\\mathrm{frc}}\\big{(}\\mathfrak{F}^{-1}(K_{n})\\big{)} \\,\\leq\\,{\\mathfrak{F}_{m}}^{-1}(f_{\\infty}(\\beta))\\,=\\,{\\mathfrak{F}_{ \\infty}}^{-1}(f_{\\infty}(\\beta))+o(1)\\,=\\,{\\log_{\\phi}(\\mu\\cdot\\widehat{F})}+ o(1).\\]\n\nFix an integer \\(t\\geq 2\\). By Proposition 5.14, we have \\(\\mu\\cdot\\widehat{F}=\\mu|t\\cdot\\widehat{F}+\\gamma_{t}<\\mu|t\\cdot\\widehat{F}\\) where \\(\\gamma_{t}\\geq 0\\) and \\(\\widetilde{\\mu|t}\\in\\mathscr{F}_{t}\\) is as defined Definition 3.7. Since \\({\\log_{\\phi}(\\widetilde{\\mu|t}\\cdot\\widehat{F})}-{\\log_{\\phi}(\\mu\\cdot\\widehat {F})}>0\\), there is \\(M_{t}\\in\\mathbb{N}\\) such that for all \\(n\\geq M_{t}\\),\n\n\\[\\Rightarrow \\operatorname{\\mathrm{frc}}\\big{(}\\mathfrak{F}^{-1}(K_{n}))\\big{)}\\, \\leq\\,{\\log_{\\phi}(\\mu\\cdot\\widehat{F})}+o(1)\\,<\\,{\\log_{\\phi}(\\widetilde{\\mu| t}\\cdot\\widehat{F})}.\\]\n\nBy Lemma 4.4, this implies \\(\\operatorname{\\mathrm{LB}}_{t}(K_{n})\\leq\\mu|t\\). Recall \\(\\Omega_{n}=\\{k\\in\\mathbb{N}:k\\leq n\\}\\);\n\n\\[\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{ \\mathrm{frc}}\\big{(}f^{-1}(K_{k})\\big{)}\\leq\\beta\\,\\big{\\}}+o(1)\\,\\leq\\, \\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{\\mathrm{ LB}}_{t}(K_{k})\\leq\\mu|t\\,\\big{\\}}+o(1)\\] \\[\\Rightarrow \\,\\limsup_{n}\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}: \\operatorname{\\mathrm{frc}}\\big{(}f^{-1}(K_{k})\\big{)}\\leq\\beta\\,\\big{\\}}\\,\\leq \\,\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{\\mathrm{ LB}}_{t}(K_{n})\\leq\\mu|t\\,\\big{\\}}.\\]\n\nLet us work on the \\(\\liminf\\) of the probability. Since \\(\\beta\\neq 0\\), there is \\(t_{0}>1\\) such that \\(\\mu(t_{0})>0\\). Thus, if \\(t>t_{0}\\) is sufficiently large, then there are at least two entries \\(1\\) in \\(\\mu|t\\), and \\(\\mu|t\\) has more entries after the second entry of \\(1\\) from the left. Recall the product \\(*\\) from Definition 2.2. This choice of \\(t\\) allows us to have the unique coefficient functions \\(\\tilde{\\mu}\\) and \\(\\widehat{\\mu}\\) in \\(\\mathscr{F}_{t}\\) such that \\(1+\\tilde{\\mu}*F=\\widehat{\\mu}*F\\) and \\(1+\\tilde{\\mu}*F=\\mu|t*F\\). Then, by Lemma 4.4,\n\n\\[\\operatorname{\\mathrm{LB}}_{t}(K_{n})\\,{\\leq}\\,\\tilde{\\mu}\\, \\Rightarrow\\,\\operatorname{\\mathrm{frc}}\\big{(}\\mathfrak{F}^{-1}(K_{n})\\big{)} \\,<\\,{\\log_{\\phi}(\\widehat{\\mu}\\cdot\\widehat{F})}+o(1)\\] \\[\\Rightarrow \\,p_{n}\\,<\\,{\\mathfrak{F}_{m}}^{-1}(\\phi(\\widehat{\\mu}\\cdot \\widehat{F}-1))+o(1)\\] \\[\\Rightarrow \\,{\\mathfrak{F}_{m}}(p_{n})\\,=\\,f_{\\infty}(p_{n}^{\\prime})\\,<\\, {\\phi}(\\widehat{\\mu}\\cdot\\widehat{F}-1)+o(1)\\] \\[\\Rightarrow \\,{p_{n}^{\\prime}}\\,=\\,\\operatorname{\\mathrm{frc}}\\big{(}f^{-1}(K _{n})\\big{)}\\,<\\,{f_{\\infty}}^{-1}(\\phi(\\widehat{\\mu}\\cdot\\widehat{F}-1))+o(1)\\] \\[\\,<\\,{f_{\\infty}}^{-1}(\\phi(\\mu|t\\cdot\\widehat{F}-1))\\quad\\text{by Proposition \\ref{prop:1},}\\] \\[\\,\\leq\\,{f_{\\infty}}^{-1}(\\phi(\\mu\\cdot\\widehat{F}-1))\\,=\\,\\beta\\] \\[\\Rightarrow \\,\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}: \\operatorname{\\mathrm{LB}}_{t}(K_{k})\\,{\\leq}\\,\\tilde{\\mu}\\,\\big{\\}}+o(1)\\, \\leq\\,\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{ \\mathrm{frc}}\\big{(}f^{-1}(K_{k})\\big{)}\\leq\\beta\\,\\big{\\}}+o(1)\\] \\[\\Rightarrow \\,\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,n\\in\\mathbb{N}: \\operatorname{\\mathrm{LB}}_{t}(K_{n})\\,{\\leq}\\,\\tilde{\\mu}\\,\\big{\\}}\\,\\leq\\, \\liminf_{n}\\,\\operatorname{\\mathrm{Prob}}\\big{\\{}\\,k\\in\\Omega_{n}: \\operatorname{\\mathrm{frc}}\\big{(}f^{-1}(K_{k})\\big{)}\\leq\\beta\\,\\big{\\}}\\]\nBy Lemma 5.17,\n\n\\[{f_{\\infty}}^{-1}(\\phi(\\hat{\\mu}\\cdot\\widehat{F}-1))\\,\\leq\\,\\liminf_{n}\\,\\, \\operatorname{Prob}\\big{\\{}\\,k\\in\\Omega_{n}:\\operatorname{frc}\\big{(}f^{-1}(K_{k })\\big{)}\\leq\\beta\\,\\big{\\}}\\,.\\]\n\nIt is given that \\(\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{t}(K_{n})\\leq \\mu|t\\,\\big{\\}}\\to{f_{\\infty}}^{-1}(\\phi(\\mu\\cdot\\widehat{F}-1))\\) as \\(t\\to\\infty\\). Let us calculate the other bound;\n\n\\[2+\\tilde{\\mu}*F\\,=\\,\\mu|t*F\\,\\,\\Rightarrow\\,\\,\\,2+\\sum_{k=1}^{t }\\tilde{\\mu}(k)F_{t-k+1}=\\sum_{k=1}^{t}\\mu(k)F_{t-k+1}\\] \\[\\,\\,\\Rightarrow\\,\\,\\,2+\\sum_{k=1}^{t}\\tilde{\\mu}(k)\\Big{(} \\alpha\\phi^{t-k+1}+O(\\phi^{-t+k-1})\\Big{)}\\,=\\,\\,\\sum_{k=1}^{t}\\mu(k)\\Big{(} \\alpha\\phi^{t-k+1}+O(\\phi^{-t+k-1})\\Big{)}\\] \\[\\,\\,\\Rightarrow\\,\\,O(1)+\\alpha\\sum_{k=1}^{t}\\tilde{\\mu}(k)\\phi^ {t-k+1}\\,=\\,\\,\\alpha\\sum_{k=1}^{t}\\mu(k)\\phi^{t-k+1}\\] \\[\\,\\,\\Rightarrow\\,\\,O(\\phi^{-t})+\\sum_{k=1}^{t}\\tilde{\\mu}(k) \\omega^{k-1}\\,=\\,\\,\\sum_{k=1}^{t}\\mu(k)\\omega^{k-1}\\] \\[\\,\\,\\Rightarrow\\,\\,{o(1)}+\\tilde{\\mu}\\cdot\\widehat{F}\\,=\\,\\,\\mu |t\\cdot\\widehat{F}\\,\\,\\Rightarrow\\,\\,\\tilde{\\mu}\\cdot\\widehat{F}\\to\\mu\\cdot \\widehat{F}\\] \\[\\,\\,\\Rightarrow\\,{f_{\\infty}}^{-1}(\\phi(\\hat{\\mu}\\cdot\\widehat{ F}-1))\\to{f_{\\infty}}^{-1}(\\phi(\\mu\\cdot\\widehat{F}-1))\\,=\\,\\,\\beta.\\]\n\nIt is clear that if \\(f\\) is a uniform continuation of \\(F\\), and \\(K\\) is a sequence of positive integers approaching \\(\\infty\\) such that \\(\\operatorname{frc}\\big{(}f^{-1}(K_{n})\\big{)}\\) is equidistributed, then, by Lemma 4.4, \\(K\\) has continuous leading block distribution under \\(\\mathscr{F}\\)-expansion. Therefore, we have the following.\n\n**Theorem 5.19**.: _Let \\(K\\) be a sequence of positive integers approaching \\(\\infty\\). Then, \\(K\\) has continuous leading block distribution under \\(\\mathscr{F}\\)-expansion if and only if there is a uniform continuation \\(f\\) of \\(F\\) such that \\(\\operatorname{frc}\\big{(}f^{-1}(K_{n})\\big{)}\\) is equidistributed._\n\n## 6 Benford's Law under generalized Zeckendorf expansion\n\nThe contents in Sections 3, 4, and 5 are for Zeckendorf expansion, but the arguments of the proofs apply to the setup for generalized Zeckendorf expansion without difficulties. In this section, we introduce definitions and results for generalized Zeckendorf expansion without proofs, but only refer to the corresponding theorems for Zeckendorf expansion proved in the earlier sections.\n\n### Generalized Zeckendorf expansion\n\nLet us review the generalized Zeckendorf expansion. Recall \\(\\mathbb{N}_{0}\\) from Definition 2.1\n**Definition 6.1**.: Given a tuple \\(L=(a_{1},a_{2},\\ldots,a_{N})\\in\\mathbb{N}_{0}^{N}\\) where \\(N\\geq 2\\) and \\(a_{1}>0\\), let \\(\\Theta\\) be the following infinite tuple in \\(\\prod_{k=1}^{\\infty}\\mathbb{N}_{0}\\):\n\n\\[(a_{1},a_{2},\\ldots,a_{N-1},a_{N},a_{1},a_{2},\\ldots,a_{N-1},a_{N},\\ldots)\\]\n\nwhere the finite tuple \\((a_{1},a_{2},\\ldots,a_{N-1},a_{N})\\) repeats. Let \\(\\Theta(k)\\) denote the \\(k\\)th entry of \\(\\Theta\\), and let \\(\\Theta|s=(\\Theta(1),\\ldots,\\Theta(s))\\) for \\(s\\in\\mathbb{N}\\).\n\nRecall len from Definition 2.2. Let \\(\\mathscr{H}^{\\circ}\\) be the recursively-defined set of tuples \\(\\epsilon\\) with arbitrary finite length such that \\(\\epsilon\\in\\mathscr{H}^{\\circ}\\) if and only if there is smallest \\(s\\in\\mathbb{N}_{0}\\) such that \\(\\epsilon|s=\\Theta|s\\), \\(\\epsilon(s+1)<\\Theta(s+1)\\), and \\((\\epsilon(s+2),\\ldots,\\epsilon(n))\\in\\mathscr{H}^{\\circ}\\) where \\(n=\\operatorname{len}(\\epsilon)\\) and \\(s\\) is allowed to be \\(\\operatorname{len}(\\epsilon)\\). Let \\(\\mathscr{H}:=\\{\\epsilon\\in\\mathscr{H}^{\\circ}:\\epsilon(1)>0\\}\\). The set \\(\\mathscr{H}\\) is called a _periodic Zeckendorf collection of coefficient functions for positive integers_, and \\(L\\) is called _a principal maximal block of the periodic Zeckendorf collection \\(\\mathscr{H}\\)_.\n\nNotice that if \\(L=(1,0,1,0)\\) is a principal maximal block of the periodic Zeckendorf collection \\(\\mathscr{H}\\), then \\(L^{\\prime}=(1,0)\\) is a principal maximal block of \\(\\mathscr{H}\\) as well. For this reason, the indefinite article was used in the statement of the definition of principal maximal blocks.\n\n**Example 6.2**.: Let \\(\\mathscr{H}\\) be the (periodic) Zeckendorf collection determined by the principal maximal block \\(L=(3,2,1)\\). Then, \\(\\Theta=(3,2,1,3,2,1,\\ldots)\\), and \\((0)\\) and \\((3,2,1)\\) are members of \\(\\mathscr{H}^{\\circ}\\). For \\((0)\\in\\mathscr{H}^{\\circ}\\), we set \\(s=0\\) in Definition 6.1, and for \\((3,2,1)\\in\\mathscr{H}^{\\circ}\\), we set \\(s=3\\).\n\nLet \\(\\epsilon=(3,2,0)\\) and \\(\\mu=(3,1,3,2,0)\\). For \\(\\epsilon\\), if \\(s=2\\), by the definition, we have \\(\\epsilon\\in\\mathscr{H}\\). For \\(\\mu\\), if \\(s=1\\), then \\(\\mu|1=\\Theta|1\\), \\(\\mu(2)<\\Theta(2)\\), and \\((\\mu(3),\\ldots,\\mu(5))=\\epsilon\\in\\mathscr{H}^{\\circ}\\). Listed below are more examples of members of \\(\\mathscr{H}\\):\n\n\\[(3,2,1,3,2,1),\\,(3,0,0,3),\\,(1,2,3,1,0,3),\\,(1,2,3,1,1,0).\\]\n\nRecall the product notation from Definition 2.2\n\n**Definition 6.3**.: Let \\(\\mathscr{H}\\) be a set of coefficient functions, and let \\(H\\) be an increasing sequence of positive integers. If given \\(n\\in\\mathbb{N}\\), there is a unique \\(\\epsilon\\in\\mathscr{H}\\) such that \\(\\epsilon*H=n\\), then \\(H\\) is called a _fundamental sequence_ of \\(\\mathscr{H}\\), and the expression \\(\\epsilon*H\\) is called an \\(\\mathscr{H}\\)-_expansion_.\n\nIf \\(\\mathscr{H}\\) is a periodic Zeckendorf collection for positive integers, then, by Theorem 6.4 below, there is a unique fundamental sequence of \\(\\mathscr{H}\\).\n\n**Theorem 6.4** ([10, 17]).: _Let \\(\\mathscr{H}\\) be a periodic Zeckendorf collection, and let \\(L=(a_{1},\\ldots,a_{N})\\) be its principal maximal block. Then, there is a unique fundamental sequence \\(H\\) of \\(\\mathscr{H}\\), and it is given by the following recursion:_\n\n\\[H_{n+N}\\,=\\,a_{1}H_{n+N-1}+\\cdots+a_{N-1}H_{n+1}+(1+a_{N})H_{n} \\ \\ \\text{for all}\\ n\\in\\mathbb{N},\\ \\text{and} \\tag{8}\\] \\[H_{n}\\,=\\,1+\\sum_{k=1}^{n-1}a_{k}H_{n-k}\\ \\ \\text{for all}\\ \\ 1\\leq n\\leq N+1.\\]\nIf \\(L=(1,0)\\), then its periodic Zeckendorf collection is \\(\\mathcal{F}\\) defined in Definition 3.1, and its fundamental sequence is the Fibonacci sequence. If \\(L=(9,9)\\), then the fundamental sequence \\(H\\) is given by \\(H_{n}=10^{n-1}\\), and \\(\\epsilon*H\\) for \\(\\epsilon\\in\\mathcal{H}\\) are base-10 expansions.\n\n**Definition 6.5**.: Let \\(L=(a_{1},\\ldots,a_{N})\\) be the list defined in Definition 6.1. Let \\(\\psi=\\psi_{\\mathcal{H}}=\\psi_{L}\\) be the dominant real zero of the polynomial \\(g=g_{\\mathcal{H}}=g_{L}(x):=x^{N}-\\sum_{k=1}^{N-1}a_{k}x^{N-k}-(1+a_{N})\\), and \\(\\theta:=\\psi^{-1}\\). Let \\(\\widehat{H}\\) be the sequence given by \\(\\widehat{H}_{n}=\\theta^{n-1}\\).\n\nBy (8), the sequence \\(\\widehat{H}\\) in Definition 6.5 satisfies\n\n\\[\\widehat{H}_{n}\\,=\\,a_{1}\\widehat{H}_{n+1}+\\cdots+a_{N-1}\\widehat{H}_{n+N-1}+ (1+a_{N})\\widehat{H}_{n+N}\\quad\\text{for all }n\\in\\mathbb{N}. \\tag{9}\\]\n\nThe following proposition is proved in [10, Lemma 43] and [16, Lemma 2.1].\n\n**Proposition 6.6**.: _Let \\(L=(a_{1},\\ldots,a_{N})\\) be the list defined in Definition 6.1, and let \\(g=x^{N}-\\sum_{k=1}^{N-1}a_{k}x^{N-k}-(1+a_{N})\\) be the polynomial. Then, \\(g\\) has one and only one positive real zero \\(\\psi\\), it is a simple zero, and there are no other complex zeros \\(z\\) such that \\(|z|\\geq\\psi\\)._\n\n**Theorem 6.7**.: _Let \\(\\mathcal{H}\\) be a periodic Zeckendorf collection with a principal maximal block \\(L=(a_{1},\\ldots,a_{N})\\), and let \\(H\\) be the fundamental sequence of \\(\\mathcal{H}\\). Then \\(H_{n}=\\delta\\psi^{n}+O(\\psi^{rn})\\) for \\(n\\in\\mathbb{N}\\) where \\(\\delta\\) and \\(r\\) are positive (real) constants, \\(r<1\\), and \\(\\psi\\) is the dominant zero defined in Definition 6.5._\n\nProof.: Let \\(g\\) be the characteristic polynomial of degree \\(N\\) defined in Definition 6.5, and let \\(\\{\\lambda_{1},\\ldots,\\lambda_{m}\\}\\) be the set of \\(m\\) distinct (complex) zeros of \\(g\\) where \\(m\\leq N\\) and \\(\\lambda_{1}=\\psi\\). Then, by Proposition 6.6, we have \\(|\\lambda_{k}|<\\psi\\) for \\(2\\leq k\\leq m\\). Since \\(\\psi\\) is a simple zero, by the generalized Binet's formula [15], there are polynomials \\(h_{k}\\) for \\(2\\leq k\\leq m\\) and a constant \\(\\delta\\) such that \\(H_{n}=\\delta\\psi^{n}+\\sum_{k=2}^{m}h_{k}(n)\\lambda_{k}^{n}\\) for \\(n\\in\\mathbb{N}\\). Thus, there is a positive real number \\(r<1\\) such that \\(H_{n}=\\delta\\psi^{n}+O(\\psi^{rn})\\) for \\(n\\in\\mathbb{N}\\).\n\nNotice that \\(\\lim_{n\\to\\infty}H_{n}\/\\psi^{n}=\\delta\\), and let us show that \\(\\delta\\) is a positive real number, and in particular, it is non-zero. By [11, Theorem 5.1],\n\n\\[\\delta\\,=\\,\\lim_{n\\to\\infty}\\frac{H_{n}}{\\psi^{n}}\\,=\\,\\frac{1}{\\psi g^{\\prime}( \\psi)}\\sum_{k=1}^{N}\\frac{H_{k}}{(k-1)!}\\left[\\frac{d^{k-1}}{dx^{k-1}}\\frac{g( x)}{x-\\psi}\\,\\right]_{x=0}. \\tag{10}\\]\n\nBy the product rule, we have\n\n\\[\\left[\\frac{d^{k-1}}{dx^{k-1}}\\frac{g(x)}{x-\\psi}\\,\\right]_{x=0}\\,=\\,\\left[ \\sum_{j=0}^{k-1}\\binom{k-1}{j}g^{(j)}(x)(x-\\psi)^{-1-j}\\prod_{t=1}^{j}(-t) \\right]_{x=0}.\\]\n\nNotice that if \\(1\\leq j\\leq N-1\\), then \\(g^{(j)}(0)=-a_{N-j}j!\\leq 0\\), and if \\(g(0)=-(1+a_{N})<0\\). The inequality \\((-\\psi)^{-1-j}\\prod_{t=1}^{j}(-t)<0\\) for all \\(0\\leq j\\leq k-1\\) follows immediately from considering the cases of \\(j\\) being even or odd. Thus, the summands in (10) are non-negative, and some are positive. This concludes the proof of \\(\\delta\\) being a positive real number. \n\nFor the remainder of the paper, let \\(\\mathcal{H}\\), \\(H\\), and \\(\\psi\\) be as defined in Definition 6.1.\n### Strong Benford's Law\n\nLet us begin with definitions related to leading blocks under \\(\\mathcal{H}\\)-expansion.\n\n**Definition 6.8**.: Let \\(n=\\epsilon*H\\) for \\(n\\in\\mathbb{N}\\) and \\(\\epsilon\\in\\mathcal{H}\\). If \\(s\\leq\\operatorname{len}(\\epsilon)\\), then \\((\\epsilon(1),\\ldots,\\epsilon(s))\\in\\mathcal{H}\\) is called _the leading block of \\(n\\) with length \\(s\\) under \\(\\mathcal{H}\\)-expansion_. Recall that \\(N=\\operatorname{len}(L)\\). If \\(N\\leq s\\leq\\operatorname{len}(\\epsilon)\\), let \\(\\operatorname{LB}_{s}^{\\mathcal{H}}(n)\\), or simply \\(\\operatorname{LB}_{s}(n)\\) if the context is clear, denote the leading block of length \\(s\\), and if \\(s\\leq\\operatorname{len}(\\epsilon)\\) and \\(s<N\\), then let \\(\\operatorname{LB}_{s}^{\\mathcal{H}}(n)\\) or simply \\(\\operatorname{LB}_{s}(n)\\) denote \\((\\epsilon(1),\\ldots,\\epsilon(s),0,\\ldots,0)\\in\\mathbb{N}_{0}^{N}\\). If \\(s>\\operatorname{len}(\\epsilon)\\), \\(\\operatorname{LB}_{s}(n)\\) is declared to be undefined.\n\nRecall the product \\(*\\) from Definition 2.2. Given an integer \\(s\\geq N\\), let \\(\\mathcal{H}_{s}:=\\{\\mathbf{b}_{1},\\mathbf{b}_{2},\\ldots,\\mathbf{b}_{\\ell}\\}\\) be the finite set of the leading blocks of length \\(s\\) occurring in the \\(\\mathcal{H}\\)-expansions of \\(\\mathbb{N}\\) such that \\(1+\\mathbf{b}_{k}*H=\\mathbf{b}_{k+1}*H\\) for all \\(k\\leq\\ell-1\\). Recall the truncation notation from Definition 4.3. If \\(1\\leq s<N\\), then let \\(\\mathcal{H}_{s}:=\\{\\mathbf{b}_{1},\\mathbf{b}_{2},\\ldots,\\mathbf{b}_{\\ell}\\}\\) be the finite set of the leading blocks of length \\(N\\) occurring in the \\(\\mathcal{H}\\)-expansions of \\(\\mathbb{N}\\) such that \\(\\mathbf{b}_{k}(j)=0\\) for all \\(1\\leq k\\leq\\ell\\) and \\(j>s\\) and \\(1+\\mathbf{b}_{k}|s*H=\\mathbf{b}_{k+1}|s*H\\) for all \\(k\\leq\\ell-1\\). The leading block \\(\\mathbf{b}_{\\ell}\\) is called _the largest leading block in \\(\\mathcal{H}_{s}\\)_.\n\nThe exclusive block \\(\\mathbf{b}_{\\ell+1}\\) is a coefficient function of length \\(s\\) defined as follows. If \\(s\\geq N\\), \\(s\\equiv p\\pmod{N}\\), and \\(0\\leq p<N\\), then\n\n\\[\\mathbf{b}_{\\ell+1}:=(a_{1},\\ldots,a_{N-1},a_{N},\\ldots,a_{1},\\ldots,a_{N-1},1 +a_{N},c_{1},\\ldots,c_{p})\\]\n\nwhere \\(c_{k}=0\\) for all \\(k\\). If \\(1\\leq s<N\\), then \\(\\mathbf{b}_{\\ell+1}:=(a_{1},\\ldots,a_{N-1},1+a_{N})\\). If \\(\\mathbf{b}\\) is a leading block \\(\\mathbf{b}_{k}\\in\\mathcal{H}_{s}\\), then we denote \\(\\mathbf{b}_{k+1}\\) by \\(\\widetilde{\\mathbf{b}}\\).\n\nIf \\(s<N\\), then the leading blocks \\(\\mathbf{b}\\) in \\(\\mathcal{H}_{s}\\) has lengths \\(N\\) with \\(N-s\\) last entries of \\(0\\), and this case is treated as above in order to make \\(\\mathbf{b}\\) and \\(\\widetilde{\\mathbf{b}}\\) in the statement and proof of Lemma 4.4 fit into the case of periodic Zeckendorf collections; see Lemma 6.13.\n\nBy [10, Definition 2 & Lemma 3] and Theorem 6.4, the subscript numbering of \\(\\mathbf{b}_{k}\\in\\mathcal{H}_{s}\\) for \\(1\\leq k\\leq\\ell\\) coincides with the lexicographical order on the coefficient functions. If \\(\\mathbf{b}\\) is the largest leading block in \\(\\mathcal{H}_{s}\\) where \\(s\\geq N\\), then\n\n\\[\\mathbf{b}=(\\ldots,a_{1},\\ldots,a_{N},a_{1},\\ldots,a_{p})\\text{ if }s\\equiv p \\pmod{N}\\text{ and }0\\leq p<N\\text{,}\\]\n\nand \\(1+\\mathbf{b}*H=\\widetilde{\\mathbf{b}}*H=(\\ldots,a_{1},\\ldots,1+a_{N},0,\\ldots,0 )*H=H_{s+1}\\) where the last \\(p\\) entries of \\(\\widetilde{\\mathbf{b}}\\) are zeros. If \\(s\\equiv 0\\pmod{N}\\) and \\(\\mathbf{b}\\) is the largest leading block in \\(\\mathcal{H}_{s}\\), then\n\n\\[\\widetilde{\\mathbf{b}}=(a_{1},\\ldots,a_{N-1},a_{N},\\ldots,a_{1},\\ldots,a_{N-1},1+a_{N}).\\]\n\nIf \\(s<N\\) and \\(\\mathbf{b}\\) is the largest leading block in \\(\\mathcal{H}_{s}\\), then \\(\\widetilde{\\mathbf{b}}=(a_{1},\\ldots,a_{N-1},1+a_{N})\\). Recall \\(\\widehat{H}\\) from Definition 6.5. For all cases, if \\(\\mathbf{b}\\) is the largest leading block in \\(\\mathcal{F}_{s}\\), then \\(\\widetilde{\\mathbf{b}}\\cdot\\widehat{H}=\\psi\\).\n\nThe proof of Theorem 6.9 below follows immediately from Lemma 6.12 and Theorem 6.14.\n\n**Theorem 6.9**.: _Let \\(K\\) be a sequence of positive integers such that \\(K_{n}=ab^{n}(1+o(1))\\) where \\(a\\) and \\(b\\) are positive real numbers such that \\(\\log_{\\psi}b\\) is irrational. Then, given \\(\\mathbf{b}\\in\\mathcal{H}_{s}\\),_\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}\\,\\colon\\operatorname{LB}_{s}(K_{n })=\\mathbf{b}\\,\\right\\}\\;=\\;\\log_{\\psi}\\frac{\\widetilde{\\mathbf{b}}\\cdot\\widehat {H}}{\\mathbf{b}\\cdot\\widehat{H}}.\\]\nMotivated from the leading block distributions of the exponential sequences considered in Theorem 6.9, we declare strong Benford's Law under \\(\\mathscr{H}\\)-expansion as follows.\n\n**Definition 6.10**.: A sequence \\(K\\) of positive integers is said to _satisfy strong Benford's Law under \\(\\mathscr{H}\\)-expansion_ if given \\(\\mathbf{b}\\in\\mathscr{H}_{s}\\),\n\n\\[\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})= \\mathbf{b}\\,\\right\\}\\;=\\;\\log_{\\psi}\\frac{\\widetilde{\\mathbf{b}}\\cdot \\widehat{H}}{\\mathbf{b}\\cdot\\widehat{H}}.\\]\n\n### Benford continuation of \\(H\\)\n\nWe used a real analytic continuation of the Fibonacci sequence for Zeckendorf expansion, but as demonstrated in the earlier sections, the leading block distributions are determined by its limit \\(\\mathfrak{F}_{\\infty}\\). Thus, rather than using a real analytic continuation of \\(H\\), we may use the limit version directly, which is far more convenient. By Theorem 6.7, \\(H_{n}=\\delta\\psi^{n}+O(\\psi^{rn})=\\delta\\psi^{n}(1+o(1))\\) where \\(\\delta\\) and \\(r<1\\) are positive real constants, and we define the following:\n\n**Definition 6.11**.: Let \\(\\mathfrak{H}:[1,\\infty)\\to\\mathbb{R}\\) be the function given by\n\n\\[\\mathfrak{H}(x)=H_{n}+(H_{n+1}-H_{n})\\frac{\\psi^{p}-1}{\\psi-1}\\]\n\nwhere \\(x=n+p\\) and \\(p=\\operatorname{frc}(x)\\), and it is called _a Benford continuation of \\(H\\)_.\n\nRecall Definition 1.6. Then, \\(\\mathfrak{H}\\) is a uniform continuation of \\(H\\), and \\(\\mathfrak{H}_{\\infty}(p)=\\frac{\\psi^{p}-1}{\\psi-1}\\) for all \\(p\\in[0,1]\\). We leave the proof of the following to the reader.\n\n**Lemma 6.12**.: _For real numbers \\(x\\in[1,\\infty)\\), we have \\(\\mathfrak{H}(x)=\\delta\\psi^{x}(1+o(1))\\), and \\(\\mathfrak{H}^{-1}(x)=\\log_{\\psi}(x)-\\log_{\\psi}\\delta+o(1)\\)._\n\nRecall \\(\\mathscr{H}_{s}\\) from Definition 6.8 and \\(\\widehat{H}\\) from Definition 6.5.\n\n**Lemma 6.13**.: _Let \\(K\\) be a sequence of positive real numbers approaching \\(\\infty\\). Let \\(\\mathbf{b}\\in\\mathscr{H}_{s}\\), and let \\(A_{\\mathbf{b}}:=\\{n\\in\\mathbb{N}:\\operatorname{LB}_{s}(K_{n})=\\mathbf{b}\\}\\). Then, there are real numbers \\(\\gamma_{n}=o(1)\\) and \\(\\widetilde{\\gamma}_{n}=o(1)\\) such that \\(n\\in A_{\\mathbf{b}}\\) if and only if_\n\n\\[\\log_{\\psi}\\mathbf{b}\\cdot\\widehat{H}+\\gamma_{n}\\;\\leq\\;\\operatorname{frc} \\left(\\mathfrak{H}^{-1}(K_{n})\\right)\\;<\\;\\log_{\\psi}\\widetilde{\\mathbf{b}} \\cdot\\widehat{H}+\\widetilde{\\gamma}_{n}, \\tag{11}\\]\n\n_where \\(\\widetilde{\\gamma}_{n}=0\\) when \\(\\mathbf{b}\\) is the largest leading block of length \\(s\\)._\n\nThere is no difficulty in applying the arguments of the proof of Lemma 4.4 to Lemma 6.13, and we leave the proof to the reader.\n\nRecall Definition 6.10.\n\n**Theorem 6.14**.: _Let \\(K\\) be an increasing sequence of positive integers such that \\(\\operatorname{frc}\\left(\\mathfrak{H}^{-1}(K_{n})\\right)\\) is equidistributed. Then, \\(K\\) satisfies strong Benford's Law under the \\(\\mathscr{H}\\)-expansion._\n\nThere is no difficulty in applying the arguments of the proof of Theorem 4.5 to Theorem 6.14, and we leave the proof to the reader.\n### Absolute Benford's Law\n\nIntroduced in [10] is a full generalization of Zeckendorf expressions, which is based on the very principle of how Zeckendorf expressions are constructed in terms of lexicographical order. In this most general sense, the collection \\(\\mathcal{H}\\) in Definition 6.1 is called a periodic Zeckendorf collection of coefficient functions. We believe that a property concerning all periodic Zeckendorf collections may be noteworthy, and as in the notion of normal numbers, we introduce the following definition.\n\n**Definition 6.15**.: A sequence \\(K\\) of positive integers is said to _satisfy absolute Benford's Law_ if \\(K\\) satisfies strong \\(\\mathcal{H}\\)-Benford's Law for each periodic Zeckendorf collection \\(\\mathcal{H}\\).\n\nRecall the Lucas sequence \\(K=(2,1,3,4,\\ldots)\\) from Example 5.10. It satisfies strong Benford's Law under all base-\\(b\\) expansions, but it does not satisfy strong Benford's Law under Zeckendorf expansion. Thus, the Lucas sequence does not satisfy absolute Benford's Law.\n\n**Theorem 6.16**.: _Let \\(\\gamma\\) be a positive real number such that \\(\\gamma\\) is not equal to \\(\\psi^{r}\\) for any \\(r\\in\\mathbb{Q}\\) and any dominant real zero \\(\\psi\\) of \\(g_{\\mathcal{H}}\\) where \\(\\mathcal{H}\\) is as defined in Definition 6.5. Let \\(K\\) be the sequence given by \\(K_{n}=\\left\\lfloor\\gamma^{n}\\right\\rfloor\\). Then, \\(K\\) satisfies absolute Benford's Law._\n\nProof.: Let \\(H\\) and \\(\\psi\\) be as defined in Definitions 6.3 and 6.5, and let \\(\\mathfrak{H}\\) be the Benford continuation defined in Definition 6.11. Note that \\(\\psi\\) is algebraic. Notice that \\(\\left\\lfloor\\gamma^{n}\\right\\rfloor=\\gamma^{n+o(1)}\\), and \\(\\log_{\\psi}(\\gamma)\\) is irrational. Thus, by Lemma 6.12,\n\n\\[\\mathfrak{H}^{-1}(K_{n})\\,=\\,(n+o(1))\\log_{\\psi}(\\gamma)-\\log_{\\psi}(\\delta)+ o(1)\\,=\\,n\\log_{\\psi}(\\gamma)-\\log_{\\psi}(\\delta)+o(1).\\]\n\nBy Weyl's Equidistribution Theorem,\n\n\\[\\Rightarrow\\,\\operatorname{Prob}\\left\\{\\,n\\in\\mathbb{N}\\,\\colon\\operatorname{ frc}\\left(\\mathfrak{H}^{-1}(K_{n})\\right)\\leq\\beta\\,\\right\\}\\,=\\,\\operatorname{Prob} \\left\\{\\,n\\in\\mathbb{N}\\,\\colon\\operatorname{frc}\\left(n\\log_{\\psi}(\\gamma) \\right)\\leq\\beta\\,\\right\\}\\,=\\,\\beta.\\]\n\nBy Theorem 6.14, \\(K\\) satisfies Benford's Law under \\(\\mathcal{H}\\)-expansion. \n\n**Corollary 6.17**.: _Let \\(\\gamma>1\\) be a real number that is not an algebraic integer. Then, the sequence \\(K\\) given by \\(K_{n}=\\left\\lfloor\\gamma^{n}\\right\\rfloor\\) satisfies absolute Benford's Law._\n\nProof.: The dominant real zero \\(\\psi\\) defined in Definition 6.5 is an algebraic integer, and so is \\(\\psi^{r}\\) for all \\(r\\in\\mathbb{Q}\\). Thus, if \\(\\gamma\\in\\mathbb{R}\\) is not an algebraic integer, then by Theorem 6.16, \\(K\\) satisfies absolute Benford's Law. \n\n**Example 6.18**.: Let \\(K\\) be the sequence given by \\(K_{n}=\\left\\lfloor\\frac{\\phi}{\\sqrt{5}}(\\frac{89}{55})^{n}\\right\\rfloor\\), which is considered in the introduction. Since \\(\\frac{89}{55}\\) is not an algebraic integer, by Corollary 6.17, the sequence \\(K\\) satisfies absolute Benford's Law.\n### Other Continuations\n\nRecall Definition 1.6, and that \\(H\\) is the fundamental sequence of \\(\\mathcal{H}\\) defined in Definition 6.3. As in Section 5, we relate other continuations of \\(H\\) to the distributions of leading blocks under \\(\\mathcal{H}\\)-expansion.\n\nRecall the Benford continuation \\(\\mathfrak{H}\\) from Definition 6.11, uniform continuations \\(h\\) and \\(h_{\\infty}\\) from Definition 1.6, and the definition of \\(\\widetilde{\\mathbf{b}}\\) from Definition 6.8.\n\n**Theorem 6.19**.: _Let \\(h:[1,\\infty)\\to\\mathbb{R}\\) be a uniform continuation of \\(H\\). Then, there is a sequence \\(K\\) of positive integers approaching \\(\\infty\\), e.g., \\(K_{n}=\\big{|}\\mathfrak{H}(n+\\mathfrak{H}_{n}{}^{-1}\\circ h_{n}\\big{(}\\mathrm{ frc}(n\\pi)\\big{)}\\big{|}\\), such that \\(\\mathrm{frc}\\left(h^{-1}(K_{n})\\right)\\) is equidistributed._\n\n_Let \\(K\\) be a sequence of of positive integers approaching \\(\\infty\\) such that \\(\\mathrm{frc}\\left(h^{-1}(K_{n})\\right)\\) is equidistributed. Let \\(\\mathbf{b}\\in\\mathcal{H}_{s}\\). Then,_\n\n\\[\\mathrm{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\mathrm{LB}_{s}(K_{n})= \\mathbf{b}\\,\\big{\\}} =\\,h_{\\infty}{}^{-1}\\circ\\mathfrak{H}_{\\infty}(\\log_{\\psi} \\widetilde{\\mathbf{b}}\\cdot\\widehat{H})-h_{\\infty}{}^{-1}\\circ\\mathfrak{H}_ {\\infty}(\\log_{\\psi}\\mathbf{b}\\cdot\\widehat{H})\\] \\[=\\,h_{\\infty}{}^{-1}\\Bigg{(}\\frac{\\widetilde{\\mathbf{b}}\\cdot \\widehat{H}-1}{\\psi-1}\\Bigg{)}-h_{\\infty}{}^{-1}\\Bigg{(}\\frac{\\mathbf{b}\\cdot \\widehat{H}-1}{\\psi-1}\\Bigg{)}.\\]\n\nThere is no difficulty in applying the arguments of the proof of Theorem 5.6 to Theorem 6.19, and we leave the proof to the reader.\n\nRecall that \\(\\mathbf{I}=(0,1)\\). As in Definition 5.11, we introduce expressions for \\(\\mathbf{I}\\) that are associated with \\(\\mathcal{H}\\). Recall also the infinite tuple \\(\\Theta\\), \\(\\theta\\), and \\(\\widehat{H}\\), from Definitions 6.1 and 6.5.\n\n**Definition 6.20**.: An infinite tuple \\(\\mu\\in\\prod_{k=1}^{\\infty}\\mathbb{N}_{0}\\) is called an \\(\\mathcal{H}\\)-_expression for \\(\\mathbf{I}\\)_ if there is a smallest \\(i\\in\\mathbb{N}\\) such that \\(\\mu(i)>0\\), \\((\\mu(i),\\ldots,\\mu(k))\\in\\mathcal{H}\\) for all \\(k\\geq i\\), and for all \\(j\\in\\mathbb{N}_{0}\\), the sequence \\(\\{\\mu(j+n)\\}_{n=1}^{\\infty}\\) is not equal to the sequence \\(\\{\\Theta(n)\\}_{n=1}^{\\infty}\\). Let \\(\\mathcal{H}^{*}\\) be the set of \\(\\mathcal{H}\\)-expressions for \\(\\mathbf{I}\\).\n\nGiven \\(s\\in\\mathbb{N}\\) and \\(\\{\\mu,\\tau\\}\\subset\\mathcal{H}^{*}\\), we declare \\(\\mu|s<\\tau|s\\) if \\(\\mu|s\\cdot\\widehat{H}<\\tau|s\\cdot\\widehat{H}\\), which coincides with the lexicographical order on \\(\\mathbb{N}_{0}^{s}\\). We define \\(\\mu\\cdot\\widehat{H}:=\\sum_{k=1}^{\\infty}\\mu(k)\\theta^{k-1}\\), which is a convergent series.\n\nTheorem 6.21 and Proposition 6.22 below are proved in [10].\n\n**Theorem 6.21** (Zeckendorf Theorem for \\(\\mathbf{I}\\)).: _Given a real number \\(\\beta\\in\\mathbf{I}\\), there is a unique \\(\\mu\\in\\mathcal{H}^{*}\\) such that \\(\\beta=\\sum_{k=1}^{\\infty}\\mu(k)\\theta^{k}=(\\mu\\cdot\\widehat{H})\\theta\\)._\n\n**Proposition 6.22**.: _Let \\(\\{\\mu,\\tau\\}\\subset\\mathcal{H}^{*}\\). Then, \\(\\mu\\cdot\\widehat{H}<\\tau\\cdot\\widehat{H}\\) if and only if \\(\\mu|s<\\tau|s\\) for some \\(s\\in\\mathbb{N}\\)._\n\nBy Theorem 6.21, Proposition 6.22 and (9), the function from \\(\\{\\mu\\in\\mathcal{F}^{*}:\\mu(1)=1\\}\\) to \\([0,1)\\) given by the following is bijective:\n\n\\[\\mu\\mapsto\\frac{\\mu\\cdot\\widehat{H}-1}{\\psi-1},\\]\n\nand hence, \\(h_{K}^{*}\\) defined in Definition 6.23 is well-defined.\n**Definition 6.23**.: Let \\(K\\) be a sequence of positive integers approaching \\(\\infty\\) such that given \\(\\mu\\in\\mathscr{H}^{*}\\) such that \\(\\mu(1)=1\\), the following limit exists:\n\n\\[\\lim_{s\\to\\infty}\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB} _{s}(K_{n})\\leq\\mu|s\\,\\big{\\}}\\,. \\tag{12}\\]\n\nLet \\(h_{K}^{*}:[0,1]\\to[0,1]\\) be the function given by \\(h_{K}^{*}(0)=0\\), \\(h_{K}^{*}(1)=1\\), and \\(h_{K}^{*}\\left(\\frac{\\mu\\cdot\\hat{H}-1}{\\psi-1}\\right)\\) is equal to the value in (12). If \\(h_{K}^{*}\\) is continuous and increasing, then \\(K\\) is said to _have continuous leading block distribution under \\(\\mathscr{H}\\)-expansion_.\n\n**Theorem 6.24**.: _Let \\(K\\) be a sequence with continuous leading block distribution under \\(\\mathscr{H}\\)-expansion. Let \\(h_{K}^{*}\\) be the function defined in Definition 6.23. Then, there is a uniform continuation \\(h\\) of \\(H_{n}\\) such that \\(h_{\\infty}{}^{-1}=h_{K}^{*}\\) and \\(\\operatorname{frc}\\big{(}h^{-1}(K_{n})\\big{)}\\) is equidistributed._\n\nThere is no difficulty in applying the arguments of the proof of Theorem 5.18 to Theorem 6.24, and we leave the proof to the reader.\n\n## 7 Benford behavior within expansions\n\nAs mentioned in the introduction, Benford's Law under base-\\(b\\) expansion arises with Zeckendorf expansion, and let us review this result, which is available in [4].\n\nLet \\(\\mathscr{K}\\) be a periodic Zeckendorf collection defined in Definition 6.1, and let \\(K\\) be the fundamental sequence of \\(\\mathscr{K}\\), defined in Definition 6.3. Let \\(S\\) be an infinite subset of \\(\\{K_{n}:n\\in\\mathbb{N}\\}\\) such that \\(q(S):=\\operatorname{Prob}\\big{\\{}\\,n\\in\\mathbb{N}:K_{n}\\in S\\,\\big{\\}}\\) exists. Recall the product \\(*\\) from Definition 2.2. For a randomly selected integer \\(n\\in[1,K_{t+1})\\), let \\(\\mu*K\\) be the \\(\\mathscr{K}\\)-expansion of \\(n\\), let \\(M=\\operatorname{len}(\\mu)\\), and define\n\n\\[P_{t}(n):=\\frac{\\sum_{k=1}^{M}\\mu(k)\\chi_{S}(K_{k})}{\\sum_{k=1}^{M}\\mu(k)} \\tag{13}\\]\n\nwhere \\(\\chi_{S}\\) is the characteristic function on \\(\\{K_{k}:k\\in\\mathbb{N}\\}\\), i.e., \\(\\chi_{S}(K_{k})=1\\) if \\(K_{k}\\in S\\) and \\(\\chi_{S}(K_{k})=0\\), otherwise. Proved in [3] is that given a real number \\(\\epsilon>0\\), the probability of \\(n\\in[1,K_{t+1})\\) such that \\(|P_{t}(n)-q(S)|<\\epsilon\\) is equal to \\(1+o(1)\\) as a function of \\(t\\). For Benford behavior, we let \\(S\\) be the set of \\(K_{n}\\) that have leading (fixed) leading decimal digit \\(d\\). Then, \\(q(S)=\\log_{10}(1+\\frac{1}{d})\\), and the probability of having a summand \\(K_{n}\\) with leading digit \\(d\\) within the \\(\\mathscr{K}\\)-expansion is nearly \\(q(S)\\) most of the times.\n\nThis result immediately applies to our setup. Let \\(\\mathscr{H}\\) and \\(H\\) be as defined in Definition 6.1 different from \\(\\mathscr{K}\\) and \\(K\\). For example, let \\(\\mathscr{H}\\) be the base-\\(b\\) expressions, and let \\(\\mathscr{K}\\) be the Zeckendorf expressions. Then, \\(H\\) is the sequence given by \\(H_{n}=b^{n-1}\\) and \\(K=F\\) is the Fibonacci sequence. Recall from Definition 6.8 that \\(\\mathscr{H}_{s}\\) is a set of leading blocks under \\(\\mathscr{H}\\)-expansion, and that \\(\\operatorname{LB}_{s}^{\\mathscr{H}}(n)\\) denotes the leading block of \\(n\\) in \\(\\mathscr{H}_{s}\\) under \\(\\mathscr{H}\\)-expansion. By Corollary 4.7, the sequence \\(K\\) satisfies (strong) Benford's Law under \\(\\mathscr{H}\\)-expansion, i.e.,\n\n\\[\\operatorname{Prob}\\Big{\\{}\\,n\\in\\mathbb{N}:\\operatorname{LB}_{s}^{\\mathscr{H }}(K_{n})=\\mathbf{b}\\,\\Big{\\}}\\,=\\,\\log_{\\psi}\\frac{\\tilde{\\mathbf{b}}\\cdot \\widehat{H}}{\\mathbf{b}\\cdot\\widehat{H}}\\]\nwhere \\(\\mathbf{b}\\in\\mathscr{H}_{s}\\) and \\(\\psi=b\\), and this is Benford's Law under base-\\(b\\) expansion. The case considered in the introduction is that \\(\\mathscr{H}\\) is the Zeckendorf expansion and \\(\\mathscr{K}\\) is the binary expansion. The following is a corollary of [4, Theorem 1.1]. Recall Definition 6.5.\n\n**Theorem 7.1**.: _Let \\(\\mathscr{H}\\) and \\(H\\) be as defined in Definition 6.1, and Let \\(K\\) be the fundamental sequence of a periodic Zeckendorf collection \\(\\mathscr{K}\\) such that \\(\\psi^{r}_{\\mathscr{H}}\\neq\\psi_{\\mathscr{K}}\\) for all \\(r\\in\\mathbb{Q}\\) where \\(\\psi_{\\mathscr{H}}\\) and \\(\\psi_{\\mathscr{K}}\\) are the dominant real zeros of \\(g_{\\mathscr{H}}\\) and \\(g_{\\mathscr{K}}\\), respectively. Given \\(\\mathbf{b}\\in\\mathscr{H}_{s}\\), let \\(S_{\\mathbf{b}}:=\\langle K_{n}:\\mathrm{LB}_{s}^{\\mathscr{H}}(K_{n})=\\mathbf{b},\\ n\\in\\mathbb{N}\\rangle\\). For a randomly selected integer \\(n\\in[1,K_{t+1})\\), let \\(P_{t}(n)\\) be the proportion defined in (13) with respect to \\(S=S_{\\mathbf{b}}\\). Then, given a real number \\(\\epsilon>0\\), the probability of \\(n\\in[1,K_{t+1})\\) such that_\n\n\\[\\left|P_{t}(n)-\\log_{\\psi_{\\mathscr{K}}}\\frac{\\widetilde{\\mathbf{b}}\\cdot \\widehat{H}}{\\mathbf{b}\\cdot\\widehat{H}}\\right|\\,<\\,\\epsilon\\]\n\n_is equal to \\(1+o(1)\\) as a function of \\(t\\)._\n\n## 8 Future work\n\nInstead of the leading digit, one can look at the distribution of the digit in the second, third, or generally any location. For a sequence that is strong Benford, the further to the right we move in location, the more uniform is the distribution of digits. A natural question is to ask whether or not a similar phenomenon happens with Zeckendorf decompositions, especially as there is a natural furthest to the right one can move.\n\nWe can also look at signed Zeckendorf decompositions. Alpert [1] proved that every integer can be written uniquely as a sum of Fibonacci numbers and their additive inverses where two if two consecutive summands have the same sign then their indices differ by at least \\(4\\) and if they are of opposite sign then their indices differ by at least \\(3\\). We now have more possibilities for the leading block, and one can ask about the various probabilities. More generally, one can consider the \\(f\\)-decompositions introduced in [13], or the non-periodic Zeckendorf collections introduced in [10].\n\nAdditionally, one can explore sequences where there is no longer a unique decomposition, see for example [5, 6, 7, 8, 9], and ask what is the distribution of possible leading blocks. There are many ways we can formulate this question. We could look at all legal decompositions, we could look at what happens for specific numbers, we could look at what happens for specific types of decompositions, such as those arising from the greedy algorithm or those that use the fewest or most summands.\n","pdf_link":"http:\/\/arxiv.org\/pdf\/2309.00090v1.pdf"}
